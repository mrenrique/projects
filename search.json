[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Projects",
    "section": "",
    "text": "Web Scraping Video Details from a Youtube Channel using Selenium\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEDA. Analizing Videos‚Äô Details of Red Bull Batalla de Gallos‚Äô Youtube Channel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a Word Cloud in an image‚Äôs shape by Scraping an Article\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 1, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 28, 2024\n\n\nTristan O‚ÄôMalley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#tldr",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#tldr",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "TL;DR ü§ì",
    "text": "TL;DR ü§ì\nHave you ever crossed some blog post, video or presentation having A fun way to show & analize which are the most relevant topics(repeated words) on a text. Bellow I show you some examples whe can create, it‚Äôs like art made out of words üé®\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#installing-libraries",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#installing-libraries",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "Installing Libraries ‚úîÔ∏è",
    "text": "Installing Libraries ‚úîÔ∏è\nFirst, you need to install all libraries you‚Äôll be using.\n\n!pip install numpy\n!pip install matplotlib\n!pip install newspaper3k\n!pip install pillow\n!pip install wordcloud\n!pip install nltk\n\nprint('Library installation Done!')\n\nCollecting newspaper3k\n  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 211 kB 7.6 MB/s \nRequirement already satisfied: lxml&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\nRequirement already satisfied: Pillow&gt;=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\nCollecting feedparser&gt;=5.2.1\n  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81 kB 10.0 MB/s \nRequirement already satisfied: python-dateutil&gt;=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\nRequirement already satisfied: requests&gt;=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\nCollecting jieba3k&gt;=0.35.1\n  Downloading jieba3k-0.35.1.zip (7.4 MB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.4 MB 60.7 MB/s \nRequirement already satisfied: nltk&gt;=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\nCollecting feedfinder2&gt;=0.0.4\n  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\nCollecting cssselect&gt;=0.9.2\n  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\nRequirement already satisfied: beautifulsoup4&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\nCollecting tinysegmenter==0.3\n  Downloading tinysegmenter-0.3.tar.gz (16 kB)\nRequirement already satisfied: PyYAML&gt;=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\nCollecting tldextract&gt;=2.0.1\n  Downloading tldextract-3.1.2-py2.py3-none-any.whl (87 kB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87 kB 5.8 MB/s \nRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2&gt;=0.0.4-&gt;newspaper3k) (1.15.0)\nCollecting sgmllib3k\n  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (3.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (2021.10.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (1.24.3)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (2.10)\nCollecting requests-file&gt;=1.4\n  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\nRequirement already satisfied: filelock&gt;=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract&gt;=2.0.1-&gt;newspaper3k) (3.4.2)\nBuilding wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n  Building wheel for tinysegmenter (setup.py) ... done\n  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13553 sha256=73e9156b3836ce85dc512439196c449be3d5d33749708372688c3f5d99f1a059\n  Stored in directory: /root/.cache/pip/wheels/df/67/41/faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa\n  Building wheel for feedfinder2 (setup.py) ... done\n  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3357 sha256=fe3990b9a89a29307f18a4767d156083b809112c9ca71e5ef2bbb556ddbd874e\n  Stored in directory: /root/.cache/pip/wheels/7f/d4/8f/6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346\n  Building wheel for jieba3k (setup.py) ... done\n  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398404 sha256=86ab131c0f61eeaef932b8967c1915eaaabc4a1089ea070652c21631f839e9b4\n  Stored in directory: /root/.cache/pip/wheels/4c/91/46/3c208287b726df325a5979574324878b679116e4baae1af3c3\n  Building wheel for sgmllib3k (setup.py) ... done\n  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=9ad6b6bbe0b981e0262c6294017917f7b9cb32e8af7463272df81aaf6f7f2001\n  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\nSuccessfully built tinysegmenter feedfinder2 jieba3k sgmllib3k"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#importing-libraries",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#importing-libraries",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "Importing Libraries üß∞",
    "text": "Importing Libraries üß∞\n\nfrom newspaper import Article\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, ImageColorGenerator\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download(\"stopwords\")\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#step-01-getting-the-corpus-from-an-article",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#step-01-getting-the-corpus-from-an-article",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "Step 01: Getting the Corpus from an Article üìù",
    "text": "Step 01: Getting the Corpus from an Article üìù\n\narticle = Article('https://www.repsol.pe/es/sala-prensa/notas-prensa/comunicado.cshtml')\narticle.download()\narticle.parse()\n\n\nGenerate a Simple Word Cloud Image\n\n# Generate a word cloud image\nwc = WordCloud()\nwc.generate(article.text)\n\n# Display the generated image:\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nGenerate a Customized Word Cloud Image\n\nstopwords = set(stopwords.words('spanish', 'english'))\n\nstopwords.update(['ello', 'cinco', 'd√≠a'])\n\nConverting an image to a numpy array results in an array containing a sequence of values that each represent an individual pixel in the image.\n\nmask = np.array(Image.open('repsol.jpg'))\n# mask = np.array(Image.open('pedro-castillo.jpg'))\n\nwc = WordCloud(stopwords=stopwords,\n               background_color=\"white\",\n               max_words=2000,\n               mask=mask,\n               )\n\nwc.generate(article.text)\n\n# Display the generated image:\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nmask = np.array(Image.open('repsol.jpg'))\n# mask = np.array(Image.open('pedro-castillo.jpg'))\n\nwc = WordCloud(stopwords=stopwords,\n               background_color=\"white\",\n               max_words=2000,\n               mask=mask,\n               max_font_size=256,\n            #    random_state=42,\n            #    width=mask.shape[1],\n            #    height=mask.shape[0]\n               )\n\nwc.generate(article.text)\n\n# create coloring from image\nimage_colors = ImageColorGenerator(mask)\nplt.figure(figsize=[16,14])\nplt.imshow(wc.recolor(color_func=image_colors), interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#references",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#references",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "References:",
    "text": "References:\n\nhttps://medium.com/towards-data-science/create-word-cloud-into-any-shape-you-want-using-python-d0b88834bc32\nhttps://www.repsol.pe/es/sala-prensa/notas-prensa/comunicado.cshtml"
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "",
    "text": "¬øAlguna vez te topaste con alguna tabla de Wikipedia que justo necesitabas para tu investigaci√≥n o proyecto de an√°lisis? Como la tabla de medallas de los Juegos Panamericanos, Las Elecciones Presidenciales o datos de las Exportaciones de un Pais, ¬øSi? Bueno, Aqu√≠ aprender√°s como obtener datos de cualquier tabla.\nEsta es la tabla que buscamos obtener que contiene informaci√≥n del Censo de la Provincia de Trujillo del a√±o 2017. Para hacer Web Scraping a esta p√°gina, utilizaremos las bibliotecas Requests y Pandas de Python.\nAl finalizar este Tutorial, tendr√°s como resultado una tabla lista para su an√°lisis. Lo podr√°s guardar en tu directorio de trabajo o descargar en tu computadora como archivo CSV (o el formato que gustes). Espero que te resulte √∫til; sin m√°s que a√±adir‚Ä¶ ‚ÄúHappy Coding!‚Äù"
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-1-conecci√≥n-y-obtenci√≥n-de-codigo-fuente",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-1-conecci√≥n-y-obtenci√≥n-de-codigo-fuente",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "Fase 1: Conecci√≥n y obtenci√≥n de codigo fuente",
    "text": "Fase 1: Conecci√≥n y obtenci√≥n de codigo fuente\nComenzaremos importando la biblioteca requests para realizar la petici√≥n HTTP que nos devolver√° el c√≥digo fuente de la p√°gina web y pandas, para usar su m√©todo .read_html() que nos ayudar√° a extraer todas las tablas HTML. Usaremos este m√©todo y no la biblioteca Beautiful Soup porque de esta forma es mucho m√°s f√°cil y funciona bien en cualquier p√°gina web que contenga tablas HTML debido a su estructura simple de leer.\n\nPara tu conocimiento: Usando .read_html() no solo nos permite extraer tablas HTML desde un enlace (lo que usaremos hoy), sino tambi√©n desde un archivo html o una cadena de caracteres (string) que contenga HTML.\n\n\n# Hacer peticion HTTP\nimport requests\n# Manipular c√≥digo y guardar datos tabulares en archivo CSV\nimport pandas as pd\n\n\n# url de la p√°gina web a ¬´escrapear¬ª\nurl = 'https://es.wikipedia.org/wiki/Provincia_de_Trujillo_(Per%C3%BA)'\n\n# pasar \"User-agent\" para simular interacci√≥n con la p√°gina usando Navegador web\nheaders = {\"User-agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'}\n\nrespuesta = requests.get(url, headers=headers)\n\n# El c√≥digo de respuesta &lt;200&gt; indicar√° que todo sali√≥ bien\nprint(respuesta)\n\n&lt;Response [200]&gt;\n\n\nAparte de la respuesta positiva que nos devolvi√≥ la petici√≥n, seguro te diste cuenta que pasamos el par√°metro headers al m√©todo requests.get(), esto sirve para evitar que el servidor de la p√°gina web nos bloqueen el acceso. Aunque con Wikipedia no tenemos ese problema, otras p√°ginas son restrictivas cuando se trata de bots tratando de ¬´escrapear¬ª sus datos. Siguiento esta buena pr√°ctica, nos ahorarremos dolores de cabeza luego.\nPor √∫ltimo, guardamos en una variable todas las tablas encontradas en el objecto HTML.\n\nall_tables = pd.read_html(respuesta.content, encoding = 'utf8')"
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-2-an√°lisis-de-estructura-html-y-extracci√≥n-de-datos",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-2-an√°lisis-de-estructura-html-y-extracci√≥n-de-datos",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "Fase 2: An√°lisis de estructura HTML y extracci√≥n de datos",
    "text": "Fase 2: An√°lisis de estructura HTML y extracci√≥n de datos\nAhora veamos cuantas tablas hay en la p√°gina web\n\nprint(f'Total de tablas encontradas: {len(all_tables)}')\n\nTotal de tablas encontradas: 6\n\n\nDebido a que hay varias tablas, usaremos el par√°metro match y le pasaremos una palabra o frase que solo se encuentre en la tabla que nos interesa.\n\nmatched_table = pd.read_html(respuesta.text, match='Fuente: Censos Nacionales 2017')\n\n# imprime numero de tablas que coinciden con parametro match\nprint(f'Total de tablas encontradas: {len(matched_table)}')\n\nTotal de tablas encontradas: 1\n\n\nGuardamos nuestra tabla de inter√©s en una variable.\n\n# Guardar tabla en variable con nombre sem√°ntico\ncenso_trujillo = matched_table[0]\n\n# Verificamos si es la tabla que buscamos\ncenso_trujillo.tail(5)\n\n\n\n\n\n\n\n\n\nUBIGEO\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\n\n\n8\n130109\nSalaverry\n5599\n5244\n18¬†944\n\n\n9\n130110\nSimbal\n1662\n1151\n4061\n\n\n10\n130111\nV√≠ctor Larco Herrera\n19¬†543\n18¬†461\n68¬†506\n\n\n11\nNaN\nTOTAL\n273¬†619\n250¬†835\n970¬†016\n\n\n12\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\n\n\n\n\n\n\n\n\n¬°Bien! Es la tabla que buscamos exportar, pero vemos que las 2 √∫ltimas filas no son necesarias, por lo que pasamos a eliminarlas.\n\n# Remover ultima(s) n fila(s)\ncenso_trujillo.drop(censo_trujillo.tail(2).index, inplace=True)\n\n# Verificar si se eliminaron los registros no deseados\ncenso_trujillo.tail(2)\n\n\n\n\n\n\n\n\n\nUBIGEO\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\n\n\n9\n130110\nSimbal\n1662\n1151\n4061\n\n\n10\n130111\nV√≠ctor Larco Herrera\n19¬†543\n18¬†461\n68¬†506\n\n\n\n\n\n\n\n\nAhora asignaremos el UBIGEO como √≠ndice de la tabla.\n\ncenso_trujillo.set_index('UBIGEO', inplace = True)\n\n# Verificamos el cambio de √≠ndice\ncenso_trujillo.head(2)\n\n\n\n\n\n\n\n\n\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\nUBIGEO\n\n\n\n\n\n\n\n\n130101\nTrujillo\n87¬†963\n82¬†236\n314¬†939\n\n\n130102\nEl Porvenir\n57¬†878\n50¬†805\n190¬†461\n\n\n\n\n\n\n\n\nTambi√©n, vemos que en las columnas Hogares,Viviendas y Poblaci√≥n que contienen n√∫meros, hay un espacio en medio de los n√∫meros que le da Wikipedia como formato para mejorar su visualizaci√≥n. Sin embargo, necesitamo que nuestros datos num√©ricos est√©n limpios sin ning√∫n simbolo o espacios para poder realizar operaciones.\nRemovamos ese espacio que no aporta nuestros datos. Para ello usaremos la biblioteca normalize.\n\nfrom unicodedata import normalize\n\nCreamos una funcion y lo correremos a toda nuestra tabla para quitar los molestos espacios en blanco.\n\ndef remove_whitespace(x):\n    \"\"\"Funcion para normalizar datos con Unicode para luego quitar los espacios usando .replace().\n\n    Argumentos de entrada: Nombre de columna o lista con nombres de columnas.\n    Retorna: columna o columnas sin espacios en blanco\n    \"\"\"\n    if isinstance(x, str):\n        return normalize('NFKC', x).replace(' ', '')\n    else:\n        return x\n\nAplicamos la funcion para quitar espacios en blanco a toda las columnas con datos num√©ricos.\n\n# Guardamos en variable nombre de columnas a quitar espacios en blanco\nnumeric_cols = ['Hogares','Viviendas','Poblaci√≥n']\n\n# Aplicar funci√≥n remove_whitespace a columnas en variable y las reemplazamos en tabla\ncenso_trujillo[numeric_cols] = censo_trujillo[numeric_cols].applymap(remove_whitespace)\n\n# Verificamos si se quitaron los espacios en blanco\ncenso_trujillo.head()\n\n\n\n\n\n\n\n\n\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\nUBIGEO\n\n\n\n\n\n\n\n\n130101\nTrujillo\n87963\n82236\n314939\n\n\n130102\nEl Porvenir\n57878\n50805\n190461\n\n\n130103\nFlorencia De Mora\n7777\n8635\n37262\n\n\n130104\nHuanchaco\n20206\n16534\n68409\n\n\n130105\nLa Esperanza\n49773\n47896\n189206\n\n\n\n\n\n\n\n\nAhora veamos los tipos de datos.\n\n# Mostrar tipo de datos de la tabla\ncenso_trujillo.dtypes\n\nDistrito     object\nHogares      object\nViviendas    object\nPoblaci√≥n    object\ndtype: object\n\n\nComo vemos, todos las columnas son de tipo de dato Objeto (lo que Pandas considera como una cadena de caracteres o String). Como Object es muy amplio, necesitamos definir el tipo de dato correcto a cada columna para que luego se realizar operaciones con los datos.\nAqu√≠ va la primera opci√≥n para hacerlo, reusando la variable numeric_cols usada l√≠neas arriba.\n\n# Asignar el tipo de dato num√©rico a columnas en variable\ncenso_trujillo[numeric_cols] = censo_trujillo[numeric_cols].apply(pd.to_numeric)\n\n# Verificamos que las columnas con n√∫meros tengan el tipo de dato num√©rico asignado\ncenso_trujillo.dtypes\n\nDistrito     object\nHogares       int64\nViviendas     int64\nPoblaci√≥n     int64\ndtype: object\n\n\nComo ves, nuestras columnas con n√∫meros ya tienen el formato correcto, pero la columna Distrito a√∫n se mantiene como Object. Aunque no habr√≠a mayor inconveniente, es mejor especificar que datos contiene cada columna.\nAqu√≠ va la seguida opci√≥n para cambiar el tipo de dato a m√∫ltiples columnas.\n\n# Creamos diccionario y pasamos m√∫ltiples columnas con el tipo de dato a asignar\nconvert_dict = {\n    'Distrito': 'string',\n    'Hogares': 'int',\n    'Viviendas': 'int',\n    'Poblaci√≥n': 'int'\n}\n\ncenso_trujillo = censo_trujillo.astype(convert_dict)\n\n# Verificamos que las columnas con n√∫meros tengan el tipo de dato num√©rico asignado\ncenso_trujillo.dtypes\n\nDistrito     string\nHogares       int64\nViviendas     int64\nPoblaci√≥n     int64\ndtype: object"
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-3-guardado-del-conjunto-de-datos",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-3-guardado-del-conjunto-de-datos",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "Fase 3: Guardado del Conjunto de Datos",
    "text": "Fase 3: Guardado del Conjunto de Datos\nPor fin, una ves los datos est√°n limpios y con el tipo de dato correcto, vamos a guardarlos en formato CSV para usarlos luego.\n\n# Guarda Dataframe a archivo CSV\ncenso_trujillo.to_csv('censo_provincia_trujillo_2017.csv')\n\n# Leamos el archivo para verificar su creacion\npd.read_csv('censo_provincia_trujillo_2017.csv').head(3)\n\n\n\n\n\n\n\n\n\nUBIGEO\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\n\n\n0\n130101\nTrujillo\n87963\n82236\n314939\n\n\n1\n130102\nEl Porvenir\n57878\n50805\n190461\n\n\n2\n130103\nFlorencia De Mora\n7777\n8635\n37262\n\n\n\n\n\n\n\n\nSi lo trabajaste en Jupyter Notebook o tu Editor de C√≥digo Favorito, debe estar ubicado en la misma carpeta donde corriste tu archivo .pynb. Pero, si lo trabajaste en Google Colab (como yo), puedes usar la biblioteca files para descargar el archivo en la computadora donde est√©s trabajando.\n\n# Cargar o descargar archivos\nfrom google.colab import files\n\n# Descarga archivo con datos de tabla\nfiles.download(\"censo_provincia_trujillo_2017.csv\")\n\nprint('Listo, en un momento saldr√° la opci√≥n \"Guardar Como\" para descargar el archivo...')\n\n\n\n\n\n\n\nListo, en un momento saldr√° la opci√≥n \"Guardar Como\" para descargar el archivo...\n\n\n¬°Genial! Ya tenemos los datos de una tabla de Wikipedia lista para su uso. Aunque la tabla extraida es peque√±a en dimensi√≥n, esta forma de trabajo puedes aplicarla para extraer cualquier tabla que encuentres interesante, ya sea de Wikipedia o de cualquier otra p√°gina web que contenga tablas HTML."
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#resumiendo-lo-realizado",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#resumiendo-lo-realizado",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "Resumiendo lo realizado",
    "text": "Resumiendo lo realizado\n\nLe√≠mos las tablas HTML de una p√°gina de Wikipedia\nRemovimos los espacios en formato Unicode que imped√≠an la conversi√≥n al tipo de dato correcto\nConvertimos el tipo de dato de todas las columnas al correcto\nGuardamos la tabla extra√≠da como formato csv para su posterior utilizaci√≥n\nFinalmente, descargamos el archivo csv en la computadora de trabajo\n\n¬°Espera! Una cosa m√°s, como est√°mos en el mes de diciembre, te regalo este pensamiento:\n\nSi lo lees y lo entiendes est√° genial, aunque eso no te asegura haberlo aprendido. Para dominarlo necesitas practicarlo.\n\nPor eso, te sugiero que guardes este art√≠culo en tus Favoritos para que lo practiques luego. O si quieres practicarlo ya mismo, aqu√≠ te dejo el art√≠culo en Google Colab, donde podr√°s abrir y correr el c√≥digo sin necesidad de instalar nada, solo tu navegador web y tus ganas de aprender.\nSi te gust√≥, puedes ver mis otras publicaciones, seguro te ser√°n de utilidad. Si gustas apoyarme, comparte este art√≠culo en tus Redes Sociales (Facebook, Linkedint, Twitter) o si est√°s de buen √°nimo, inv√≠tame una taza de caf√© ‚òï. Nos vemos üèÉüí®"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#tldr",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#tldr",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "TL;DR üëÄ",
    "text": "TL;DR üëÄ\nThis project is to perform most common tasks of Web Scraping by using Selenium as a Scraper Tool and Python for coding. The output will be a CSV file containing information of all International Matches of Freestyle organized by Red Bull from 2015 to 2020 (filtered by internacional and vs keywords). Here you can take a peek or download the csv file which is the result of this project. (Also added at the bottom of this notebook)\nFYI: Red Bull Batalla de los Gallos is the Most Recognized Freestyle Competition in Spanish that brings together the 16 winning Freestylers from the competitions organized by Red Bull in each country. After all matches only one of them is crowned as international champion Click here to learn more\nHer I leave you a screenshot of the Youtube Channel used for this project:\n\n\n\nred bull batalla de los gallos 2020.JPG\n\n\nSatisfying the requirements\nAs always, let‚Äôs first install libraries we‚Äôll be using thought the project These ones are Chromium(browser), Selenium (scraper tool), and tqdm (progress bar)."
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#installing-libraries",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#installing-libraries",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Installing Libraries ‚úîÔ∏è",
    "text": "Installing Libraries ‚úîÔ∏è\n\n# install chromium, selenium and tqdm\n!apt update\n!apt install chromium-chromedriver\n!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n!pip install selenium\n!pip install tqdm\n\nprint('Library installation Done!')\n\n0% [Working]            Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Waiting for header                                                                               Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n0% [Connecting to archive.ubuntu.com] [2 InRelease 14.2 kB/88.7 kB 16%] [Waitin                                                                               Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n0% [Connecting to archive.ubuntu.com (91.189.88.152)] [2 InRelease 14.2 kB/88.70% [Connecting to archive.ubuntu.com (91.189.88.152)] [2 InRelease 14.2 kB/88.7                                                                               Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n0% [Connecting to archive.ubuntu.com (91.189.88.152)] [2 InRelease 14.2 kB/88.70% [3 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)                                                                               Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\nHit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\nGet:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\nHit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\nGet:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\nGet:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [41.5 kB]\nHit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\nGet:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\nHit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\nGet:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,700 kB]\nGet:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [870 kB]\nFetched 2,884 kB in 4s (790 kB/s)\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\n17 packages can be upgraded. Run 'apt list --upgradable' to see them.\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\nSuggested packages:\n  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\nThe following NEW packages will be installed:\n  chromium-browser chromium-browser-l10n chromium-chromedriver\n  chromium-codecs-ffmpeg-extra\n0 upgraded, 4 newly installed, 0 to remove and 17 not upgraded.\nNeed to get 81.0 MB of archives.\nAfter this operation, 273 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 87.0.4280.66-0ubuntu0.18.04.1 [1,122 kB]\nGet:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 87.0.4280.66-0ubuntu0.18.04.1 [71.7 MB]\nGet:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 87.0.4280.66-0ubuntu0.18.04.1 [3,716 kB]\nGet:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 87.0.4280.66-0ubuntu0.18.04.1 [4,488 kB]\nFetched 81.0 MB in 5s (15.5 MB/s)\nSelecting previously unselected package chromium-codecs-ffmpeg-extra.\n(Reading database ... 145480 files and directories currently installed.)\nPreparing to unpack .../chromium-codecs-ffmpeg-extra_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\nUnpacking chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\nSelecting previously unselected package chromium-browser.\nPreparing to unpack .../chromium-browser_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\nUnpacking chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\nSelecting previously unselected package chromium-browser-l10n.\nPreparing to unpack .../chromium-browser-l10n_87.0.4280.66-0ubuntu0.18.04.1_all.deb ...\nUnpacking chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\nSelecting previously unselected package chromium-chromedriver.\nPreparing to unpack .../chromium-chromedriver_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\nUnpacking chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\nSetting up chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\nSetting up chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\nupdate-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\nupdate-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\nSetting up chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\nSetting up chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\nProcessing triggers for hicolor-icon-theme (0.17-2) ...\nProcessing triggers for mime-support (3.60ubuntu1) ...\nProcessing triggers for man-db (2.8.3-2ubuntu0.1) ...\ncp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\nCollecting selenium\n  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 911kB 8.4MB/s \nRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\nInstalling collected packages: selenium\nSuccessfully installed selenium-3.141.0\nRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\nLibrary installation Done!"
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#importing-libraries",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#importing-libraries",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Importing Libraries üß∞",
    "text": "Importing Libraries üß∞\nOnce Installed, We‚Äôll procced to import them.\n\n# set options to be headless\nfrom selenium import webdriver\n#the followings are to avoid NoSuchElementException by using WebDriverWait - to wait until an element appears in the DOM\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n# add random pause seconds to avoid getting blocked\nimport time, random\n\n# to use a progress bar for visual feedback\nfrom tqdm import tqdm\n# to get the current date\nfrom datetime import date\n\n# to save Dataframe into a CSV file format\nimport pandas as pd\nimport numpy as np\n\n# Upload or download files\nfrom google.colab import files\n\nprint('All Libraries imported!')\n\nAll Libraries imported!"
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-01-accessing-the-web-page",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-01-accessing-the-web-page",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Phase 01: Accessing the Web Page üåê",
    "text": "Phase 01: Accessing the Web Page üåê\n\nOpening the Browser and Visiting the Target Web Page\n\n# Setting options for the web browser\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('-headless')\nchrome_options.add_argument('-no-sandbox')\nchrome_options.add_argument('-disable-dev-shm-usage')\n\n# Open browser, go to a website, and get results\nbrowser = webdriver.Chrome('chromedriver',options=chrome_options)\nbrowser.execute_script(\"return navigator.userAgent;\")\nprint(browser.execute_script(\"return navigator.userAgent;\"))\n\nchannel_url = 'https://www.youtube.com/c/RedbullOficialGallos/videos'\n\n# Open website\nbrowser.get(channel_url)\n\n# Print page title\nprint(browser.title)\n\nMozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/87.0.4280.66 Safari/537.36\nRed Bull Batalla De Los Gallos - YouTube\n\n\n\n\nReaching the bottom of this Dynamically Loaded Page\nSince this Page‚Äôs content is dinamically loaded by scrolling down, we use a function to dinamically change the scrollHeight.\n\ndef scroll_to_the_page_bottom(browser):\n    height = browser.execute_script(\"return document.documentElement.scrollHeight\")\n    lastheight = 0\n\n    while True:\n        if lastheight == height:\n            break\n        lastheight = height\n        browser.execute_script(\"window.scrollTo(0, \" + str(height) + \");\")\n        # Pause 2 seconds per iteration\n        time.sleep(2)\n        height = browser.execute_script(\"return document.documentElement.scrollHeight\")\n\n    print('The scroll down reached the bottom of the page, all content loaded!')\n\nscroll_to_the_page_bottom(browser)\n\nThe scroll down reached the bottom of the page, all content loaded!"
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-02-scraping-the-data",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-02-scraping-the-data",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Phase 02: Scraping the data ‚õèÔ∏è",
    "text": "Phase 02: Scraping the data ‚õèÔ∏è\n\nGetting the links of all videos\n\nvideo_anchors = browser.find_elements_by_css_selector('#video-title')\n\nprint(f'This Channel has {len(video_anchors)} videos published')\n\nThis Channel has 3226 videos published\n\n\nFor this project, we‚Äôre gonna gather all the videos link that contains the words: - internacional + vs\nTo do so, we‚Äôll use a list comprehension along with all().\nWe‚Äôre using all() instead of any because we want to filter having all elements present inside each text item. Think about it as the and operator.the any() method then would be like any, because any text item that match at least one of the matches would be inserted in the list called video_links.\n\n# initializing  list of keywords to filter (16 videos only should be)\n\nmatchers = [x.lower() for x in ['Internacional', 'vs']]\nvideo_links = [link.get_attribute('href') for link in tqdm(video_anchors, position=0) if all(match in link.text.lower() for match in matchers)]\n\nprint(len(video_links))\n\n#Show the first link\nvideo_links[0]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3226/3226 [02:30&lt;00:00, 21.44it/s]\n\n\n95\n\n\n'https://www.youtube.com/watch?v=Fwda4AWZ6V4'\n\n\n\n\nGetting all details for each video\nNow, we are going to retrieve the details of all videos we are interested in such us title, views, upload date, lenght of video,likes and dislikes. We‚Äôll use a for loop to iterate over the video_links variable which contains all videos‚Äô urls and per each url we extract the data and save them in variables.\nOnce saved the collection or variables are stored in a Dictionary called data. Finally each dictionary are saved in the variable video_details which basically is a list of dictionaries containing all details per each video scraped. Let‚Äôs jump in the code to better understanding.\n\nvideo_details = []\n\ndelay = 10\n\nfor link in tqdm(video_links, desc='Getting all details for each video', position=0, leave=True):\n\n    try:\n        browser.get(link)\n    except:\n        continue\n\n    # Pause 3 seconds to load content\n    time.sleep(3)\n\n    # Get element  after explicitly waiting for up to 10 seconds\n    title = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , '.title'))).text\n    views = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , '.view-count'))).text.split('\\n')[0].split()[0]\n    upload_date = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , '#date &gt; yt-formatted-string'))).text\n    length = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , '.ytp-time-duration'))).text\n    likes = WebDriverWait(browser, delay).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR , '#top-level-buttons #text')))[0].get_attribute('aria-label').split()[0]\n    dislikes = WebDriverWait(browser, delay).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR , '#top-level-buttons #text')))[1].get_attribute('aria-label').split()[0]\n    url = link\n\n    # inserting all data in the list. We'll also use aternary expression/operator to save a value depending on a condition\n    data = {\n            'title': title,\n            'views': views,\n            'upload_date': upload_date,\n            'length': length,\n            'likes': likes,\n            'dislikes': dislikes,\n            'url': url\n            }\n\n    video_details.append(data)\n\n    # Pause 3 seconds per iteration\n    time.sleep(3)\n\n# Close the browser once the for loop is done\nbrowser.quit()\n\nprint(f'All details of {len(video_links)} videos successfully retrieved')\n\nGetting all details for each video: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [11:53&lt;00:00,  7.51s/it]\n\n\nAll details of 95 videos successfully retrieved\n\n\nExcellent, we just got all videos details and insert them into a list called video_details for convinence.\nTo verify the details per each video were saved correctly let‚Äôs print the first element whitin the list.\n\nvideo_details[0]\n\n{'dislikes': '270',\n 'length': '6:16',\n 'likes': '14,040',\n 'title': 'ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | Red Bull Internacional 2020',\n 'upload_date': 'Dec 12, 2020',\n 'url': 'https://www.youtube.com/watch?v=Fwda4AWZ6V4',\n 'views': '577,503'}"
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-03-saving-the-gathered-data",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-03-saving-the-gathered-data",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Phase 03: Saving the Gathered Data üíæ",
    "text": "Phase 03: Saving the Gathered Data üíæ\n\nSaving data to a CSV file\nTo dynamically name our output csv file, we‚Äôll use from datetime import date which is already imported in the Importing Libraries Section. Let‚Äôs first get the current date and the Youtube Channel‚Äôs Name from the url we provided.\n\ntoday = date.today()\n\n# Month abbreviation, day and year\ntodays_date = today.strftime(\"%b-%d-%Y\")\nprint(f'Fecha de hoy: {todays_date}')\n\nchannel_name = channel_url.split('/')[4]\nprint(channel_name)\n\nFecha de hoy: Dec-27-2020\nRedbullOficialGallos\n\n\nNow, let‚Äôs put all variables together to name the file.\n\n# Programatically naming csv file\ncsv_file_name = f'{channel_name}_videos_details_{todays_date}.csv'.lower()\nprint(csv_file_name)\n\n# Assign columns names\nfield_names = ['title', 'views', 'upload_date', 'length', 'likes', 'dislikes', 'url']\n\nredbulloficialgallos_videos_details_dec-27-2020.csv\n\n\nWe‚Äôre almost done, with the csv_file_name and field_names variables, let‚Äôs turn video_details into a Dataframe which can be used later for any analysis. We‚Äôll need to install Pandas and Numpy to do so. These libraries were already imported in the Importing Libraries Section.\n\n# Create DataFrame\ndf = pd.DataFrame(video_details, columns=field_names)\n\n# Show first 3 rows to verify the dataframe creation\ndf.head(3)\n\n\n\n\n\n\n\n\n\ntitle\nviews\nupload_date\nlength\nlikes\ndislikes\nurl\n\n\n\n\n0\nACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R...\n577,503\nDec 12, 2020\n6:16\n14,040\n270\nhttps://www.youtube.com/watch?v=Fwda4AWZ6V4\n\n\n1\nEXODO LIRICAL vs RAPDER - Semifinal | Red Bull...\n238,463\nDec 12, 2020\n12:30\n8,135\n927\nhttps://www.youtube.com/watch?v=wIcz1_7qx-4\n\n\n2\nACZINO vs SKONE - Semifinal | Red Bull Interna...\n756,352\nDec 12, 2020\n10:06\n18,458\n1,146\nhttps://www.youtube.com/watch?v=yv8yFhRsWVc\n\n\n\n\n\n\n\n\n\n# Save Dataframe into a CSV file format\ndf.to_csv(csv_file_name, index=False)\n\n# Read the file and print the first 3 rows to verify its creation\npd.read_csv(csv_file_name).head(3)\n\n\n\n\n\n\n\n\n\ntitle\nviews\nupload_date\nlength\nlikes\ndislikes\nurl\n\n\n\n\n0\nACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R...\n577,503\nDec 12, 2020\n6:16\n14,040\n270\nhttps://www.youtube.com/watch?v=Fwda4AWZ6V4\n\n\n1\nEXODO LIRICAL vs RAPDER - Semifinal | Red Bull...\n238,463\nDec 12, 2020\n12:30\n8,135\n927\nhttps://www.youtube.com/watch?v=wIcz1_7qx-4\n\n\n2\nACZINO vs SKONE - Semifinal | Red Bull Interna...\n756,352\nDec 12, 2020\n10:06\n18,458\n1,146\nhttps://www.youtube.com/watch?v=yv8yFhRsWVc\n\n\n\n\n\n\n\n\nYay! You reach the end of this article. By now you know how retrieve all videos details from a Youtube Channel. As earlier mentioned, the scraped data should be in the generated csv file. If you worked on it in Jupyter Notebook or your Favorite Code Editor, you can find it in the same folder where you ran your¬†.pynb file. But, if you worked on Google Colab (like me), you need to use the following code to download it: from google.colab import files. This library was already imported in the Importing Libraries Section\n\n# Download the file that contains the scraped table\nfiles.download(csv_file_name)\n\nprint('In a moment the option \"Save As\" will appear to download the file...')\n\n\n\n\n\n\n\nIn a moment the option \"Save As\" will appear to download the file..."
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#takeaways",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#takeaways",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Takeaways",
    "text": "Takeaways\n\nSince Youtube is a loading content Page, I‚Äôve decided to use Selenium as a tool to scrape\nWhen scraping the video_lenght of a video, For some reason some of them return a None value, so we need to use its text version from the arial-label of the same element\nI‚Äôve decided to use Pandas instead of the CSV library to create and save the Dataframe into a CSV file because it‚Äôs easier to use.\nThe elements were accesed using its css selector because is faster and easier to read\nThis project is to show off skills of Web Scraping using Selenium. For the next tutorial, we‚Äôll do the same but using the Youtube API\nSince this project‚Äôs scope is just to gather all data needed in a machine readable format (CSV). What remains to be done is Data Preprocessing and Exploratory Data Analysis\n\nHere I leave you the csv file we‚Äôve just scraped from Youtube."
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#references",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#references",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "References",
    "text": "References\nThis is where I got inspiration from\nHow to Extract & Analyze YouTube Data using YouTube API?\nUsing Selenium wthinin Google Colab\nScroll to end of page in dynamically loading webpage. Answered by: user53558\nSaving a Pandas Dataframe as a CSV\nScroll to end of page in dynamically loading webpage\nAsign variables to dictionary based on value\nWebDriverWait on finding element by CSS Selector\nUse of if else inside a dict to set a value to key using Python\nHow to get back to the for loop after exception handling\ntqdm printing to newline"
  },
  {
    "objectID": "posts/data-preprocessing-with-pandas/index.html#tldr",
    "href": "posts/data-preprocessing-with-pandas/index.html#tldr",
    "title": "Data Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy",
    "section": "TL;DR üëÄ",
    "text": "TL;DR üëÄ\nIn a previous project, I made a Dataset by Scraping the videos details of a Youtube Channel using Selenium and Python. This time I‚Äôll be showing how to perform many tasks in order to process all the gathered information. The output of this project is a Clean and ready-to-analyse Dataset containing information of all International Matches of Freestyle organized by Red Bull from 2015 to 2020 (filtered by internacional and vs keywords). Also, here I leave you the Output Dataset from the previous Web Scraping Project, so you can compare them.\nBut first, let‚Äôs learn a bit about the International Competition. Red Bull Batalla de los Gallos is the Most Recognized Freestyle Competition in Spanish that brings together the 16 winning Freestylers from the competitions organized by Red Bull in each country. After all matches only one of them is crowned as international champion. Click here to learn more"
  },
  {
    "objectID": "posts/data-preprocessing-with-pandas/index.html#importing-libraries",
    "href": "posts/data-preprocessing-with-pandas/index.html#importing-libraries",
    "title": "Data Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy",
    "section": "Importing Libraries üß∞",
    "text": "Importing Libraries üß∞\n\n# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport re\n\nfrom datetime import datetime\n\n# check Pandas' version\npd.__version__\n\n'1.1.5'"
  },
  {
    "objectID": "posts/data-preprocessing-with-pandas/index.html#importing-dataset",
    "href": "posts/data-preprocessing-with-pandas/index.html#importing-dataset",
    "title": "Data Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy",
    "section": "Importing Dataset üóÉÔ∏è",
    "text": "Importing Dataset üóÉÔ∏è\n\n# importing from url\ndata_url = 'https://raw.githubusercontent.com/mrenrique/EDA-to-Youtube-Channel-Videos/main/redbulloficialgallos_videos_details_dec-27-2020.csv'\n\n# reading dataset with pandas and asigning  to a variable\ndata = pd.read_csv(data_url)\n\n# show first three rows\ndata.head(3)\n\n\n\n\n\n\n\n\n\ntitle\nviews\nupload_date\nlength\nlikes\ndislikes\nurl\n\n\n\n\n0\nACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R...\n577,503\nDec 12, 2020\n6:16\n14,040\n270\nhttps://www.youtube.com/watch?v=Fwda4AWZ6V4\n\n\n1\nEXODO LIRICAL vs RAPDER - Semifinal | Red Bull...\n238,463\nDec 12, 2020\n12:30\n8,135\n927\nhttps://www.youtube.com/watch?v=wIcz1_7qx-4\n\n\n2\nACZINO vs SKONE - Semifinal | Red Bull Interna...\n756,352\nDec 12, 2020\n10:06\n18,458\n1,146\nhttps://www.youtube.com/watch?v=yv8yFhRsWVc"
  },
  {
    "objectID": "posts/data-preprocessing-with-pandas/index.html#learning-the-datasets-properties",
    "href": "posts/data-preprocessing-with-pandas/index.html#learning-the-datasets-properties",
    "title": "Data Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy",
    "section": "Learning the Dataset‚Äôs Properties üí°",
    "text": "Learning the Dataset‚Äôs Properties üí°\nLet‚Äôs take a look at the datafame‚Äôs properties for a better understanding to know what needs to be done. To do so, we can use the info() method which gives us the number of columns, columns names and their data types all together.\n\n# Display number of columns, columns names , Non-Null Count and data types\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 95 entries, 0 to 94\nData columns (total 7 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   title        95 non-null     object\n 1   views        95 non-null     object\n 2   upload_date  95 non-null     object\n 3   length       95 non-null     object\n 4   likes        95 non-null     object\n 5   dislikes     95 non-null     object\n 6   url          95 non-null     object\ndtypes: object(7)\nmemory usage: 5.3+ KB\n\n\nNow that we learn about the dataset in a general way, let‚Äôs also learn in a detailed way by showing a random sample of the dataset to give us an idea of what kind of values we are dealing with. Let‚Äôs start by showing a random sample of the dataset.\n\n# Random sample of 50 % of Dataset\ndata.sample(frac=0.5).head()\n\n\n\n\n\n\n\n\n\ntitle\nviews\nupload_date\nlength\nlikes\ndislikes\nurl\n\n\n\n\n19\nTRUENO vs TITO MC - Octavos | Red Bull Interna...\n2,082,852\nNov 30, 2019\n5:51\n43,267\n3,996\nhttps://www.youtube.com/watch?v=KJbIAlUdmLw\n\n\n71\nJONY BELTRAN vs CHUTY - Octavos | Red Bull Int...\n13,138,438\nNov 12, 2016\n7:09\n182,494\n11,726\nhttps://www.youtube.com/watch?v=C2rXItCS8I0\n\n\n23\nJAZE vs SNK - Octavos | Red Bull Internacional...\n1,407,134\nNov 30, 2019\n7:06\n28,687\n890\nhttps://www.youtube.com/watch?v=gkfOnJI4Byc\n\n\n52\nARKANO vs. YENKY ONE - 3 y 4 Puesto: Final Int...\n2,093,488\nDec 3, 2017\n4:47\n30,378\n1,347\nhttps://www.youtube.com/watch?v=VOHgIr6dSZI\n\n\n60\nJONY BELTRAN vs. ARKANO - Cuartos: Final Inter...\n3,352,057\nDec 3, 2017\n4:41\n37,794\n1,164\nhttps://www.youtube.com/watch?v=wWtcdK7bd4Y"
  },
  {
    "objectID": "posts/data-preprocessing-with-pandas/index.html#data-cleaning-and-transformation",
    "href": "posts/data-preprocessing-with-pandas/index.html#data-cleaning-and-transformation",
    "title": "Data Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy",
    "section": "Data Cleaning üßº and Transformation üî™",
    "text": "Data Cleaning üßº and Transformation üî™\nThere are many tasks involved in Data Preprocessing which in turn are grouped into 4 main processes (Data Integration, Data Cleaning, Data Transformation and Data Reduction) but depending on the data and the scope of this project (Exploratory Data Analysis) we‚Äôll just need to perform some of them. let‚Äôs start assuring the Data Quality for further Analysis.\n\nRenaming Columns Names\nLet‚Äôs first show all Columns Names to check if they required changes.\n\n# Show all columns\ndata.columns\n\nIndex(['title', 'views', 'upload_date', 'length', 'likes', 'dislikes', 'url'], dtype='object')\n\n\nAs we see, almost all Columns Names are ok except for upload_date. Let‚Äôs change it for year Since we only need the year of the date.\n\ndata.rename(columns={'upload_date': 'year'}, inplace=True)\n\n# Verify changes\ndata.columns\n\nIndex(['title', 'views', 'year', 'length', 'likes', 'dislikes', 'url'], dtype='object')\n\n\n\n# see types of all columns and change its type if needed\ndata.dtypes\n\ntitle       object\nviews       object\nyear        object\nlength      object\nlikes       object\ndislikes    object\nurl         object\ndtype: object\n\n\n\n\nDeleting Columns not needed (First Attempt)\nIt‚Äôs useful to remove some Columns that doesn‚Äôt contributed to the Analysis Goal. In this case, url Column is not necesary.\n\n# Another way is not using the axis argument but instead asign columns argument\ndata.drop(columns=['url'], inplace=True)\ndata.columns\n\nIndex(['title', 'views', 'year', 'length', 'likes', 'dislikes'], dtype='object')\n\n\n\n\nModifying values by Removing (Additional meaningless data), Adding or Formating them\nNow in order to Set the proper Data Type to each Column we need to make sure that all Columns Values are clean. Let‚Äôs see a few rows to know what kind of values the dataset has.\n\ndata.head()\n\n\n\n\n\n\n\n\n\ntitle\nviews\nyear\nlength\nlikes\ndislikes\n\n\n\n\n0\nACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R...\n577,503\nDec 12, 2020\n6:16\n14,040\n270\n\n\n1\nEXODO LIRICAL vs RAPDER - Semifinal | Red Bull...\n238,463\nDec 12, 2020\n12:30\n8,135\n927\n\n\n2\nACZINO vs SKONE - Semifinal | Red Bull Interna...\n756,352\nDec 12, 2020\n10:06\n18,458\n1,146\n\n\n3\nRAPDER vs YARTZI - Cuartos | Red Bull Internac...\n47,082\nDec 12, 2020\n6:46\n1,822\n206\n\n\n4\nEXODO LIRICAL vs BNET - Cuartos | Red Bull Int...\n685,109\nDec 12, 2020\n6:40\n23,202\n1,842\n\n\n\n\n\n\n\n\nAs you can see, there are some Undesired characters among the values of some Columns. So it‚Äôs necesary to remove Unnecessary Characteres before doing any conversion task. Let‚Äôs start cleaning the title Column to keep only the Names of Freestylers\n\nImportant: Be careful, sometimes there are some characteres that seems similar like these ones - and ‚Äì but they are completely different and it can take you a while figure out why is not spliting as espected. I also had to add a conditional because the name of a participal has the - in it and it was spliting up incorrectly.\n\n\n# Split by multiple different delimiters\npattern = '[-‚Äì|:]'\n\n# data['title'] = [re.split(pattern, i)[0].strip() if 'VALLES-T' not in i else i for i in data['title']]\ndata['title'] = [re.split(pattern, i)[0].strip() if 'VALLES-T' not in i else re.split(' - ', i)[0].strip() for i in data['title']]\n\ndata['title'] = [i.replace('.', '').strip() for i in data['title']]\n\n# verify changes\ndata[data['title'].str.contains('VALLES')].head()\n\n\n\n\n\n\n\n\n\ntitle\nviews\nyear\nlength\nlikes\ndislikes\n\n\n\n\n10\nBNET vs VALLES-T\n1,350,908\nDec 12, 2020\n9:08\n49,448\n3,012\n\n\n18\nBNET vs VALLES-T\n16,680,349\nNov 30, 2019\n17:12\n282,481\n32,957\n\n\n20\nVALLES-T vs CHANG\n11,477,492\nNov 30, 2019\n6:43\n161,561\n2,969\n\n\n26\nVALLES-T vs JOKKER\n3,221,089\nNov 30, 2019\n6:21\n48,888\n1,155\n\n\n31\nVALLES-T vs ACZINO\n16,277,039\nNov 30, 2019\n13:46\n279,388\n9,027\n\n\n\n\n\n\n\n\nLets continue cleaning the Columns views, likes and dislikes, In this case, we‚Äôll remove the comma (,) from views, likes and dislikes Columns Values. Also, in the row 8 (and some others rows) there is the word Premiered before the date string. It needs to be removed.\n\n# List of characters to remove\nchars_to_remove = [' ', ',']\n\n# List of column names to clean\ncols_to_clean = ['views', 'dislikes', 'likes']\n\n# Loop for each column\nfor col in cols_to_clean:\n\n    # Replace each character with an empty string\n    for char in chars_to_remove:\n        data[col] = data[col].astype(str).str.replace(char,'')\n\n# verify changes\ndata.head(3)\n\n\n\n\n\n\n\n\n\ntitle\nviews\nyear\nlength\nlikes\ndislikes\n\n\n\n\n0\nACZINO vs EXODO LIRICAL\n577503\nDec 12, 2020\n6:16\n14040\n270\n\n\n1\nEXODO LIRICAL vs RAPDER\n238463\nDec 12, 2020\n12:30\n8135\n927\n\n\n2\nACZINO vs SKONE\n756352\nDec 12, 2020\n10:06\n18458\n1146\n\n\n\n\n\n\n\n\nAs we said earlier, we only need the last part of the string for each upload_date Column Value.\n\ndata['year'] = [re.split(',', i)[1].strip() for i in data['year']]\n\n# verify changes\ndata['year'].head()\n\n0    2020\n1    2020\n2    2020\n3    2020\n4    2020\nName: year, dtype: object\n\n\n\n\nData Type Convertion\n(less memory usage)\nLet‚Äôs check what Data Types the Columns are\n\ndata.dtypes\n\ntitle       object\nviews       object\nyear        object\nlength      object\nlikes       object\ndislikes    object\ndtype: object\n\n\nSince we already saw the dataset have String, Datetime and Number values, this is not so specific, we need to set the right Data Type to all Columns. Let‚Äôs first try an Automatic Data Type Conversion Method toy see if this will do the trick.\n\n# Convert to Best Data Types Automatically\ndata.convert_dtypes().dtypes\n\ntitle       string\nviews       string\nyear        string\nlength      string\nlikes       string\ndislikes    string\ndtype: object\n\n\nSince we see the code above it‚Äôs not quite effective, we‚Äôll need to convert them manually. Also, from the above code, we see that it‚Äôs neccesary remove some characteres inside Columns Values, that‚Äôs why the automatic method set all columns as a string.\n\ndata['title'] = data['title'].astype(str)\n\n# List of column names to convert to numberic data\ncols_to_modify_dtype = ['views', 'dislikes', 'likes']\n\nfor col in cols_to_modify_dtype:\n    # Convert col to numeric\n    data[col] = pd.to_numeric(data[col])\n\ndata['length'] = pd.to_datetime(data['length'], format='%M:%S').dt.time\n\ndata['year'] = pd.DatetimeIndex(data['year']).year\n\n# verify changes\ndata.dtypes\n\ntitle       object\nviews        int64\nyear         int64\nlength      object\nlikes        int64\ndislikes     int64\ndtype: object\n\n\nLets print once again a few rows of the dataset to see if changes were applied.\n\ndata.head()\n\n\n\n\n\n\n\n\n\ntitle\nviews\nyear\nlength\nlikes\ndislikes\n\n\n\n\n0\nACZINO vs EXODO LIRICAL\n577503\n2020\n00:06:16\n14040\n270\n\n\n1\nEXODO LIRICAL vs RAPDER\n238463\n2020\n00:12:30\n8135\n927\n\n\n2\nACZINO vs SKONE\n756352\n2020\n00:10:06\n18458\n1146\n\n\n3\nRAPDER vs YARTZI\n47082\n2020\n00:06:46\n1822\n206\n\n\n4\nEXODO LIRICAL vs BNET\n685109\n2020\n00:06:40\n23202\n1842\n\n\n\n\n\n\n\n\n\n\nDealing with Missing Values\nFirst we verify if the dataset have Missing Values.\n\n# Show Whether the dataset contain any Missing Value\ndata.isnull().values.any()\n\nFalse\n\n\nSince there is not Missing Values, Let‚Äôs move on to the next task.\n\n\nRemoving Duplicated Values\nIn order to identify if there are Duplicated Values, we‚Äôll use duplicated() method.\n\n# Select duplicate rows except first occurrence based on all columns\nduplicateRowsDF = data[data.duplicated()]\nif duplicateRowsDF.empty == True:\n    print('There arent Duplicated Values. Good to go!')\nelse:\n    print('Duplicate Rows except first occurrence based on all columns are :')\n    print(duplicateRowsDF)\n\nThere arent Duplicated Values. Good to go!\n\n\n\n\nDealing with Inconsistencies Data\n(Business Rule | Domain Expertice required) Modifying | Removing Erroneus Values\nBecause I‚Äôm myself a fan of such Freestyle Competitions, I know that normally there are up to 16 matches every year. Let‚Äôs verify that.\n\ndata['year'].value_counts()\n\n2018    18\n2020    17\n2019    16\n2017    16\n2016    16\n2015    12\nName: year, dtype: int64\n\n\nAs we can see there are more than that in the year 2018 and 2020, Lets find out what‚Äôs going on.\n\ndata[data['year'] == 2018]\n\n\n\n\n\n\n\n\n\ntitle\nviews\nyear\nlength\nlikes\ndislikes\n\n\n\n\n33\nSWITCH vs BNET\n1637196\n2018\n00:07:19\n33768\n493\n\n\n34\nWOS vs RAPDER Octavos\n1831544\n2018\n00:07:01\n33093\n5586\n\n\n35\nBNET vs ARKANO\n3506340\n2018\n00:07:17\n58063\n1371\n\n\n36\nVALLES T vs PEPE GRILLO\n10975462\n2018\n00:07:18\n166863\n2964\n\n\n37\nNEON vs LETRA\n4750022\n2018\n00:07:36\n77815\n1101\n\n\n38\nWOS vs LETRA\n3875917\n2018\n00:07:34\n72927\n5913\n\n\n39\nVALLES T vs BNET\n3609190\n2018\n00:08:09\n62295\n4511\n\n\n40\nWOS vs ACZINO\n39525308\n2018\n00:19:03\n680254\n73824\n\n\n41\nVALLES T vs KDT\n1858540\n2018\n00:07:50\n33888\n877\n\n\n42\nACZINO vs JAZE\n4673494\n2018\n00:07:53\n70535\n5986\n\n\n43\nBNET vs ACZINO\n6880359\n2018\n00:08:14\n108931\n8573\n\n\n44\nYERIKO vs PEPE GRILLO\n263529\n2018\n00:07:14\n5625\n655\n\n\n45\nRVS vs INDICO\n1488867\n2018\n00:10:48\n31762\n495\n\n\n46\nINDICO vs ACZINO\n1578144\n2018\n00:07:28\n21910\n466\n\n\n47\nVALLES T vs WOS\n6938112\n2018\n00:09:14\n116773\n25799\n\n\n48\nDOZER vs ARKANO\n1231381\n2018\n00:08:07\n27260\n4257\n\n\n49\nPerfil de Gallo\n16675\n2018\n00:00:52\n863\n30\n\n\n50\nMARK GRIST vs GALLOS\n33867\n2018\n00:03:13\n1442\n44\n\n\n\n\n\n\n\n\nRows 49 and 50 are not part of the International Competition‚Äô videos, so they need to be removed. Now, let‚Äôs see the rows of 2020 year\n\ndata[data['year'] == 2020]\n\n\n\n\n\n\n\n\n\ntitle\nviews\nyear\nlength\nlikes\ndislikes\n\n\n\n\n0\nACZINO vs EXODO LIRICAL\n577503\n2020\n00:06:16\n14040\n270\n\n\n1\nEXODO LIRICAL vs RAPDER\n238463\n2020\n00:12:30\n8135\n927\n\n\n2\nACZINO vs SKONE\n756352\n2020\n00:10:06\n18458\n1146\n\n\n3\nRAPDER vs YARTZI\n47082\n2020\n00:06:46\n1822\n206\n\n\n4\nEXODO LIRICAL vs BNET\n685109\n2020\n00:06:40\n23202\n1842\n\n\n5\nSKONE vs ACERTIJO\n179664\n2020\n00:09:36\n5341\n4847\n\n\n6\nACZINO vs NAICEN\n158269\n2020\n00:06:51\n5507\n1713\n\n\n7\nSKONE vs RAPDER\n1651540\n2020\n00:15:19\n64965\n3259\n\n\n8\nELEVN vs YARTZI\n56480\n2020\n00:06:30\n2041\n131\n\n\n9\nRAPDER vs STICK\n122237\n2020\n00:06:21\n4389\n2710\n\n\n10\nBNET vs VALLES-T\n1350908\n2020\n00:09:08\n49448\n3012\n\n\n11\nEXODO LIRICAL vs MAC\n105707\n2020\n00:06:13\n5319\n254\n\n\n12\nACERTIJO vs MINOS\n57738\n2020\n00:06:24\n2530\n55\n\n\n13\nSKONE vs TATA\n436674\n2020\n00:06:36\n12055\n17890\n\n\n14\nNAICEN vs SNK\n66128\n2020\n00:06:44\n3196\n200\n\n\n15\nACZINO vs SHIELD MASTER\n154369\n2020\n00:06:51\n5439\n471\n\n\n16\nBLON vs NEW ERA vs YOIKER\n932161\n2020\n00:25:18\n49375\n859\n\n\n\n\n\n\n\n\nThe same, Even though the row 16 is a international Competition Video (info), this match was done to have a reserve competitor just in case any of the 16 couldn‚Äôt make it. But it didn‚Äôt occur. Now let‚Äôs remove all rows are not part of the Oficial Matches‚Äô Videos.\n\n# Delete rows with index label\ndata = data.drop([16, 49 , 50])\n\n\n\nSetting & Modifying the Index Column\nBencause it was necessary to remove some rows (16,49 and 50), the index was changed. Let fix that. Also, I‚Äôll asign a name to the Index Column.\n\ndata.reset_index(inplace = True, drop=True)\n\n\ndata.loc[48:50,:]\n\n\n\n\n\n\n\n\n\ntitle\nviews\nyear\nlength\nlikes\ndislikes\n\n\n\n\n48\nG vs EL TANQUE\n253069\n2017\n00:05:08\n3640\n424\n\n\n49\nARKANO vs YENKY ONE\n2093488\n2017\n00:04:47\n30378\n1347\n\n\n50\nWOS vs ACZINO\n23008624\n2017\n00:07:51\n261785\n13990"
  },
  {
    "objectID": "posts/data-preprocessing-with-pandas/index.html#exporting-the-clean-dataset",
    "href": "posts/data-preprocessing-with-pandas/index.html#exporting-the-clean-dataset",
    "title": "Data Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy",
    "section": "Exporting the Clean Dataset üíæ",
    "text": "Exporting the Clean Dataset üíæ\nNow that we‚Äôre assure the dataset is clean and contain only the right values. Let‚Äôs export it to move on to Exporing and Analizing the dataset.\n\ndata.to_csv('clean_data.csv')\n\nOr if you prefer, you can download it to your Computer.\n\ndata.to_csv('clean_data.csv')\n\nfrom google.colab import files\nfiles.download('clean_data.csv')\n\n\n\n\n\n\n\nYou‚Äôre Awesome üòç, you just reached the end of this post. If you have any questions just drop me a message on my LikedIn. Also, any suggestion or kudos would be quite appreciated. Did you find it useful? Check out my other posts here, I‚Äôm sure you‚Äôll find something interesting üí°.\nShare this post with your friends/colleagues, and if you are in a good mood, buy me a cup of coffee ‚òï. Nos vemos! üèÉüí®"
  },
  {
    "objectID": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#tldr",
    "href": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#tldr",
    "title": "EDA. Analizing Videos‚Äô Details of Red Bull Batalla de Gallos‚Äô Youtube Channel",
    "section": "TL;DR ü§ì",
    "text": "TL;DR ü§ì\nThis project‚Äôs aim is to perform some common EDA tasks on the created dataset containing information of all International Matches of Freestyle organized by Red Bull from 2015 to 2020 (filtered by internacional and vs keywords). Red Bull Batalla de los Gallos is the Most Recognized Freestyle Competition in Spanish that brings together the 16 winning Freestylers from the competitions organized by Red Bull in each country. After all matches only one of them is crowned as international champion Click here to learn more"
  },
  {
    "objectID": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#importing-libraries",
    "href": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#importing-libraries",
    "title": "EDA. Analizing Videos‚Äô Details of Red Bull Batalla de Gallos‚Äô Youtube Channel",
    "section": "Importing Libraries ‚úîÔ∏è",
    "text": "Importing Libraries ‚úîÔ∏è\nIn order to achieve the goal of this project, It‚Äôs necesary to install & import some libraries that will make our life a lot easier.\n\nNumpy for doing mathematical operations\nPandas for manipulating structured data & making EDA\nMatplotlib & Seaborn, this one help us create graphs to visually understand the EDA\nDatetime will make the task of dealing with time data a lot easier\n\nOnce imported all the libraries requieres, let‚Äôs also check their version as reference.\n\n#collapse-hide\n# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nsns.set_theme(style=\"ticks\", color_codes=True)\n\n# Check Libraries' version\nprint('Numpy version: '+np.__version__)\nprint('Pandas version: '+pd.__version__)\nprint('Matplotlib version: '+matplotlib.__version__)\nprint('Seaborn version: '+sns.__version__)\n\nNumpy version: 1.21.6\nPandas version: 1.3.5\nMatplotlib version: 3.2.2\nSeaborn version: 0.11.2"
  },
  {
    "objectID": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#customized-settings",
    "href": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#customized-settings",
    "title": "EDA. Analizing Videos‚Äô Details of Red Bull Batalla de Gallos‚Äô Youtube Channel",
    "section": "Customized Settings üé®",
    "text": "Customized Settings üé®\nIn the hidden code cell bellow there is two functions, both of them customizes som of the default parameters of the graphs, to make them look a bit cleaner and easy to digest.\nAlso, It‚Äôs advisable to present graphs with the same colors as the brand to make it a bit relatable, so I picked & used the event‚Äôs logo colors for this purpose. To get these color palette I used this website which is very useful for this task: https://coolors.co/image-picker\n\n#collapse-hide\n# Custom palette\n# https://www.youtube.com/watch?v=2wRHBodrWuY\ng=[]\n\n# def customPlotSettings(graph=g, figW=6.4, figH=5, XorY=plt.yticks([])):\ndef customPlotSettings(graph=g, figW=6.4, figH=5, dimension=1000, Character='k'):\n\n    g.fig.set_figwidth(figW)\n    g.fig.set_figheight(figH)\n\n    ax=g.facet_axis(0,0)\n\n    for p in ax.patches:\n        height = p.get_height() # height of each horizontal bar is the same\n        width = p.get_width()\n        ax.text(p.get_x() + (width / 2),\n                height * 1.03, # # y-coordinate position of data label, padded to be in the middle of the bar,\n                f'{(height / dimension ):.0f}'+Character+'',\n                # f'{(height / fHeight ):.0f}K',\n                ha='center'\n                )\n\n    # Remove frame (or all the spines at the same time)\n    ax.set_frame_on(False)\n\n    custom_params = {\n                     'axes.titlesize':16,\n                     'ytick.left': False,\n                     'axes.titlepad': 20\n                    }\n\n    sns.set_theme(style='white', font_scale=1.1 , rc=custom_params)\n\n    custom_palette = ['#203175','#E30C4C','#FDCA24']\n    sns.set_palette(custom_palette)\n\ndef customHistSettings(figW=6.4):\n\n    fig, ax = plt.subplots()\n    custom_params = {\n                     'figure.figsize':(figW,5),\n                     'axes.titlesize':16,\n                     'ytick.left': False\n                    }\n    sns.set_theme(style='white', rc=custom_params)\n\n    ax.grid(axis ='x', color ='0.95')\n    ax.set_frame_on(False)\n    plt.yticks([])\n\n    custom_palette = ['#203175','#E30C4C','#FDCA24']\n    sns.set_palette(custom_palette)"
  },
  {
    "objectID": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#importing-dataset",
    "href": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#importing-dataset",
    "title": "EDA. Analizing Videos‚Äô Details of Red Bull Batalla de Gallos‚Äô Youtube Channel",
    "section": "Importing dataset üóÉÔ∏è",
    "text": "Importing dataset üóÉÔ∏è\nLet‚Äôs start by importing from Github the tidy dataset which was a result from the previous tutorial. This tutorial covered Data Preprocessing Videos Details of a Youtube Channel\nAlso, I‚Äôll print 3 random rows to check the dataset was imported succesfully.\n\ndata_url = 'https://raw.githubusercontent.com/mrenrique/EDA-to-Youtube-Channel-Videos/main/clean_data.csv'\ndata = pd.read_csv(data_url, index_col='id')\n\n# show first three rows\ndata.sample(3)\n\n\n\n  \n    \n      \n\n\n\n\n\n\ntitle\nviews\nyear\nlength\nlikes\ndislikes\n\n\nid\n\n\n\n\n\n\n\n\n\n\n43\nYERIKO vs PEPE GRILLO\n263529\n2018\n00:07:14\n5625\n655\n\n\n65\nGASPER vs SHADOW\n1219652\n2016\n00:04:52\n9546\n3275\n\n\n54\nWOS vs. SKONE\n4220417\n2017\n00:00:14\n58139\n2761"
  },
  {
    "objectID": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#data-pre-processing",
    "href": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#data-pre-processing",
    "title": "EDA. Analizing Videos‚Äô Details of Red Bull Batalla de Gallos‚Äô Youtube Channel",
    "section": "Data Pre-processing üßº",
    "text": "Data Pre-processing üßº\nLet‚Äôs get a glance of the structure of the dataset and their properties\n\ndata.info()\n\nNow, I‚Äôll start with some modifications on the features. From above, I noticed that the column length has time related values, so it‚Äôs requiered to give it a proper format and assign the data type.\n\ndata['length'] = pd.to_datetime(data['length'], format=\"%H:%M:%S\")\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 92 entries, 0 to 91\nData columns (total 6 columns):\n #   Column    Non-Null Count  Dtype         \n---  ------    --------------  -----         \n 0   title     92 non-null     object        \n 1   views     92 non-null     int64         \n 2   year      92 non-null     int64         \n 3   length    92 non-null     datetime64[ns]\n 4   likes     92 non-null     int64         \n 5   dislikes  92 non-null     int64         \ndtypes: datetime64[ns](1), int64(4), object(1)\nmemory usage: 5.0+ KB\n\n\nHow about the videos title? Are there any duplicated value?\n\ndata['title'].unique()\n\nAlmost correct, except for the name of a Frestyler which appears as VALLES T and VALLEST. Since it make reference to the same artist, we go on and replace the assure only one way of naming him.\nWe‚Äôll check the changes by filtering part of his nicknake that contain VALLES in the title column.\n\n# https://note.nkmk.me/en/python-str-replace-translate-re-sub/\ndata['title'] = [i.replace('VALLEST', 'VALLES-T').replace('VALLES T', 'VALLES-T') for i in data['title']]\n\ndata['title'][data['title'].str.contains('VALLES')]\n\nid\n10    BNET vs VALLES-T - Octavos | Red Bull Internac...\n17    BNET vs VALLES-T - Final | Red Bull Internacio...\n19    VALLES-T vs CHANG - Octavos | Red Bull Interna...\n25    VALLES-T vs JOKKER - Cuartos | Red Bull Intern...\n30    VALLES-T vs ACZINO - Semifinal | Red Bull Inte...\n35                              VALLES-T vs PEPE GRILLO\n38                                     VALLES-T vs BNET\n40                                      VALLES-T vs KDT\n46                                      VALLES-T vs WOS\n66                                VALLES-T vs CIUDADANO\n72                                     JOTA vs VALLES-T\nName: title, dtype: object\n\n\nOne these changes were made, we‚Äôre good to go to enrich the dataset."
  },
  {
    "objectID": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#feature-engineering",
    "href": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#feature-engineering",
    "title": "EDA. Analizing Videos‚Äô Details of Red Bull Batalla de Gallos‚Äô Youtube Channel",
    "section": "Feature Engineering üèóÔ∏è",
    "text": "Feature Engineering üèóÔ∏è\nMoving on, to enrich this small dataset & find some insights, I split the title column into Freestyler A & Freesttyler B that are the two rival artists. I used list comprehensions for achieving this task. As always, I printed some samples to check last changes.\n\n# https://stackoverflow.com/questions/40705480/python-pandas-remove-everything-after-a-delimiter-in-a-string\ndata['Freestyler_A'] = [i.replace('.', '').lower().split(' vs ')[0].strip().title() for i in data['title']]\ndata['Freestyler_B'] = [i.replace('.', '').split(' -')[0].lower().split(' vs ')[-1].strip().title() for i in data['title']]\n\n#Moving the columns position\ndata.columns.tolist()\n\ndata = data[['title', 'Freestyler_A', 'Freestyler_B', 'views', 'year', 'length', 'likes', 'dislikes']]\n\ndata.sample(5)\n\n\n\n  \n    \n      \n\n\n\n\n\n\ntitle\nFreestyler_A\nFreestyler_B\nviews\nyear\nlength\nlikes\ndislikes\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n58\nYENKY ONE vs. G\nYenky One\nG\n322030\n2017\n1900-01-01 00:04:48\n5734\n1363\n\n\n15\nACZINO vs SHIELD MASTER\nAczino\nShield Master\n154369\n2020\n1900-01-01 00:06:51\n5439\n471\n\n\n23\nJOKKER vs LITZEN\nJokker\nLitzen\n2546416\n2019\n1900-01-01 00:09:46\n40625\n900\n\n\n72\nJOTA vs VALLES-T\nJota\nValles-T\n1073224\n2016\n1900-01-01 00:05:04\n13330\n6872\n\n\n11\nEXODO LIRICAL vs MAC\nExodo Lirical\nMac\n105707\n2020\n1900-01-01 00:06:13\n5319\n254"
  },
  {
    "objectID": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#exploratory-data-analysis",
    "href": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#exploratory-data-analysis",
    "title": "EDA. Analizing Videos‚Äô Details of Red Bull Batalla de Gallos‚Äô Youtube Channel",
    "section": "Exploratory Data Analysis üí°",
    "text": "Exploratory Data Analysis üí°\nNow we are finally in the exciting part of this notebook: EDA Process.\n\nGeneral View of the Dataset\nLet‚Äôs take a look at the datafame‚Äôs properties for a better understanding to know what needs to be done. To do so, we can use the info() method which gives us the number of columns, columns names and their data types all together.\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 92 entries, 0 to 91\nData columns (total 6 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   title     92 non-null     object\n 1   views     92 non-null     int64 \n 2   year      92 non-null     int64 \n 3   length    92 non-null     object\n 4   likes     92 non-null     int64 \n 5   dislikes  92 non-null     int64 \ndtypes: int64(4), object(2)\nmemory usage: 5.0+ KB\n\n\nHow about how many rows and columns the dataset has?\n\ndata.shape\n\nprint(\"The Dataset has\", data.shape[0],\"rows with\", data.shape[1],\"features.\")\n\nThe Dataset has 92 rows with 6 features\n\n\nLet‚Äôs summarize some statistical metrics of the dataset by using describe() function.\n\ndata.describe().T\n\n\n\n  \n    \n      \n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nviews\n92.0\n4.773642e+06\n8.383880e+06\n47082.0\n661099.50\n1715757.0\n4372855.00\n44005544.0\n\n\nyear\n92.0\n2.017609e+03\n1.670405e+00\n2015.0\n2016.00\n2018.0\n2019.00\n2020.0\n\n\nlikes\n92.0\n6.649529e+04\n1.203502e+05\n1510.0\n9971.25\n29532.5\n59178.00\n729024.0\n\n\ndislikes\n92.0\n8.634130e+03\n2.347569e+04\n55.0\n607.00\n1645.0\n5667.75\n194847.0\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nHow about how many unique values it has?\n\ndata.nunique()\n\ntitle       88\nviews       92\nyear         6\nlength      77\nlikes       92\ndislikes    92\ndtype: int64\n\n\n\n\nLet‚Äôs Analyze each Feature\nUnivariate Analysis\nOnce we get a general glance of the datasets properties & statistics, now we can proceed to leverage the power of Data Visualization (graphs) to better understand any aspect of each feature of the dataset.\n\n#collapse-hide\ng = sns.catplot(data=data, x='year', kind='count', palette=sns.blend_palette(['#203175','#E30C4C','#FDCA24'])) # Set your custom color palette\ng.set(ylabel=None)\n\nplt.title('Number of Videos by Year');\n\n\n\n\n\n\n\n\nLet‚Äôs find out, how many times each Freestyler appears on the video‚Äôs title? Put it in other words, how many times Each Freestyler has a battle participation on this international event?\n\nF_concated = pd.concat([data['Freestyler_A'], data['Freestyler_B']])\nF_concated.value_counts()\n\nAczino       21\nArkano       14\nValles-T     11\nSkone        10\nBnet         10\n             ..\nYeriko        1\nRvs           1\nDozer         1\nRedencion     1\nMrjunior      1\nLength: 62, dtype: int64\n\n\nThe same as above, but graphically presented\n\n#collapse-hide\nimport matplotlib.ticker as mticker\n\nF_concated.value_counts().sort_values(ascending=True).plot(kind='barh', figsize=(12, 15), color=['#203175','#E30C4C','#FDCA24'])\n# Show x Axis as integer\nplt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n\ng.set(xlabel=None)\ng.set(ylabel=None)\n\nplt.title('Numbers of Appereances By each Freestyler in any International from 2015 to 2020');\n\n\n\n\n\n\n\n\nNow I wanted to present the distribution of each variable. In this case, the distribution of Views feature.\n\n#collapse-hide\ncustomHistSettings(figW=9)\n\ng=sns.histplot(data.views, bins=25, kde=True, stat='density', linewidth=0)\nplt.xlabel('Views')\nplt.title('Distribution of Views')\n\nxlabels = ['{:,.0f}'.format(x) + 'M' for x in g.get_xticks()/(1000000)]\ng.set_xticklabels(xlabels)\n\n#Plooting the median\nmean = data.views.median()\nmean\nplt.axvline(mean, 0, 1, color='#E30C4C');\n\n\n\n\n\n\n\n\nLet‚Äôs present the same as before but using a boxplot graph that help us to undestard the data ranges by quartiles and also point out any outlies that lies outside the whiskers.\n\n#collapse-hide\ng=sns.catplot(data=data, x='views', kind='box')\ncustomPlotSettings(figW=9)\nplt.title('Distribution of Views (M)')\n\nText(0.5, 1.0, 'Distribution of Views')\n\n\n\n\n\n\n\n\n\nFrom above, we can tell that many videos has less than 1 Million views and that there are some outiers, even so, 3 of them has over 40 million views.\nHow behaves the Likes feature?\n\n#collapse-hide\ncustomHistSettings(figW=9)\n\ng=sns.histplot(data.likes, bins=25, kde=True, stat='density', linewidth=0)\nplt.xlabel('Likes')\nplt.title('Distribution of Likes')\n\nxlabels = ['{:,.0f}'.format(x) + 'K' for x in g.get_xticks()/1000]\ng.set_xticklabels(xlabels)\n\n#Plooting the median\nmean = data.likes.median()\nmean\nplt.axvline(mean, 0, 1, color='#E30C4C');\n\n\n\n\n\n\n\n\nMany of the videos are quite popular & likeables, they range from between 100k & 300k of likes, except for the outlier that has more than 700k.\nNow let‚Äôs analyzed the opposite, the dislikes feature.\n\n#collapse-hide\ncustomHistSettings(figW=9)\n\ng=sns.histplot(data.dislikes, bins=25, kde=True, stat='density', linewidth=0)\nplt.xlabel('Dislikes')\nplt.title('Distribution of Dislikes')\n\nxlabels = ['{:,.0f}'.format(x) + 'K' for x in g.get_xticks()/1000]\ng.set_xticklabels(xlabels)\n\n#Plooting the median\nmean = data.dislikes.median()\nmean\nplt.axvline(mean, 0, 1, color='#E30C4C');\n\n\n\n\n\n\n\n\nMany of the videos falls into the range of 0k to 25k of dislikes, which is okey for videos with views over 170k and likes on average of +20k\n\n\nHow They Behave if We Put Them Together? ü§î\nBivariate Analysis\nLet‚Äôs moving on to find out how these featues behave when we analyzed them together.\nWe can see that, on average, many views were gathered mostly in 2019 & 2015, the latter one also surpass the other four years. Also, the year with less views was 2020.\n\n#collapse-hide\ng=sns.catplot(data=data, x='year', y='views', estimator=np.mean, kind='bar', palette=sns.blend_palette(['#203175','#E30C4C','#FDCA24']))\nplt.title('Average of Views By Year')\n\ncustomPlotSettings(figW=9);\n\n\n\n\n\n\n\n\nWhen plotting the views by year, it‚Äôs noticeable that most years, except for 2020, have outlies which will increment the average of views. Furthermore, 2005, 2018 y 2019 have battle videos (outliers) with more than 40M of views.\n\n#collapse-hide\ng=sns.catplot(data=data, x='year', y='views', kind='box', palette=sns.blend_palette(['#203175','#E30C4C','#FDCA24']))\nplt.title('Distribution of Views By Year')\n\ncustomPlotSettings(figW=9);\n\n\n\n\n\n\n\n\n\n#collapse-hide\ng=sns.catplot(data=data, x='year', y='likes', estimator=np.mean, kind='bar', palette=sns.blend_palette(['#203175','#E30C4C','#FDCA24']))\nplt.title('Average of Likes By Year')\n\ncustomPlotSettings(figW=9);\n\n\n\n\n\n\n\n\nHere, we can see that videos from 2018 and 2019 has the most number of likes (+700K). Also, except for 2019, most years has a close range with not much variation.\n\n#collapse-hide\ng=sns.catplot(data=data, x='year', y='likes', kind='box', palette=sns.blend_palette(['#203175','#E30C4C','#FDCA24']))\nplt.title('Distribution of Likes By Year')\n\ncustomPlotSettings(figW=9);\n\n\n\n\n\n\n\n\n\n\nCorrelation between Numerical Features\nNow let‚Äôs continue to analyze if there is any correlation between Numerical Features. I used .corr() and then seaborn‚Äôs .heatmap() function to plot a heatmap graph for an easy-to-digest understanding of correlation for each numerical features\n\n#collapse-hide\n# Calculate correlation between each pair of variable\ncorr = data.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Insert a figure\nf, ax = plt.subplots(figsize=(10, 7))\ncmap = sns.diverging_palette(10, 220, as_cmap=True)\n\n# Draw the heatmap with the mask\nax = sns.heatmap(corr,\n                 mask=mask,\n                 cmap=cmap,\n                 annot=True,\n                 annot_kws= {'size':11},\n                 square=True, xticklabels=True,\n                 yticklabels=True,\n                 linewidths=.5,\n                 cbar_kws={'shrink': .5},\n                 ax=ax\n                )\nax.set_title('Correlation between Numerical Features', fontsize=20);\n\n\n\n\n\n\n\n\nWe can drew from the previous graph that there is a high positive correlation between views & likes (not surprising). Besides that, theres is a high negative correlation between years & views and a low negative corrlation between years and dislikes.\nHaving into consideration the previous insight, let‚Äôs plot a Scatterplot to show what this correlation between views and likes looks like.\n\n#collapse-hide\nplt.figure(figsize=(12,6))\n# use the scatterplot function to build the bubble map\ng=sns.regplot(data=data, x='likes', y='views')\nsns.despine()\n\n# Add titles (main and on axis)\nplt.xlabel('Likes')\nplt.ylabel('Views')\nplt.title('Relationshitp Between Views & Likes');\n\n\n\n\n\n\n\n\nFinally, let‚Äôs plot it by years to see how this relationship behaves.\n\n#collapse-hide\ng = sns.relplot(data=data,\n                 x='likes',\n                 y='views',\n                 col='year',\n                 kind='scatter',\n                 col_wrap=3,\n                 height=6)\n\ng.fig.subplots_adjust(top=0.9) # adjust the Figure in g\ng.fig.suptitle('Relationshitp Between Views & Likes By Year');\n\nText(0.5, 0.98, 'Relationshitp Between Views & Likes By Year')\n\n\n\n\n\n\n\n\n\nYou‚Äôre Awesome üòç, you just reached the end of this post. If you have any questions just drop me a message on my LikedIn. Also, any suggestion or kudos would be quite appreciated. Did you find it useful? Check out my other posts here, I‚Äôm sure you‚Äôll find something interesting üí°.\nShare this post with your friends/colleagues, and if you are in a good mood, buy me a cup of coffee ‚òï. Nos vemos! üèÉüí®"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  }
]