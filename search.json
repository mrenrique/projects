[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Projects",
    "section": "",
    "text": "Web Scraping Video Details from a Youtube Channel using Selenium\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Wild Blueberry Yield Using Supervised Machine Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEDA. Analizing Videos‚Äô Details of Red Bull Batalla de Gallos‚Äô Youtube Channel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a Word Cloud in an image‚Äôs shape by Scraping an Article\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 1, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 28, 2024\n\n\nTristan O‚ÄôMalley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#tldr",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#tldr",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "TL;DR ü§ì",
    "text": "TL;DR ü§ì\nHave you ever crossed some blog post, video or presentation having A fun way to show & analize which are the most relevant topics(repeated words) on a text. Bellow I show you some examples whe can create, it‚Äôs like art made out of words üé®\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#installing-libraries",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#installing-libraries",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "Installing Libraries ‚úîÔ∏è",
    "text": "Installing Libraries ‚úîÔ∏è\nFirst, you need to install all libraries you‚Äôll be using.\n\n!pip install numpy\n!pip install matplotlib\n!pip install newspaper3k\n!pip install pillow\n!pip install wordcloud\n!pip install nltk\n\nprint('Library installation Done!')\n\nCollecting newspaper3k\n  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 211 kB 7.6 MB/s \nRequirement already satisfied: lxml&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\nRequirement already satisfied: Pillow&gt;=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\nCollecting feedparser&gt;=5.2.1\n  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81 kB 10.0 MB/s \nRequirement already satisfied: python-dateutil&gt;=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\nRequirement already satisfied: requests&gt;=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\nCollecting jieba3k&gt;=0.35.1\n  Downloading jieba3k-0.35.1.zip (7.4 MB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.4 MB 60.7 MB/s \nRequirement already satisfied: nltk&gt;=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\nCollecting feedfinder2&gt;=0.0.4\n  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\nCollecting cssselect&gt;=0.9.2\n  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\nRequirement already satisfied: beautifulsoup4&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\nCollecting tinysegmenter==0.3\n  Downloading tinysegmenter-0.3.tar.gz (16 kB)\nRequirement already satisfied: PyYAML&gt;=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\nCollecting tldextract&gt;=2.0.1\n  Downloading tldextract-3.1.2-py2.py3-none-any.whl (87 kB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87 kB 5.8 MB/s \nRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2&gt;=0.0.4-&gt;newspaper3k) (1.15.0)\nCollecting sgmllib3k\n  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (3.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (2021.10.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (1.24.3)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (2.10)\nCollecting requests-file&gt;=1.4\n  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\nRequirement already satisfied: filelock&gt;=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract&gt;=2.0.1-&gt;newspaper3k) (3.4.2)\nBuilding wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n  Building wheel for tinysegmenter (setup.py) ... done\n  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13553 sha256=73e9156b3836ce85dc512439196c449be3d5d33749708372688c3f5d99f1a059\n  Stored in directory: /root/.cache/pip/wheels/df/67/41/faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa\n  Building wheel for feedfinder2 (setup.py) ... done\n  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3357 sha256=fe3990b9a89a29307f18a4767d156083b809112c9ca71e5ef2bbb556ddbd874e\n  Stored in directory: /root/.cache/pip/wheels/7f/d4/8f/6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346\n  Building wheel for jieba3k (setup.py) ... done\n  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398404 sha256=86ab131c0f61eeaef932b8967c1915eaaabc4a1089ea070652c21631f839e9b4\n  Stored in directory: /root/.cache/pip/wheels/4c/91/46/3c208287b726df325a5979574324878b679116e4baae1af3c3\n  Building wheel for sgmllib3k (setup.py) ... done\n  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=9ad6b6bbe0b981e0262c6294017917f7b9cb32e8af7463272df81aaf6f7f2001\n  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\nSuccessfully built tinysegmenter feedfinder2 jieba3k sgmllib3k"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#importing-libraries",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#importing-libraries",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "Importing Libraries üß∞",
    "text": "Importing Libraries üß∞\n\nfrom newspaper import Article\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, ImageColorGenerator\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download(\"stopwords\")\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#step-01-getting-the-corpus-from-an-article",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#step-01-getting-the-corpus-from-an-article",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "Step 01: Getting the Corpus from an Article üìù",
    "text": "Step 01: Getting the Corpus from an Article üìù\n\narticle = Article('https://www.repsol.pe/es/sala-prensa/notas-prensa/comunicado.cshtml')\narticle.download()\narticle.parse()\n\n\nGenerate a Simple Word Cloud Image\n\n# Generate a word cloud image\nwc = WordCloud()\nwc.generate(article.text)\n\n# Display the generated image:\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nGenerate a Customized Word Cloud Image\n\nstopwords = set(stopwords.words('spanish', 'english'))\n\nstopwords.update(['ello', 'cinco', 'd√≠a'])\n\nConverting an image to a numpy array results in an array containing a sequence of values that each represent an individual pixel in the image.\n\nmask = np.array(Image.open('repsol.jpg'))\n# mask = np.array(Image.open('pedro-castillo.jpg'))\n\nwc = WordCloud(stopwords=stopwords,\n               background_color=\"white\",\n               max_words=2000,\n               mask=mask,\n               )\n\nwc.generate(article.text)\n\n# Display the generated image:\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nmask = np.array(Image.open('repsol.jpg'))\n# mask = np.array(Image.open('pedro-castillo.jpg'))\n\nwc = WordCloud(stopwords=stopwords,\n               background_color=\"white\",\n               max_words=2000,\n               mask=mask,\n               max_font_size=256,\n            #    random_state=42,\n            #    width=mask.shape[1],\n            #    height=mask.shape[0]\n               )\n\nwc.generate(article.text)\n\n# create coloring from image\nimage_colors = ImageColorGenerator(mask)\nplt.figure(figsize=[16,14])\nplt.imshow(wc.recolor(color_func=image_colors), interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#references",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#references",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "References:",
    "text": "References:\n\nhttps://medium.com/towards-data-science/create-word-cloud-into-any-shape-you-want-using-python-d0b88834bc32\nhttps://www.repsol.pe/es/sala-prensa/notas-prensa/comunicado.cshtml"
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "",
    "text": "¬øAlguna vez te topaste con alguna tabla de Wikipedia que justo necesitabas para tu investigaci√≥n o proyecto de an√°lisis? Como la tabla de medallas de los Juegos Panamericanos, Las Elecciones Presidenciales o datos de las Exportaciones de un Pais, ¬øSi? Bueno, Aqu√≠ aprender√°s como obtener datos de cualquier tabla.\nEsta es la tabla que buscamos obtener que contiene informaci√≥n del Censo de la Provincia de Trujillo del a√±o 2017. Para hacer Web Scraping a esta p√°gina, utilizaremos las bibliotecas Requests y Pandas de Python.\nAl finalizar este Tutorial, tendr√°s como resultado una tabla lista para su an√°lisis. Lo podr√°s guardar en tu directorio de trabajo o descargar en tu computadora como archivo CSV (o el formato que gustes). Espero que te resulte √∫til; sin m√°s que a√±adir‚Ä¶ ‚ÄúHappy Coding!‚Äù"
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-1-conecci√≥n-y-obtenci√≥n-de-codigo-fuente",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-1-conecci√≥n-y-obtenci√≥n-de-codigo-fuente",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "Fase 1: Conecci√≥n y obtenci√≥n de codigo fuente",
    "text": "Fase 1: Conecci√≥n y obtenci√≥n de codigo fuente\nComenzaremos importando la biblioteca requests para realizar la petici√≥n HTTP que nos devolver√° el c√≥digo fuente de la p√°gina web y pandas, para usar su m√©todo .read_html() que nos ayudar√° a extraer todas las tablas HTML. Usaremos este m√©todo y no la biblioteca Beautiful Soup porque de esta forma es mucho m√°s f√°cil y funciona bien en cualquier p√°gina web que contenga tablas HTML debido a su estructura simple de leer.\n\nPara tu conocimiento: Usando .read_html() no solo nos permite extraer tablas HTML desde un enlace (lo que usaremos hoy), sino tambi√©n desde un archivo html o una cadena de caracteres (string) que contenga HTML.\n\n\n# Hacer peticion HTTP\nimport requests\n# Manipular c√≥digo y guardar datos tabulares en archivo CSV\nimport pandas as pd\n\n\n# url de la p√°gina web a ¬´escrapear¬ª\nurl = 'https://es.wikipedia.org/wiki/Provincia_de_Trujillo_(Per%C3%BA)'\n\n# pasar \"User-agent\" para simular interacci√≥n con la p√°gina usando Navegador web\nheaders = {\"User-agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'}\n\nrespuesta = requests.get(url, headers=headers)\n\n# El c√≥digo de respuesta &lt;200&gt; indicar√° que todo sali√≥ bien\nprint(respuesta)\n\n&lt;Response [200]&gt;\n\n\nAparte de la respuesta positiva que nos devolvi√≥ la petici√≥n, seguro te diste cuenta que pasamos el par√°metro headers al m√©todo requests.get(), esto sirve para evitar que el servidor de la p√°gina web nos bloqueen el acceso. Aunque con Wikipedia no tenemos ese problema, otras p√°ginas son restrictivas cuando se trata de bots tratando de ¬´escrapear¬ª sus datos. Siguiento esta buena pr√°ctica, nos ahorarremos dolores de cabeza luego.\nPor √∫ltimo, guardamos en una variable todas las tablas encontradas en el objecto HTML.\n\nall_tables = pd.read_html(respuesta.content, encoding = 'utf8')"
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-2-an√°lisis-de-estructura-html-y-extracci√≥n-de-datos",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-2-an√°lisis-de-estructura-html-y-extracci√≥n-de-datos",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "Fase 2: An√°lisis de estructura HTML y extracci√≥n de datos",
    "text": "Fase 2: An√°lisis de estructura HTML y extracci√≥n de datos\nAhora veamos cuantas tablas hay en la p√°gina web\n\nprint(f'Total de tablas encontradas: {len(all_tables)}')\n\nTotal de tablas encontradas: 6\n\n\nDebido a que hay varias tablas, usaremos el par√°metro match y le pasaremos una palabra o frase que solo se encuentre en la tabla que nos interesa.\n\nmatched_table = pd.read_html(respuesta.text, match='Fuente: Censos Nacionales 2017')\n\n# imprime numero de tablas que coinciden con parametro match\nprint(f'Total de tablas encontradas: {len(matched_table)}')\n\nTotal de tablas encontradas: 1\n\n\nGuardamos nuestra tabla de inter√©s en una variable.\n\n# Guardar tabla en variable con nombre sem√°ntico\ncenso_trujillo = matched_table[0]\n\n# Verificamos si es la tabla que buscamos\ncenso_trujillo.tail(5)\n\n\n\n\n\n\n\n\n\nUBIGEO\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\n\n\n8\n130109\nSalaverry\n5599\n5244\n18¬†944\n\n\n9\n130110\nSimbal\n1662\n1151\n4061\n\n\n10\n130111\nV√≠ctor Larco Herrera\n19¬†543\n18¬†461\n68¬†506\n\n\n11\nNaN\nTOTAL\n273¬†619\n250¬†835\n970¬†016\n\n\n12\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\n\n\n\n\n\n\n\n\n¬°Bien! Es la tabla que buscamos exportar, pero vemos que las 2 √∫ltimas filas no son necesarias, por lo que pasamos a eliminarlas.\n\n# Remover ultima(s) n fila(s)\ncenso_trujillo.drop(censo_trujillo.tail(2).index, inplace=True)\n\n# Verificar si se eliminaron los registros no deseados\ncenso_trujillo.tail(2)\n\n\n\n\n\n\n\n\n\nUBIGEO\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\n\n\n9\n130110\nSimbal\n1662\n1151\n4061\n\n\n10\n130111\nV√≠ctor Larco Herrera\n19¬†543\n18¬†461\n68¬†506\n\n\n\n\n\n\n\n\nAhora asignaremos el UBIGEO como √≠ndice de la tabla.\n\ncenso_trujillo.set_index('UBIGEO', inplace = True)\n\n# Verificamos el cambio de √≠ndice\ncenso_trujillo.head(2)\n\n\n\n\n\n\n\n\n\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\nUBIGEO\n\n\n\n\n\n\n\n\n130101\nTrujillo\n87¬†963\n82¬†236\n314¬†939\n\n\n130102\nEl Porvenir\n57¬†878\n50¬†805\n190¬†461\n\n\n\n\n\n\n\n\nTambi√©n, vemos que en las columnas Hogares,Viviendas y Poblaci√≥n que contienen n√∫meros, hay un espacio en medio de los n√∫meros que le da Wikipedia como formato para mejorar su visualizaci√≥n. Sin embargo, necesitamo que nuestros datos num√©ricos est√©n limpios sin ning√∫n simbolo o espacios para poder realizar operaciones.\nRemovamos ese espacio que no aporta nuestros datos. Para ello usaremos la biblioteca normalize.\n\nfrom unicodedata import normalize\n\nCreamos una funcion y lo correremos a toda nuestra tabla para quitar los molestos espacios en blanco.\n\ndef remove_whitespace(x):\n    \"\"\"Funcion para normalizar datos con Unicode para luego quitar los espacios usando .replace().\n\n    Argumentos de entrada: Nombre de columna o lista con nombres de columnas.\n    Retorna: columna o columnas sin espacios en blanco\n    \"\"\"\n    if isinstance(x, str):\n        return normalize('NFKC', x).replace(' ', '')\n    else:\n        return x\n\nAplicamos la funcion para quitar espacios en blanco a toda las columnas con datos num√©ricos.\n\n# Guardamos en variable nombre de columnas a quitar espacios en blanco\nnumeric_cols = ['Hogares','Viviendas','Poblaci√≥n']\n\n# Aplicar funci√≥n remove_whitespace a columnas en variable y las reemplazamos en tabla\ncenso_trujillo[numeric_cols] = censo_trujillo[numeric_cols].applymap(remove_whitespace)\n\n# Verificamos si se quitaron los espacios en blanco\ncenso_trujillo.head()\n\n\n\n\n\n\n\n\n\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\nUBIGEO\n\n\n\n\n\n\n\n\n130101\nTrujillo\n87963\n82236\n314939\n\n\n130102\nEl Porvenir\n57878\n50805\n190461\n\n\n130103\nFlorencia De Mora\n7777\n8635\n37262\n\n\n130104\nHuanchaco\n20206\n16534\n68409\n\n\n130105\nLa Esperanza\n49773\n47896\n189206\n\n\n\n\n\n\n\n\nAhora veamos los tipos de datos.\n\n# Mostrar tipo de datos de la tabla\ncenso_trujillo.dtypes\n\nDistrito     object\nHogares      object\nViviendas    object\nPoblaci√≥n    object\ndtype: object\n\n\nComo vemos, todos las columnas son de tipo de dato Objeto (lo que Pandas considera como una cadena de caracteres o String). Como Object es muy amplio, necesitamos definir el tipo de dato correcto a cada columna para que luego se realizar operaciones con los datos.\nAqu√≠ va la primera opci√≥n para hacerlo, reusando la variable numeric_cols usada l√≠neas arriba.\n\n# Asignar el tipo de dato num√©rico a columnas en variable\ncenso_trujillo[numeric_cols] = censo_trujillo[numeric_cols].apply(pd.to_numeric)\n\n# Verificamos que las columnas con n√∫meros tengan el tipo de dato num√©rico asignado\ncenso_trujillo.dtypes\n\nDistrito     object\nHogares       int64\nViviendas     int64\nPoblaci√≥n     int64\ndtype: object\n\n\nComo ves, nuestras columnas con n√∫meros ya tienen el formato correcto, pero la columna Distrito a√∫n se mantiene como Object. Aunque no habr√≠a mayor inconveniente, es mejor especificar que datos contiene cada columna.\nAqu√≠ va la seguida opci√≥n para cambiar el tipo de dato a m√∫ltiples columnas.\n\n# Creamos diccionario y pasamos m√∫ltiples columnas con el tipo de dato a asignar\nconvert_dict = {\n    'Distrito': 'string',\n    'Hogares': 'int',\n    'Viviendas': 'int',\n    'Poblaci√≥n': 'int'\n}\n\ncenso_trujillo = censo_trujillo.astype(convert_dict)\n\n# Verificamos que las columnas con n√∫meros tengan el tipo de dato num√©rico asignado\ncenso_trujillo.dtypes\n\nDistrito     string\nHogares       int64\nViviendas     int64\nPoblaci√≥n     int64\ndtype: object"
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-3-guardado-del-conjunto-de-datos",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-3-guardado-del-conjunto-de-datos",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "Fase 3: Guardado del Conjunto de Datos",
    "text": "Fase 3: Guardado del Conjunto de Datos\nPor fin, una ves los datos est√°n limpios y con el tipo de dato correcto, vamos a guardarlos en formato CSV para usarlos luego.\n\n# Guarda Dataframe a archivo CSV\ncenso_trujillo.to_csv('censo_provincia_trujillo_2017.csv')\n\n# Leamos el archivo para verificar su creacion\npd.read_csv('censo_provincia_trujillo_2017.csv').head(3)\n\n\n\n\n\n\n\n\n\nUBIGEO\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\n\n\n0\n130101\nTrujillo\n87963\n82236\n314939\n\n\n1\n130102\nEl Porvenir\n57878\n50805\n190461\n\n\n2\n130103\nFlorencia De Mora\n7777\n8635\n37262\n\n\n\n\n\n\n\n\nSi lo trabajaste en Jupyter Notebook o tu Editor de C√≥digo Favorito, debe estar ubicado en la misma carpeta donde corriste tu archivo .pynb. Pero, si lo trabajaste en Google Colab (como yo), puedes usar la biblioteca files para descargar el archivo en la computadora donde est√©s trabajando.\n\n# Cargar o descargar archivos\nfrom google.colab import files\n\n# Descarga archivo con datos de tabla\nfiles.download(\"censo_provincia_trujillo_2017.csv\")\n\nprint('Listo, en un momento saldr√° la opci√≥n \"Guardar Como\" para descargar el archivo...')\n\n\n\n\n\n\n\nListo, en un momento saldr√° la opci√≥n \"Guardar Como\" para descargar el archivo...\n\n\n¬°Genial! Ya tenemos los datos de una tabla de Wikipedia lista para su uso. Aunque la tabla extraida es peque√±a en dimensi√≥n, esta forma de trabajo puedes aplicarla para extraer cualquier tabla que encuentres interesante, ya sea de Wikipedia o de cualquier otra p√°gina web que contenga tablas HTML."
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#resumiendo-lo-realizado",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#resumiendo-lo-realizado",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "Resumiendo lo realizado",
    "text": "Resumiendo lo realizado\n\nLe√≠mos las tablas HTML de una p√°gina de Wikipedia\nRemovimos los espacios en formato Unicode que imped√≠an la conversi√≥n al tipo de dato correcto\nConvertimos el tipo de dato de todas las columnas al correcto\nGuardamos la tabla extra√≠da como formato csv para su posterior utilizaci√≥n\nFinalmente, descargamos el archivo csv en la computadora de trabajo\n\n¬°Espera! Una cosa m√°s, como est√°mos en el mes de diciembre, te regalo este pensamiento:\n\nSi lo lees y lo entiendes est√° genial, aunque eso no te asegura haberlo aprendido. Para dominarlo necesitas practicarlo.\n\nPor eso, te sugiero que guardes este art√≠culo en tus Favoritos para que lo practiques luego. O si quieres practicarlo ya mismo, aqu√≠ te dejo el art√≠culo en Google Colab, donde podr√°s abrir y correr el c√≥digo sin necesidad de instalar nada, solo tu navegador web y tus ganas de aprender.\nSi te gust√≥, puedes ver mis otras publicaciones, seguro te ser√°n de utilidad. Si gustas apoyarme, comparte este art√≠culo en tus Redes Sociales (Facebook, Linkedint, Twitter) o si est√°s de buen √°nimo, inv√≠tame una taza de caf√© ‚òï. Nos vemos üèÉüí®"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#tldr",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#tldr",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "TL;DR üëÄ",
    "text": "TL;DR üëÄ\nThis project is to perform most common tasks of Web Scraping by using Selenium as a Scraper Tool and Python for coding. The output will be a CSV file containing information of all International Matches of Freestyle organized by Red Bull from 2015 to 2020 (filtered by internacional and vs keywords). Here you can take a peek or download the csv file which is the result of this project. (Also added at the bottom of this notebook)\nFYI: Red Bull Batalla de los Gallos is the Most Recognized Freestyle Competition in Spanish that brings together the 16 winning Freestylers from the competitions organized by Red Bull in each country. After all matches only one of them is crowned as international champion Click here to learn more\nHer I leave you a screenshot of the Youtube Channel used for this project:\n\n\n\nred bull batalla de los gallos 2020.JPG\n\n\nSatisfying the requirements\nAs always, let‚Äôs first install libraries we‚Äôll be using thought the project These ones are Chromium(browser), Selenium (scraper tool), and tqdm (progress bar)."
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#installing-libraries",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#installing-libraries",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Installing Libraries ‚úîÔ∏è",
    "text": "Installing Libraries ‚úîÔ∏è\n\n# install chromium, selenium and tqdm\n!apt update\n!apt install chromium-chromedriver\n!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n!pip install selenium\n!pip install tqdm\n\nprint('Library installation Done!')\n\n0% [Working]            Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Waiting for header                                                                               Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n0% [Connecting to archive.ubuntu.com] [2 InRelease 14.2 kB/88.7 kB 16%] [Waitin                                                                               Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n0% [Connecting to archive.ubuntu.com (91.189.88.152)] [2 InRelease 14.2 kB/88.70% [Connecting to archive.ubuntu.com (91.189.88.152)] [2 InRelease 14.2 kB/88.7                                                                               Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n0% [Connecting to archive.ubuntu.com (91.189.88.152)] [2 InRelease 14.2 kB/88.70% [3 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)                                                                               Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\nHit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\nGet:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\nHit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\nGet:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\nGet:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [41.5 kB]\nHit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\nGet:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\nHit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\nGet:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,700 kB]\nGet:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [870 kB]\nFetched 2,884 kB in 4s (790 kB/s)\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\n17 packages can be upgraded. Run 'apt list --upgradable' to see them.\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\nSuggested packages:\n  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\nThe following NEW packages will be installed:\n  chromium-browser chromium-browser-l10n chromium-chromedriver\n  chromium-codecs-ffmpeg-extra\n0 upgraded, 4 newly installed, 0 to remove and 17 not upgraded.\nNeed to get 81.0 MB of archives.\nAfter this operation, 273 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 87.0.4280.66-0ubuntu0.18.04.1 [1,122 kB]\nGet:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 87.0.4280.66-0ubuntu0.18.04.1 [71.7 MB]\nGet:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 87.0.4280.66-0ubuntu0.18.04.1 [3,716 kB]\nGet:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 87.0.4280.66-0ubuntu0.18.04.1 [4,488 kB]\nFetched 81.0 MB in 5s (15.5 MB/s)\nSelecting previously unselected package chromium-codecs-ffmpeg-extra.\n(Reading database ... 145480 files and directories currently installed.)\nPreparing to unpack .../chromium-codecs-ffmpeg-extra_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\nUnpacking chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\nSelecting previously unselected package chromium-browser.\nPreparing to unpack .../chromium-browser_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\nUnpacking chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\nSelecting previously unselected package chromium-browser-l10n.\nPreparing to unpack .../chromium-browser-l10n_87.0.4280.66-0ubuntu0.18.04.1_all.deb ...\nUnpacking chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\nSelecting previously unselected package chromium-chromedriver.\nPreparing to unpack .../chromium-chromedriver_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\nUnpacking chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\nSetting up chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\nSetting up chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\nupdate-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\nupdate-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\nSetting up chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\nSetting up chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\nProcessing triggers for hicolor-icon-theme (0.17-2) ...\nProcessing triggers for mime-support (3.60ubuntu1) ...\nProcessing triggers for man-db (2.8.3-2ubuntu0.1) ...\ncp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\nCollecting selenium\n  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 911kB 8.4MB/s \nRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\nInstalling collected packages: selenium\nSuccessfully installed selenium-3.141.0\nRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\nLibrary installation Done!"
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#importing-libraries",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#importing-libraries",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Importing Libraries üß∞",
    "text": "Importing Libraries üß∞\nOnce Installed, We‚Äôll procced to import them.\n\n# set options to be headless\nfrom selenium import webdriver\n#the followings are to avoid NoSuchElementException by using WebDriverWait - to wait until an element appears in the DOM\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n# add random pause seconds to avoid getting blocked\nimport time, random\n\n# to use a progress bar for visual feedback\nfrom tqdm import tqdm\n# to get the current date\nfrom datetime import date\n\n# to save Dataframe into a CSV file format\nimport pandas as pd\nimport numpy as np\n\n# Upload or download files\nfrom google.colab import files\n\nprint('All Libraries imported!')\n\nAll Libraries imported!"
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-01-accessing-the-web-page",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-01-accessing-the-web-page",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Phase 01: Accessing the Web Page üåê",
    "text": "Phase 01: Accessing the Web Page üåê\n\nOpening the Browser and Visiting the Target Web Page\n\n# Setting options for the web browser\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('-headless')\nchrome_options.add_argument('-no-sandbox')\nchrome_options.add_argument('-disable-dev-shm-usage')\n\n# Open browser, go to a website, and get results\nbrowser = webdriver.Chrome('chromedriver',options=chrome_options)\nbrowser.execute_script(\"return navigator.userAgent;\")\nprint(browser.execute_script(\"return navigator.userAgent;\"))\n\nchannel_url = 'https://www.youtube.com/c/RedbullOficialGallos/videos'\n\n# Open website\nbrowser.get(channel_url)\n\n# Print page title\nprint(browser.title)\n\nMozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/87.0.4280.66 Safari/537.36\nRed Bull Batalla De Los Gallos - YouTube\n\n\n\n\nReaching the bottom of this Dynamically Loaded Page\nSince this Page‚Äôs content is dinamically loaded by scrolling down, we use a function to dinamically change the scrollHeight.\n\ndef scroll_to_the_page_bottom(browser):\n    height = browser.execute_script(\"return document.documentElement.scrollHeight\")\n    lastheight = 0\n\n    while True:\n        if lastheight == height:\n            break\n        lastheight = height\n        browser.execute_script(\"window.scrollTo(0, \" + str(height) + \");\")\n        # Pause 2 seconds per iteration\n        time.sleep(2)\n        height = browser.execute_script(\"return document.documentElement.scrollHeight\")\n\n    print('The scroll down reached the bottom of the page, all content loaded!')\n\nscroll_to_the_page_bottom(browser)\n\nThe scroll down reached the bottom of the page, all content loaded!"
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-02-scraping-the-data",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-02-scraping-the-data",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Phase 02: Scraping the data ‚õèÔ∏è",
    "text": "Phase 02: Scraping the data ‚õèÔ∏è\n\nGetting the links of all videos\n\nvideo_anchors = browser.find_elements_by_css_selector('#video-title')\n\nprint(f'This Channel has {len(video_anchors)} videos published')\n\nThis Channel has 3226 videos published\n\n\nFor this project, we‚Äôre gonna gather all the videos link that contains the words: - internacional + vs\nTo do so, we‚Äôll use a list comprehension along with all().\nWe‚Äôre using all() instead of any because we want to filter having all elements present inside each text item. Think about it as the and operator.the any() method then would be like any, because any text item that match at least one of the matches would be inserted in the list called video_links.\n\n# initializing  list of keywords to filter (16 videos only should be)\n\nmatchers = [x.lower() for x in ['Internacional', 'vs']]\nvideo_links = [link.get_attribute('href') for link in tqdm(video_anchors, position=0) if all(match in link.text.lower() for match in matchers)]\n\nprint(len(video_links))\n\n#Show the first link\nvideo_links[0]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3226/3226 [02:30&lt;00:00, 21.44it/s]\n\n\n95\n\n\n'https://www.youtube.com/watch?v=Fwda4AWZ6V4'\n\n\n\n\nGetting all details for each video\nNow, we are going to retrieve the details of all videos we are interested in such us title, views, upload date, lenght of video,likes and dislikes. We‚Äôll use a for loop to iterate over the video_links variable which contains all videos‚Äô urls and per each url we extract the data and save them in variables.\nOnce saved the collection or variables are stored in a Dictionary called data. Finally each dictionary are saved in the variable video_details which basically is a list of dictionaries containing all details per each video scraped. Let‚Äôs jump in the code to better understanding.\n\nvideo_details = []\n\ndelay = 10\n\nfor link in tqdm(video_links, desc='Getting all details for each video', position=0, leave=True):\n\n    try:\n        browser.get(link)\n    except:\n        continue\n\n    # Pause 3 seconds to load content\n    time.sleep(3)\n\n    # Get element  after explicitly waiting for up to 10 seconds\n    title = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , '.title'))).text\n    views = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , '.view-count'))).text.split('\\n')[0].split()[0]\n    upload_date = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , '#date &gt; yt-formatted-string'))).text\n    length = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , '.ytp-time-duration'))).text\n    likes = WebDriverWait(browser, delay).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR , '#top-level-buttons #text')))[0].get_attribute('aria-label').split()[0]\n    dislikes = WebDriverWait(browser, delay).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR , '#top-level-buttons #text')))[1].get_attribute('aria-label').split()[0]\n    url = link\n\n    # inserting all data in the list. We'll also use aternary expression/operator to save a value depending on a condition\n    data = {\n            'title': title,\n            'views': views,\n            'upload_date': upload_date,\n            'length': length,\n            'likes': likes,\n            'dislikes': dislikes,\n            'url': url\n            }\n\n    video_details.append(data)\n\n    # Pause 3 seconds per iteration\n    time.sleep(3)\n\n# Close the browser once the for loop is done\nbrowser.quit()\n\nprint(f'All details of {len(video_links)} videos successfully retrieved')\n\nGetting all details for each video: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [11:53&lt;00:00,  7.51s/it]\n\n\nAll details of 95 videos successfully retrieved\n\n\nExcellent, we just got all videos details and insert them into a list called video_details for convinence.\nTo verify the details per each video were saved correctly let‚Äôs print the first element whitin the list.\n\nvideo_details[0]\n\n{'dislikes': '270',\n 'length': '6:16',\n 'likes': '14,040',\n 'title': 'ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | Red Bull Internacional 2020',\n 'upload_date': 'Dec 12, 2020',\n 'url': 'https://www.youtube.com/watch?v=Fwda4AWZ6V4',\n 'views': '577,503'}"
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-03-saving-the-gathered-data",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-03-saving-the-gathered-data",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Phase 03: Saving the Gathered Data üíæ",
    "text": "Phase 03: Saving the Gathered Data üíæ\n\nSaving data to a CSV file\nTo dynamically name our output csv file, we‚Äôll use from datetime import date which is already imported in the Importing Libraries Section. Let‚Äôs first get the current date and the Youtube Channel‚Äôs Name from the url we provided.\n\ntoday = date.today()\n\n# Month abbreviation, day and year\ntodays_date = today.strftime(\"%b-%d-%Y\")\nprint(f'Fecha de hoy: {todays_date}')\n\nchannel_name = channel_url.split('/')[4]\nprint(channel_name)\n\nFecha de hoy: Dec-27-2020\nRedbullOficialGallos\n\n\nNow, let‚Äôs put all variables together to name the file.\n\n# Programatically naming csv file\ncsv_file_name = f'{channel_name}_videos_details_{todays_date}.csv'.lower()\nprint(csv_file_name)\n\n# Assign columns names\nfield_names = ['title', 'views', 'upload_date', 'length', 'likes', 'dislikes', 'url']\n\nredbulloficialgallos_videos_details_dec-27-2020.csv\n\n\nWe‚Äôre almost done, with the csv_file_name and field_names variables, let‚Äôs turn video_details into a Dataframe which can be used later for any analysis. We‚Äôll need to install Pandas and Numpy to do so. These libraries were already imported in the Importing Libraries Section.\n\n# Create DataFrame\ndf = pd.DataFrame(video_details, columns=field_names)\n\n# Show first 3 rows to verify the dataframe creation\ndf.head(3)\n\n\n\n\n\n\n\n\n\ntitle\nviews\nupload_date\nlength\nlikes\ndislikes\nurl\n\n\n\n\n0\nACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R...\n577,503\nDec 12, 2020\n6:16\n14,040\n270\nhttps://www.youtube.com/watch?v=Fwda4AWZ6V4\n\n\n1\nEXODO LIRICAL vs RAPDER - Semifinal | Red Bull...\n238,463\nDec 12, 2020\n12:30\n8,135\n927\nhttps://www.youtube.com/watch?v=wIcz1_7qx-4\n\n\n2\nACZINO vs SKONE - Semifinal | Red Bull Interna...\n756,352\nDec 12, 2020\n10:06\n18,458\n1,146\nhttps://www.youtube.com/watch?v=yv8yFhRsWVc\n\n\n\n\n\n\n\n\n\n# Save Dataframe into a CSV file format\ndf.to_csv(csv_file_name, index=False)\n\n# Read the file and print the first 3 rows to verify its creation\npd.read_csv(csv_file_name).head(3)\n\n\n\n\n\n\n\n\n\ntitle\nviews\nupload_date\nlength\nlikes\ndislikes\nurl\n\n\n\n\n0\nACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R...\n577,503\nDec 12, 2020\n6:16\n14,040\n270\nhttps://www.youtube.com/watch?v=Fwda4AWZ6V4\n\n\n1\nEXODO LIRICAL vs RAPDER - Semifinal | Red Bull...\n238,463\nDec 12, 2020\n12:30\n8,135\n927\nhttps://www.youtube.com/watch?v=wIcz1_7qx-4\n\n\n2\nACZINO vs SKONE - Semifinal | Red Bull Interna...\n756,352\nDec 12, 2020\n10:06\n18,458\n1,146\nhttps://www.youtube.com/watch?v=yv8yFhRsWVc\n\n\n\n\n\n\n\n\nYay! You reach the end of this article. By now you know how retrieve all videos details from a Youtube Channel. As earlier mentioned, the scraped data should be in the generated csv file. If you worked on it in Jupyter Notebook or your Favorite Code Editor, you can find it in the same folder where you ran your¬†.pynb file. But, if you worked on Google Colab (like me), you need to use the following code to download it: from google.colab import files. This library was already imported in the Importing Libraries Section\n\n# Download the file that contains the scraped table\nfiles.download(csv_file_name)\n\nprint('In a moment the option \"Save As\" will appear to download the file...')\n\n\n\n\n\n\n\nIn a moment the option \"Save As\" will appear to download the file..."
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#takeaways",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#takeaways",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Takeaways",
    "text": "Takeaways\n\nSince Youtube is a loading content Page, I‚Äôve decided to use Selenium as a tool to scrape\nWhen scraping the video_lenght of a video, For some reason some of them return a None value, so we need to use its text version from the arial-label of the same element\nI‚Äôve decided to use Pandas instead of the CSV library to create and save the Dataframe into a CSV file because it‚Äôs easier to use.\nThe elements were accesed using its css selector because is faster and easier to read\nThis project is to show off skills of Web Scraping using Selenium. For the next tutorial, we‚Äôll do the same but using the Youtube API\nSince this project‚Äôs scope is just to gather all data needed in a machine readable format (CSV). What remains to be done is Data Preprocessing and Exploratory Data Analysis\n\nHere I leave you the csv file we‚Äôve just scraped from Youtube."
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#references",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#references",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "References",
    "text": "References\nThis is where I got inspiration from\nHow to Extract & Analyze YouTube Data using YouTube API?\nUsing Selenium wthinin Google Colab\nScroll to end of page in dynamically loading webpage. Answered by: user53558\nSaving a Pandas Dataframe as a CSV\nScroll to end of page in dynamically loading webpage\nAsign variables to dictionary based on value\nWebDriverWait on finding element by CSS Selector\nUse of if else inside a dict to set a value to key using Python\nHow to get back to the for loop after exception handling\ntqdm printing to newline"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#tldr",
    "href": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#tldr",
    "title": "Predicting Wild Blueberry Yield Using Supervised Machine Learning",
    "section": "TL;DR ü§ì",
    "text": "TL;DR ü§ì\nIn this project, I went through many steps to accomplish the main goal: Build a ML Model to predict how much of Wild Blueberry will it yield based on the combinations of some features. I trained & evaluate 06 ML Algorithms for this task: Ridge, LinearRegression, SVR, DecisionTreeRegressor, RandomForestRegressor, XGBRegressor . In the end, the chosen one which provided the best results was RandomForestRegressor.\nThere was not much to do in terms of pre-processing since all features were numerical. I just check for missing values & Outliers to validate it‚Äôs a tidy dataset.\nNext, I make some Exploratory Data Analysis to understand a bit more about the target (univariable Analysis) and the relation with its features (multivariable Analysis).\nThen, I standarize by scaling all features in order to surpress the impact of values with high magnitude.\nAfter all this, I split the data and trained the 06 models. Then, I use k-fold cross validation to compared them out to find out wich on gives better results. At the end, XGBRegressor ended up being the best model, so I try to enhance it by finding the best params combination, I included them and fit it for the last time.\nFinally, I use the trained model on the test data provided and save its predictions. I also save the model to reuse it later. For explainability purpose, I plot a bar chart to show the ranking of the most important features for the model and its yield prediction.\nHappy learning! üòä"
  },
  {
    "objectID": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#problem-undestanding",
    "href": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#problem-undestanding",
    "title": "Predicting Wild Blueberry Yield Using Supervised Machine Learning",
    "section": "Problem Undestanding ‚ùì",
    "text": "Problem Undestanding ‚ùì\n\nWhat‚Äôs the problem? ü§î\nThe goal is to train a model that can predict Wild Blueberry Yield, This, will return how much yield is expected based on certain attributes that might influence it. Since the Amount of Wild Blueberry to be yielded is a continuous variable, we are dealing with a Regression Problem.\nData columns:\n\nClonesize m2: average blueberry clone size in the field\nHoneybee bees/m2/min: Honeybee density in the field\nBumbles bees/m2/min: Bumblebee density in the field\nAndrena bees/m2/min: Andrena bee density in the field\nOsmia bees/m2/min: Osmia bee density in the field\nMaxOfUpperTRange ‚ÑÉ: highest record of the upper band daily air temperature during the bloom season\nMinOfUpperTRange ‚ÑÉ: lowest record of the upper band daily air temperature\nAverageOfUpperTRange ‚ÑÉ: average of the upper band daily air temperature\nMaxOfLowerTRange ‚ÑÉ: highest record of the lower band daily air temperature\nMinOfLowerTRange ‚ÑÉ: lowest record of the lower band daily air temperature\nAverageOfLowerTRange ‚ÑÉ: average of the lower band daily air temperature\nRainingDays Day: total number of days during the bloom season, each of which has precipitation larger than zero\nAverageRainingDays Day: average of raining days of the entire bloom season\n\n\n\nWhat & How to measure KPI? üéØ\nR-Squared (R¬≤ or the coefficient of determination) is the choosen metric for evaluating how good the model is at predicting Wild Blueberry Yield. But I also used Mean Squared Error & Root Mean Squared Error for comparison purpuses.\n\nR^2 score shows how well terms (data points) fit a curve or line. R^2 is a statistical measure between 0 and 1 which calculates how similar a regression line is to the data it‚Äôs fitted to."
  },
  {
    "objectID": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#setting-things-up",
    "href": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#setting-things-up",
    "title": "Predicting Wild Blueberry Yield Using Supervised Machine Learning",
    "section": "Setting Things Up ‚öô",
    "text": "Setting Things Up ‚öô\n\nImporting Libraries üß∞\nHere it‚Äôs imported all the libraries requiered for each process of the project.\n\n# basic libraries\nimport numpy as np\nimport pandas as pd\nimport requests\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# For Identifing Outliers\nfrom scipy import stats\n\n# For Identifing Correlation Coefficient & P-value\nfrom scipy.stats import pearsonr\n\n# To Check Missing Values\nimport missingno as msno\n\n# To avoi warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# For Data Engineering\nfrom sklearn.preprocessing import StandardScaler\n\n# For Splitting data\nfrom sklearn.model_selection import train_test_split\n\n\n# For Building Predicting Models\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n\n# For Evaluating Predictive Models Using Metrics\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# For Optimizing any Model by Finding the Best Hyperparameters\nfrom sklearn.model_selection import GridSearchCV\n\n# For Evaluating Predictive Models By Using K-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score, KFold\n\n# For Saving the model\nimport joblib\n\n\n\nCustomized Settings üé®\nBellow, there is a function that customizes some of the default graph parameters to make them look a bit cleaner and easy to digest.\n\ng=''\n\ndef ChartWithCustomSettings(graph=g, figW=6, figH=4, title='Title'):\n    graph.figure.set_figwidth(figW)\n    graph.figure.set_figheight(figH)\n\n    sns.set_theme(style='white', palette='blend:#464196,#d9d9d9')\n    sns.despine() #left=True\n    g.set(ylabel=None)\n    plt.title(title, pad=15)\n\nThis chunk of code just show a pallet with the colors used in this projec‚Äôs graphs\n\nsns.palplot(['#221f1f', '#d9d9d9', '#464196'])\nplt.title('Custom Palette', loc='left', fontfamily='Sans Serif', fontsize=12, y=1.2)\nplt.show()"
  },
  {
    "objectID": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#extract-load-data",
    "href": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#extract-load-data",
    "title": "Predicting Wild Blueberry Yield Using Supervised Machine Learning",
    "section": "Extract & Load Data üóÉÔ∏è",
    "text": "Extract & Load Data üóÉÔ∏è\nHere you can find the Dataset used in this project: https://www.kaggle.com/competitions/playground-series-s3e14/overview\nThe dataset came in two files containing Training and Test Data\n\n# Loading Training and Test Data\ntrain_data = pd.read_csv('train.csv')\ntest_data = pd.read_csv('test.csv')\n\ntrain_data.head(3)\n\n\n\n  \n    \n\n\n\n\n\n\nid\nclonesize\nhoneybee\nbumbles\nandrena\nosmia\nMaxOfUpperTRange\nMinOfUpperTRange\nAverageOfUpperTRange\nMaxOfLowerTRange\nMinOfLowerTRange\nAverageOfLowerTRange\nRainingDays\nAverageRainingDays\nfruitset\nfruitmass\nseeds\nyield\n\n\n\n\n0\n0\n25.0\n0.50\n0.25\n0.75\n0.50\n69.7\n42.1\n58.2\n50.2\n24.3\n41.2\n24.0\n0.39\n0.425011\n0.417545\n32.460887\n4476.81146\n\n\n1\n1\n25.0\n0.50\n0.25\n0.50\n0.50\n69.7\n42.1\n58.2\n50.2\n24.3\n41.2\n24.0\n0.39\n0.444908\n0.422051\n33.858317\n5548.12201\n\n\n2\n2\n12.5\n0.25\n0.25\n0.63\n0.63\n86.0\n52.0\n71.9\n62.0\n30.0\n50.8\n24.0\n0.39\n0.552927\n0.470853\n38.341781\n6869.77760"
  },
  {
    "objectID": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#data-pre-processing",
    "href": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#data-pre-processing",
    "title": "Predicting Wild Blueberry Yield Using Supervised Machine Learning",
    "section": "Data Pre-processing üßº",
    "text": "Data Pre-processing üßº\n\nChecking For Missing Values\nLet‚Äôs first check the number of Missing Values for each feature\n\n# Show Total Missing Values in Percentage Format\ntrain_data.isnull().sum()/train_data.shape[0]*100\n\nid                      0.0\nclonesize               0.0\nhoneybee                0.0\nbumbles                 0.0\nandrena                 0.0\nosmia                   0.0\nMaxOfUpperTRange        0.0\nMinOfUpperTRange        0.0\nAverageOfUpperTRange    0.0\nMaxOfLowerTRange        0.0\nMinOfLowerTRange        0.0\nAverageOfLowerTRange    0.0\nRainingDays             0.0\nAverageRainingDays      0.0\nfruitset                0.0\nfruitmass               0.0\nseeds                   0.0\nyield                   0.0\ndtype: float64\n\n\nWe can also use a plot to visualize Missing Values\n\nmsno.matrix(train_data, figsize=(10, 3))\n\n\n\n\n\n\n\n\n\n\nChecking For Proper Data Type\n\ntrain_data.dtypes\n\nid                        int64\nclonesize               float64\nhoneybee                float64\nbumbles                 float64\nandrena                 float64\nosmia                   float64\nMaxOfUpperTRange        float64\nMinOfUpperTRange        float64\nAverageOfUpperTRange    float64\nMaxOfLowerTRange        float64\nMinOfLowerTRange        float64\nAverageOfLowerTRange    float64\nRainingDays             float64\nAverageRainingDays      float64\nfruitset                float64\nfruitmass               float64\nseeds                   float64\nyield                   float64\ndtype: object\n\n\nLet‚Äôs figuring out what features are numerical or categorical. Beware that some integer-valued features could actually be categorical features, and some categorical features could be text features.\n\nnumerical_features = train_data.select_dtypes(include=np.number).columns\nprint('Numerical columns:', numerical_features)\n\nprint('')\n\ncategorical_features = train_data.select_dtypes(include='object').columns\nprint('Categorical columns:', categorical_features)\n\nNumerical columns: Index(['id', 'clonesize', 'honeybee', 'bumbles', 'andrena', 'osmia',\n       'MaxOfUpperTRange', 'MinOfUpperTRange', 'AverageOfUpperTRange',\n       'MaxOfLowerTRange', 'MinOfLowerTRange', 'AverageOfLowerTRange',\n       'RainingDays', 'AverageRainingDays', 'fruitset', 'fruitmass', 'seeds',\n       'yield'],\n      dtype='object')\n\nCategorical columns: Index([], dtype='object')\n\n\nSeems like all features are numerical, and based on its values, we can confirm they have the proper data type. Let‚Äôs move on to the next part.\n\n\nIdentifing Outliers\nThere are a few values lying outside a normal range\n\ng=sns.catplot(data=train_data, y='yield', kind='violin')\nChartWithCustomSettings(g, 3, 5, 'Yield Distribution')\n\n\n\n\n\n\n\n\nLet‚Äôs use Z-score to check if they are above the usual threshold level\n\nz= np.abs(stats.zscore(train_data['yield']))\n\noutliers = len(np.where(z &gt;= 3)[0])\n\nprint(f'There were found {outliers} outliers')\n\nThere were found 26 outliers\n\n\n\n\nBackup\n\ndata_pre_processed = train_data.copy()"
  },
  {
    "objectID": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#exploratory-data-analysis",
    "href": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#exploratory-data-analysis",
    "title": "Predicting Wild Blueberry Yield Using Supervised Machine Learning",
    "section": "Exploratory Data Analysis üìä",
    "text": "Exploratory Data Analysis üìä\nIn this section we analized in three ways: - General statistics of the dataset - Visualizing Each Feature (Univariate Analysis) - Visualizing multiple Features at the same time (Multivariate Analysis) and its correlation\n\nGeneral View of the Dataset üßÆ\n\ndata_pre_processed.sample(5)\n\n\n\n  \n    \n\n\n\n\n\n\nid\nclonesize\nhoneybee\nbumbles\nandrena\nosmia\nMaxOfUpperTRange\nMinOfUpperTRange\nAverageOfUpperTRange\nMaxOfLowerTRange\nMinOfLowerTRange\nAverageOfLowerTRange\nRainingDays\nAverageRainingDays\nfruitset\nfruitmass\nseeds\nyield\n\n\n\n\n11553\n11553\n25.0\n0.50\n0.25\n0.50\n0.63\n86.0\n52.0\n71.9\n62.0\n30.0\n50.8\n24.0\n0.39\n0.466279\n0.434133\n34.660573\n5329.26320\n\n\n3198\n3198\n25.0\n0.50\n0.25\n0.50\n0.63\n94.6\n57.2\n79.0\n68.2\n33.0\n55.9\n24.0\n0.39\n0.481329\n0.438613\n34.971315\n5504.75083\n\n\n8536\n8536\n25.0\n0.50\n0.25\n0.63\n0.50\n86.0\n52.0\n71.9\n62.0\n30.0\n50.8\n16.0\n0.26\n0.583698\n0.494589\n40.351186\n5200.57849\n\n\n15125\n15125\n12.5\n0.25\n0.25\n0.63\n0.63\n69.7\n42.1\n58.2\n50.2\n24.3\n41.2\n16.0\n0.26\n0.467905\n0.411272\n33.221987\n5807.00693\n\n\n4824\n4824\n25.0\n0.50\n0.38\n0.63\n0.75\n86.0\n52.0\n71.9\n62.0\n30.0\n50.8\n24.0\n0.39\n0.559215\n0.476536\n38.171597\n7778.34916\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\ndata_pre_processed.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 15289 entries, 0 to 15288\nData columns (total 18 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   id                    15289 non-null  int64  \n 1   clonesize             15289 non-null  float64\n 2   honeybee              15289 non-null  float64\n 3   bumbles               15289 non-null  float64\n 4   andrena               15289 non-null  float64\n 5   osmia                 15289 non-null  float64\n 6   MaxOfUpperTRange      15289 non-null  float64\n 7   MinOfUpperTRange      15289 non-null  float64\n 8   AverageOfUpperTRange  15289 non-null  float64\n 9   MaxOfLowerTRange      15289 non-null  float64\n 10  MinOfLowerTRange      15289 non-null  float64\n 11  AverageOfLowerTRange  15289 non-null  float64\n 12  RainingDays           15289 non-null  float64\n 13  AverageRainingDays    15289 non-null  float64\n 14  fruitset              15289 non-null  float64\n 15  fruitmass             15289 non-null  float64\n 16  seeds                 15289 non-null  float64\n 17  yield                 15289 non-null  float64\ndtypes: float64(17), int64(1)\nmemory usage: 2.1 MB\n\n\n\ndata_pre_processed.describe().T\n\n\n\n  \n    \n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nid\n15289.0\n7644.000000\n4413.698468\n0.000000\n3822.000000\n7644.000000\n11466.000000\n15288.000000\n\n\nclonesize\n15289.0\n19.704690\n6.595211\n10.000000\n12.500000\n25.000000\n25.000000\n40.000000\n\n\nhoneybee\n15289.0\n0.389314\n0.361643\n0.000000\n0.250000\n0.500000\n0.500000\n18.430000\n\n\nbumbles\n15289.0\n0.286768\n0.059917\n0.000000\n0.250000\n0.250000\n0.380000\n0.585000\n\n\nandrena\n15289.0\n0.492675\n0.148115\n0.000000\n0.380000\n0.500000\n0.630000\n0.750000\n\n\nosmia\n15289.0\n0.592355\n0.139489\n0.000000\n0.500000\n0.630000\n0.750000\n0.750000\n\n\nMaxOfUpperTRange\n15289.0\n82.169887\n9.146703\n69.700000\n77.400000\n86.000000\n86.000000\n94.600000\n\n\nMinOfUpperTRange\n15289.0\n49.673281\n5.546405\n39.000000\n46.800000\n52.000000\n52.000000\n57.200000\n\n\nAverageOfUpperTRange\n15289.0\n68.656256\n7.641807\n58.200000\n64.700000\n71.900000\n71.900000\n79.000000\n\n\nMaxOfLowerTRange\n15289.0\n59.229538\n6.610640\n50.200000\n55.800000\n62.000000\n62.000000\n68.200000\n\n\nMinOfLowerTRange\n15289.0\n28.660553\n3.195367\n24.300000\n27.000000\n30.000000\n30.000000\n33.000000\n\n\nAverageOfLowerTRange\n15289.0\n48.568500\n5.390545\n41.200000\n45.800000\n50.800000\n50.800000\n55.900000\n\n\nRainingDays\n15289.0\n18.660865\n11.657582\n1.000000\n16.000000\n16.000000\n24.000000\n34.000000\n\n\nAverageRainingDays\n15289.0\n0.324176\n0.163905\n0.060000\n0.260000\n0.260000\n0.390000\n0.560000\n\n\nfruitset\n15289.0\n0.502741\n0.074390\n0.192732\n0.458246\n0.506600\n0.560445\n0.652144\n\n\nfruitmass\n15289.0\n0.446553\n0.037035\n0.311921\n0.419216\n0.446570\n0.474134\n0.535660\n\n\nseeds\n15289.0\n36.164950\n4.031087\n22.079199\n33.232449\n36.040675\n39.158238\n46.585105\n\n\nyield\n15289.0\n6025.193999\n1337.056850\n1945.530610\n5128.163510\n6117.475900\n7019.694380\n8969.401840\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\ndata_pre_processed.nunique()\n\nid                      15289\nclonesize                   6\nhoneybee                    7\nbumbles                    11\nandrena                    16\nosmia                      14\nMaxOfUpperTRange            6\nMinOfUpperTRange            5\nAverageOfUpperTRange        5\nMaxOfLowerTRange            6\nMinOfLowerTRange            7\nAverageOfLowerTRange        5\nRainingDays                 6\nAverageRainingDays          8\nfruitset                 1526\nfruitmass                1515\nseeds                    2066\nyield                     776\ndtype: int64\n\n\n\n\nLet‚Äôs Analyze each Feature üí°\nUnivariate Analysis\nWe can observe that Yield (the target) has a slightly left-skewed distribution\n\ng=sns.histplot(data_pre_processed['yield'], bins=20, kde=True, stat='density', linewidth=0)\nChartWithCustomSettings(g, 6, 4, 'Yield Distribution')\nplt.xlabel('')\n\n# Ploting the mean line\nmean = data_pre_processed['yield'].mean()\n\nplt.axvline(mean, 0, 1, color='lightgray', label='mean')\n\n\n\n\n\n\n\n\n\n\nHow They Behave if We Put Them Together? ü§î\nBivariate Analysis\n\nNumerical Predictors and Target\n\ng=sns.catplot(data=data_pre_processed, x='RainingDays', y='yield', estimator=np.mean, kind='violin')\nChartWithCustomSettings(g, 5, 4, 'Yield Distribution By RainingDays')\n\n\n\n\n\n\n\n\nNow let‚Äôs visualize the correlation between each pair of variable.\nAs we can see, these features 'MaxOfUpperTRange', 'MinOfLowerTRange', 'AverageRainingDays' have high correlation almost or equal to 1. Let‚Äôs leave them for now, but keep in mind that these can be removed to avoid adding complexity to the model.\n\n# Calculate correlation between each pair of variable\ncorr = data_pre_processed.drop('id', axis=1).corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Inserir a figura\nf, ax = plt.subplots(figsize=(11, 11))\ncmap = sns.diverging_palette(10, 220, as_cmap=True)\n\n# Draw the heatmap with the mask\nax = sns.heatmap(corr,\n                 mask=mask,\n                 cmap=cmap,\n                 center=0,\n                 annot=True,\n                 annot_kws= {'size':8},\n                 square=True,\n                 xticklabels=True,\n                 yticklabels=True,\n                 linewidths=.5,\n                 cbar_kws={'shrink': .5},\n                 ax=ax\n                )\n\nax.set_title('Correlation between variables', fontsize=12);\n\n\n\n\n\n\n\n\nWe can observe there is a positive correlation between Fruitset and Yield\n\nsns.regplot(data=data_pre_processed, x='fruitset', y='yield', line_kws={'color': '#221f1f'})\nChartWithCustomSettings(g, 5, 4, 'Fruitset vs. Yield')\nplt.xlabel('Fruitset')\nplt.ylabel('Yield')\n\nText(0, 0.5, 'Yield')\n\n\n\n\n\n\n\n\n\nNow, let‚Äôs find out what‚Äôs the coefficient between fruitset and yield\n\nhigh_correlation_features = data_pre_processed[['seeds', 'fruitset', 'fruitmass']]\ntarget_var = 'yield'\n\nfor col in high_correlation_features:\n    coefficent, p_value = pearsonr(high_correlation_features[col], data_pre_processed[target_var])\n\n# print(coefficent.round(3), p_value)\nprint(f\"The correlation coefficient between {col} and {target_var} is {coefficent.round(3)} and the pvlaue is {p_value}\")\nprint()\n\nThe correlation coefficient between fruitmass and yield is 0.826 and the pvlaue is 0.0"
  },
  {
    "objectID": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#feature-transformation",
    "href": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#feature-transformation",
    "title": "Predicting Wild Blueberry Yield Using Supervised Machine Learning",
    "section": "Feature Transformation üî™",
    "text": "Feature Transformation üî™\nOnce we‚Äôre done with EDA, we can start shaping the dataset ready to be used in the models\n\nRemove Unnecesary Columns\nFirst, let‚Äôs remember the dataset structure\n\n# Show a sample of the current dataset\ndata_pre_processed.sample(3)\n\n\n\n  \n    \n\n\n\n\n\n\nid\nclonesize\nhoneybee\nbumbles\nandrena\nosmia\nMaxOfUpperTRange\nMinOfUpperTRange\nAverageOfUpperTRange\nMaxOfLowerTRange\nMinOfLowerTRange\nAverageOfLowerTRange\nRainingDays\nAverageRainingDays\nfruitset\nfruitmass\nseeds\nyield\n\n\n\n\n9799\n9799\n12.5\n0.25\n0.25\n0.75\n0.63\n77.4\n46.8\n64.7\n55.8\n27.0\n45.8\n24.0\n0.39\n0.567976\n0.480779\n39.646691\n7163.94410\n\n\n11565\n11565\n12.5\n0.25\n0.25\n0.25\n0.75\n86.0\n52.0\n71.9\n62.0\n30.0\n50.8\n24.0\n0.39\n0.447781\n0.424874\n33.116091\n5085.84473\n\n\n11829\n11829\n25.0\n0.50\n0.25\n0.38\n0.75\n86.0\n52.0\n71.9\n62.0\n30.0\n50.8\n34.0\n0.56\n0.462743\n0.428118\n33.441606\n5772.91016\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nEverything but id column are okey. Let‚Äôs remove it\n\n# Remove unnecesary columns\ndata_pre_processed.drop(columns=['id'], inplace=True)\ndata_pre_processed.head(3)\n\n\n\n  \n    \n\n\n\n\n\n\nclonesize\nhoneybee\nbumbles\nandrena\nosmia\nMaxOfUpperTRange\nMinOfUpperTRange\nAverageOfUpperTRange\nMaxOfLowerTRange\nMinOfLowerTRange\nAverageOfLowerTRange\nRainingDays\nAverageRainingDays\nfruitset\nfruitmass\nseeds\nyield\n\n\n\n\n0\n25.0\n0.50\n0.25\n0.75\n0.50\n69.7\n42.1\n58.2\n50.2\n24.3\n41.2\n24.0\n0.39\n0.425011\n0.417545\n32.460887\n4476.81146\n\n\n1\n25.0\n0.50\n0.25\n0.50\n0.50\n69.7\n42.1\n58.2\n50.2\n24.3\n41.2\n24.0\n0.39\n0.444908\n0.422051\n33.858317\n5548.12201\n\n\n2\n12.5\n0.25\n0.25\n0.63\n0.63\n86.0\n52.0\n71.9\n62.0\n30.0\n50.8\n24.0\n0.39\n0.552927\n0.470853\n38.341781\n6869.77760\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n\n\nScaling Data\nStandardize the dataset using Feature Scaling\nAs we see in the Distribution Charts, there are some values with high magnitude which could cause data leakage. So, to supress this effect, we need to normalize its range of all numerical features. This can be acheived by using Feature Scaling\nBut first, let‚Äôs make a backup copy\n\nbefore_scaling_data = data_pre_processed.copy()\nbefore_scaling_data.head(3)\n\n\n\n  \n    \n\n\n\n\n\n\nclonesize\nhoneybee\nbumbles\nandrena\nosmia\nMaxOfUpperTRange\nMinOfUpperTRange\nAverageOfUpperTRange\nMaxOfLowerTRange\nMinOfLowerTRange\nAverageOfLowerTRange\nRainingDays\nAverageRainingDays\nfruitset\nfruitmass\nseeds\nyield\n\n\n\n\n0\n25.0\n0.50\n0.25\n0.75\n0.50\n69.7\n42.1\n58.2\n50.2\n24.3\n41.2\n24.0\n0.39\n0.425011\n0.417545\n32.460887\n4476.81146\n\n\n1\n25.0\n0.50\n0.25\n0.50\n0.50\n69.7\n42.1\n58.2\n50.2\n24.3\n41.2\n24.0\n0.39\n0.444908\n0.422051\n33.858317\n5548.12201\n\n\n2\n12.5\n0.25\n0.25\n0.63\n0.63\n86.0\n52.0\n71.9\n62.0\n30.0\n50.8\n24.0\n0.39\n0.552927\n0.470853\n38.341781\n6869.77760\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nNow we normalize the data by using StandardScaler(). We‚Äôll get all numerical features with proper magnitude.\n\n# Splitting the data\nX_before_scaling = before_scaling_data.drop('yield', axis=1)\n\ny = before_scaling_data['yield']\n\n# Initialise the Scaler\nscaler = StandardScaler()\n\n# Scale the training dataset\nX = scaler.fit_transform(X_before_scaling)\n\n# Convert the NumPy array back to a Pandas DataFrame\nX = pd.DataFrame(X, columns=X_before_scaling.columns)\n\nX.head(3)\n\n\n\n  \n    \n\n\n\n\n\n\nclonesize\nhoneybee\nbumbles\nandrena\nosmia\nMaxOfUpperTRange\nMinOfUpperTRange\nAverageOfUpperTRange\nMaxOfLowerTRange\nMinOfLowerTRange\nAverageOfLowerTRange\nRainingDays\nAverageRainingDays\nfruitset\nfruitmass\nseeds\n\n\n\n\n0\n0.802929\n0.306073\n-0.613664\n1.737386\n-0.662120\n-1.363365\n-1.365484\n-1.368341\n-1.365954\n-1.364693\n-1.366975\n0.458012\n0.401611\n-1.044938\n-0.783260\n-0.918905\n\n\n1\n0.802929\n0.306073\n-0.613664\n0.049454\n-0.662120\n-1.363365\n-1.365484\n-1.368341\n-1.365954\n-1.364693\n-1.366975\n0.458012\n0.401611\n-0.777454\n-0.661588\n-0.572230\n\n\n2\n-1.092448\n-0.385238\n-0.613664\n0.927179\n0.269883\n0.418756\n0.419514\n0.424487\n0.419105\n0.419198\n0.413979\n0.458012\n0.401611\n0.674659\n0.656157\n0.540029\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nLast step of this process is splitting the data into train & test set to get ready for fitting it to the models\n\n# Splitting the data into train & test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(X_train.shape, X_test.shape)\n\n(12231, 16) (3058, 16)"
  },
  {
    "objectID": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#building-evaluating-models",
    "href": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#building-evaluating-models",
    "title": "Predicting Wild Blueberry Yield Using Supervised Machine Learning",
    "section": "Building & Evaluating Model(s) ü§ñüë®‚Äçüè´",
    "text": "Building & Evaluating Model(s) ü§ñüë®‚Äçüè´\n\nBuilding\nLet‚Äôs start by definning which machine learning models will be used\n\n# Dictionary of machine learning models that will be trained\nmodels = {\n            'Ridge': Ridge(),\n            'LinearRegression': LinearRegression(),\n            'SVR': SVR(),\n            'DecisionTreeRegressor': DecisionTreeRegressor(),\n            'RandomForestRegressor': RandomForestRegressor(),\n            'XGBRegressor': XGBRegressor()\n        }\n\nThen we can create a function to fit and score models all at once\n\n# Create a fit_and_score function\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of different Scikit-Learn machine learning models\n    X_train : training data\n    X_test : testing data\n    y_train : labels assosciated with training data\n    y_test : labels assosciated with test data\n    \"\"\"\n\n    # Random seed for reproducible results\n    np.random.seed(42)\n\n    # Make a list to keep model scores\n    results = []\n\n    # Loop through models\n    for name, model in models.items():\n\n        # Fit/Train the model to the data\n        model.fit(X_train, y_train)\n\n        # Make Predictions\n        y_pred = model.predict(X_test)\n\n        # Evaluate the model and append its score to model_scores\n        R2_score = model.score(X_test, y_test) # By default it uses R2_score\n        MSE = mean_squared_error(y_test, y_pred)\n        RMSE = mean_squared_error(y_test, y_pred, squared=False)\n\n        # Add all metrics of model to a list\n        results.append((name, R2_score, MSE, RMSE))\n\n    return results\n\nLet‚Äôs call our function with all parameters needed\n\nmodel_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\n\nEvaluating Models\nOnce the models are trained, we evaluate its performance using R-Squared which is the choosen metric for evaluating how good the model is at predicting Wild Blueberry Yield. But I also used Mean Squared Error & Root Mean Squared Error for comparison purpuses.\nIt seems like RandomForestRegressor is the best one, but let‚Äôs try using K-Fold cross-validation just to make sure its accuracy is correct.\n\n# Dataframe with metrics of all trained models\nresults_df = pd.DataFrame(model_scores, columns = ['Model', 'R2_score', 'MSE', 'RMSE'])\n\n# Add highlights in the dataframe to display best (green) and worst (red) performing models\nresults_format_df = results_df.style.highlight_max(subset = ['R2_score', 'RMSE'], color = 'green').highlight_min(subset = ['MSE'], color = 'green').highlight_max(subset = ['MSE'], color = 'red').highlight_min(subset = ['R2_score','RMSE'], color = 'red')\n\ndisplay(results_format_df)\n\n\n\n\n\n\n\n¬†\nModel\nR2_score\nMSE\nRMSE\n\n\n\n\n0\nRidge\n0.810383\n333333.676304\n577.350566\n\n\n1\nLinearRegression\n0.810477\n333169.787774\n577.208617\n\n\n2\nSVR\n0.501955\n875531.280527\n935.698285\n\n\n3\nDecisionTreeRegressor\n0.635033\n641589.017464\n800.992520\n\n\n4\nRandomForestRegressor\n0.812783\n329114.664203\n573.685161\n\n\n5\nXGBRegressor\n0.806708\n339795.291480\n582.919627\n\n\n\n\n\n\n\n\nEvaluating Model Using K-Fold Cross Validation\n\nThis method is used to estimate the skill of a machine learning model on unseen data. It will evaluate the model multiple times so you can be more confident about the model design. k value is how many samples the data will split. The choice of k is usually 5 or 10, being 10 the most common in the field of applied machine learning, but there is no formal rule.\n\nUtilizing K-Fold cross-validation, Random Forest still remains as the best model.\n\n# Make a list to keep model scores\nresults = []\n\n# Loop through models\nfor name, model in models.items():\n\n    # Fit/Train the model to the data\n    model.fit(X_train, y_train)\n\n    # Make Predictions\n    y_pred = model.predict(X_test)\n\n    # Evaluate the model and append its score to model_scores\n    R2_score = model.score(X_test, y_test)\n    MSE = mean_squared_error(y_test, y_pred)\n    RMSE = mean_squared_error(y_test, y_pred, squared=False)\n\n    # Add all metrics of model to a list\n    results.append((name, R2_score, MSE, RMSE))\n\n    print(name)\n    kf = KFold(n_splits = 10, shuffle = True, random_state=42)\n    scores = cross_val_score(model, X, y, cv = kf) # scoring=\"score\" by default\n\n    # Print out the Mean CV scores & Standard deviation for each model\n    print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n    print('=' * 30)\n\nRidge\n0.81 accuracy with a standard deviation of 0.01\n==============================\nLinearRegression\n0.81 accuracy with a standard deviation of 0.01\n==============================\nSVR\n0.54 accuracy with a standard deviation of 0.01\n==============================\nDecisionTreeRegressor\n0.62 accuracy with a standard deviation of 0.02\n==============================\nRandomForestRegressor\n0.81 accuracy with a standard deviation of 0.01\n==============================\nXGBRegressor\n0.81 accuracy with a standard deviation of 0.02\n==============================\n\n\nComparing Models\nAfter doing k-fold validation, we could verify that RandomForestRegressor is the best model with an accuracy of 81% and a standard deviation of 1%.\n\n# Dataframe consisting of metrics of all the models\nresult_df = pd.DataFrame(results, columns = ['Model', 'R2_score', 'MSE', 'RMSE'])\n\n# Add highlights in the dataframe to display best (green) and worst (red) performing models\nresult_format_df = result_df.style.highlight_max(subset = ['R2_score','RMSE'], color = 'green').highlight_min(subset = ['MSE'], color = 'green').highlight_max(subset = ['MSE'], color = 'red').highlight_min(subset = ['R2_score','RMSE'], color = 'red')\n\ndisplay(result_format_df)\n\n\n\n\n\n\n\n¬†\nModel\nR2_score\nMSE\nRMSE\n\n\n\n\n0\nRidge\n0.810383\n333333.676304\n577.350566\n\n\n1\nLinearRegression\n0.810477\n333169.787774\n577.208617\n\n\n2\nSVR\n0.501955\n875531.280527\n935.698285\n\n\n3\nDecisionTreeRegressor\n0.618634\n670417.767968\n818.790430\n\n\n4\nRandomForestRegressor\n0.811807\n330831.068699\n575.179162\n\n\n5\nXGBRegressor\n0.806708\n339795.291480\n582.919627"
  },
  {
    "objectID": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#hyperparameter-tuning",
    "href": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#hyperparameter-tuning",
    "title": "Predicting Wild Blueberry Yield Using Supervised Machine Learning",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\nNow that we find out which model is the best for this scenario with the data we have, let‚Äôs try tunning a bit more the models by using Hyperparameter Tuning throughout GridSearchCV.\n\nIn this case we use Grid Search method to try several combinations. But, when the hyperparameter search space is large is a tedious task. So, instead we can use, Randomized Search It is an efficient method.\n\n\nmodel_params = [\n    {'alpha': [0.1, 1.0, 10.0]},  # Ridge parameters\n    {},  # LinearRegression doesn't require hyperparameters here\n    {},  # SVR doesn't require hyperparameters here\n    {},  # DecisionTreeRegressor doesn't require hyperparameters here\n    {'n_estimators': [10, 50, 100]},  # RandomForestRegressor parameters\n    {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.5, 1.0]}  # XGBRegressor parameters\n]\n\nHere, we performed Hyperparameter Tuning on multiple Regression Models to finally find out the best model\n\nscores = []\nbest_estimators = {}\n\nfor name, model, params in zip(models.keys(), models.values(), model_params):\n    clf = GridSearchCV(model, params, cv=5, return_train_score=False)\n    clf.fit(X_train, y_train)\n    scores.append({\n        'model': name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n\n    best_estimators[model] = clf.best_estimator_\n\nresults = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\nresults\n\n\n\n  \n    \n\n\n\n\n\n\nmodel\nbest_score\nbest_params\n\n\n\n\n0\nRidge\n0.808868\n{'alpha': 10.0}\n\n\n1\nLinearRegression\n0.807210\n{}\n\n\n2\nSVR\n0.462866\n{}\n\n\n3\nDecisionTreeRegressor\n0.628891\n{}\n\n\n4\nRandomForestRegressor\n0.807681\n{'n_estimators': 100}\n\n\n5\nXGBRegressor\n0.819147\n{'learning_rate': 0.1, 'n_estimators': 50}\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nAgain, we train our model but now with the best hyperparameter previously found and get once again its scores of evaluation metrics.\n\nbest_model=RandomForestRegressor(n_estimators=100)\nbest_model.fit(X_train,y_train)\ny_pred=best_model.predict(X_test)\n\nR2_score = model.score(X_test, y_test)\nMSE = mean_squared_error(y_test, y_pred)\nRMSE = mean_squared_error(y_test, y_pred, squared=False)\n\nprint(\"R2_score:\", R2_score)\nprint(\"MSE:\", MSE)\nprint(\"RMSE:\", RMSE)\n\nR2_score: 0.8067077856896194\nMSE: 333042.17478148366\nRMSE: 577.0980634012591\n\n\nHere, we plotted the results of our best model (predicted), against the original results (actual).\n\n# Show Actual vs. Predicted Values\nsns.scatterplot(x=y_test, y=y_pred, alpha=0.7, color='#464196')\nsns.scatterplot(x=y_test, y=y_test, alpha=0.7, color='#221f1f')\nplt.xlabel('Actual Values (y_test)')\nplt.ylabel('Predicted Values (y_pred)')\nplt.figtext(0.47, 0.96, 'Actual', fontsize='large', color='#221f1f', ha ='right')\nplt.figtext(0.53, 0.96, 'Predicted', fontsize='large', color='#464196', ha ='left')\nplt.figtext(0.50, 0.96, ' vs. ', fontsize='large', color='k', ha ='center')\nsns.despine()\n\nplt.grid(True)"
  },
  {
    "objectID": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#using-the-model-with-unseen-data",
    "href": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#using-the-model-with-unseen-data",
    "title": "Predicting Wild Blueberry Yield Using Supervised Machine Learning",
    "section": "Using the model with Unseen data",
    "text": "Using the model with Unseen data\nFirst, let‚Äôs import the Unseen data\n\n# Loading unseen data\ntest_data = pd.read_csv('test.csv')\n\n# Saving id of data\ntest_data_id = test_data['id']\n\n# Remove unnecesary columns\ntest_data.drop(columns=['id'], inplace=True)\ntest_data.sample(3)\n\n\n\n  \n    \n\n\n\n\n\n\nclonesize\nhoneybee\nbumbles\nandrena\nosmia\nMaxOfUpperTRange\nMinOfUpperTRange\nAverageOfUpperTRange\nMaxOfLowerTRange\nMinOfLowerTRange\nAverageOfLowerTRange\nRainingDays\nAverageRainingDays\nfruitset\nfruitmass\nseeds\n\n\n\n\n3444\n12.5\n0.25\n0.38\n0.38\n0.75\n77.4\n46.8\n64.7\n55.8\n27.0\n45.8\n16.0\n0.26\n0.638323\n0.527896\n45.069192\n\n\n3758\n25.0\n0.50\n0.38\n0.38\n0.50\n94.6\n57.2\n79.0\n68.2\n33.0\n55.9\n1.0\n0.10\n0.556302\n0.472468\n40.555019\n\n\n2996\n12.5\n0.25\n0.25\n0.50\n0.63\n77.4\n46.8\n64.7\n55.8\n27.0\n45.8\n34.0\n0.56\n0.557251\n0.480779\n39.474324\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nThen, we nee to standarize it as we did with the training data\n\n# Scale the training dataset\nX_test = scaler.fit_transform(test_data)\n\n# Convert the NumPy array back to a Pandas DataFrame\nX_test = pd.DataFrame(X_test, columns=test_data.columns)\n\nX_test.head(3)\n\n\n\n  \n    \n\n\n\n\n\n\nclonesize\nhoneybee\nbumbles\nandrena\nosmia\nMaxOfUpperTRange\nMinOfUpperTRange\nAverageOfUpperTRange\nMaxOfLowerTRange\nMinOfLowerTRange\nAverageOfLowerTRange\nRainingDays\nAverageRainingDays\nfruitset\nfruitmass\nseeds\n\n\n\n\n0\n0.789503\n-0.363088\n-0.606503\n-1.628845\n-2.453506\n0.414398\n0.414768\n0.420044\n0.414636\n0.414374\n0.409109\n0.466288\n0.408256\n-1.371318\n-1.032509\n-1.179639\n\n\n1\n-1.106867\n-0.363088\n-0.606503\n1.745905\n0.262907\n1.353982\n1.351365\n1.348482\n1.351655\n1.352424\n1.354337\n-1.486257\n-1.345772\n-0.184207\n-0.093898\n0.176782\n\n\n2\n-1.106867\n-0.363088\n-0.606503\n0.935965\n0.262907\n0.414398\n0.414768\n0.420044\n0.414636\n0.414374\n0.409109\n-0.212858\n-0.378032\n1.091922\n1.098780\n0.970547\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nFinally, we can predict on this new data and save the predictions as a csv file\n\npredictions = best_model.predict(X_test)\n\nsubmission = pd.DataFrame({'id': test_data_id, 'yield': predictions})\nsubmission.to_csv('RandomForestRegressor_submission.csv', index=False)"
  },
  {
    "objectID": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#saving-model",
    "href": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#saving-model",
    "title": "Predicting Wild Blueberry Yield Using Supervised Machine Learning",
    "section": "Saving Model",
    "text": "Saving Model\nA a last step, after developing the model, we need to save it for later uses.\n\n# Save the model in the 'model' directory\nmodel_dir = \"../model\"\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\n\nmodel_path = os.path.join(model_dir, \"random_forest.joblib\")\njoblib.dump(best_model, model_path)\n\nIf we want to verify that the model was saved we can load it and try it out on new data:\n\n# load, no need to initialize the loaded_rf\nloaded_rf = joblib.load(\"./random_forest.joblib\")\n\n# Loading  new data\nnew_data = pd.read_csv('new_data.csv')\n\n# Use it to verify its working\nloaded_rf.predict(new_data)"
  },
  {
    "objectID": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#results-explanation",
    "href": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#results-explanation",
    "title": "Predicting Wild Blueberry Yield Using Supervised Machine Learning",
    "section": "Results Explanation",
    "text": "Results Explanation\nThe following chart shows you the Feature importance on this data. What I mean by that is what features were most important when it comes to predicting Wild Blueberries Yield (target variable).\n\n# Getting feature importance\nfeat_importance= {'importance': model.feature_importances_, 'features': X.columns}\n\n# Parsing it into a dataframe\ndf=pd.DataFrame.from_dict(feat_importance)\ndf.sort_values(ascending=False, by=['importance'], inplace=True)\ndf=df.dropna()\n\n# Plotting Feature Importance with all features\nsns.barplot(x='importance', y='features', data=df, orient='h')\nChartWithCustomSettings(g, 6, 4, 'Feature Importance')\nsns.despine(left=True)\nplt.tight_layout()\n\n\n\n\n\n\n\n\nGetting only top 5 of the features by its importance in the model:\n\n# Parsing it into a dataframe\nimportance_df=pd.DataFrame.from_dict(feat_importance)\nimportance_df.sort_values(ascending=False, by=['importance'], inplace=True)\nimportance_df=importance_df.dropna()\n\n# Getting only top 5 features\nimportance_df=importance_df.nlargest(5, 'importance')\ndisplay(importance_df)\nprint(' ')\n\n# Plotting only top 5 features\nsns.barplot(x='importance', y='features', data=importance_df, orient='h')\nChartWithCustomSettings(g, 6, 4, 'Top 5 of features importance')\nsns.despine(left=True)\n\n\n\n  \n    \n\n\n\n\n\n\nimportance\nfeatures\n\n\n\n\n13\n0.554793\nfruitset\n\n\n15\n0.112663\nseeds\n\n\n8\n0.090891\nMaxOfLowerTRange\n\n\n12\n0.030018\nAverageRainingDays\n\n\n9\n0.029331\nMinOfLowerTRange"
  },
  {
    "objectID": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#next-steps",
    "href": "posts/predicting-wild-blueberry-yield-using-supervised-ml.html#next-steps",
    "title": "Predicting Wild Blueberry Yield Using Supervised Machine Learning",
    "section": "Next Steps üîú",
    "text": "Next Steps üîú\nThere is always room for improvement, and for this project these are some things tha could be done to improve its predictiveness of the model:\n\nBetter Feature Engineering\nBetter experiments with Scaling\nFeature Encoding if we add some categorical data\nBetter Model Tuning by playing a bit more with its hyperparameters\nRemoving Features with High Correlation\n\n\nYou‚Äôre Awesome üòç, you just reached the end of this post. If you have any questions just drop me a message on my LikedIn. Also, any suggestion or kudos would be quite appreciated. Did you find it useful? Check out my other posts here, I‚Äôm sure you‚Äôll find something interesting üí°.\nShare this post with your friends/colleagues, and if you are in a good mood, buy me a cup of coffee ‚òï. Nos vemos! üèÉüí®"
  },
  {
    "objectID": "posts/data-preprocessing-with-pandas/index.html#tldr",
    "href": "posts/data-preprocessing-with-pandas/index.html#tldr",
    "title": "Data Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy",
    "section": "TL;DR üëÄ",
    "text": "TL;DR üëÄ\nIn a previous project, I made a Dataset by Scraping the videos details of a Youtube Channel using Selenium and Python. This time I‚Äôll be showing how to perform many tasks in order to process all the gathered information. The output of this project is a Clean and ready-to-analyse Dataset containing information of all International Matches of Freestyle organized by Red Bull from 2015 to 2020 (filtered by internacional and vs keywords). Also, here I leave you the Output Dataset from the previous Web Scraping Project, so you can compare them.\nBut first, let‚Äôs learn a bit about the International Competition. Red Bull Batalla de los Gallos is the Most Recognized Freestyle Competition in Spanish that brings together the 16 winning Freestylers from the competitions organized by Red Bull in each country. After all matches only one of them is crowned as international champion. Click here to learn more"
  },
  {
    "objectID": "posts/data-preprocessing-with-pandas/index.html#importing-libraries",
    "href": "posts/data-preprocessing-with-pandas/index.html#importing-libraries",
    "title": "Data Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy",
    "section": "Importing Libraries üß∞",
    "text": "Importing Libraries üß∞\n\n# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport re\n\nfrom datetime import datetime\n\n# check Pandas' version\npd.__version__\n\n'1.1.5'"
  },
  {
    "objectID": "posts/data-preprocessing-with-pandas/index.html#importing-dataset",
    "href": "posts/data-preprocessing-with-pandas/index.html#importing-dataset",
    "title": "Data Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy",
    "section": "Importing Dataset üóÉÔ∏è",
    "text": "Importing Dataset üóÉÔ∏è\n\n# importing from url\ndata_url = 'https://raw.githubusercontent.com/mrenrique/EDA-to-Youtube-Channel-Videos/main/redbulloficialgallos_videos_details_dec-27-2020.csv'\n\n# reading dataset with pandas and asigning  to a variable\ndata = pd.read_csv(data_url)\n\n# show first three rows\ndata.head(3)\n\n\n\n\n\n\n\n\n\ntitle\nviews\nupload_date\nlength\nlikes\ndislikes\nurl\n\n\n\n\n0\nACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R...\n577,503\nDec 12, 2020\n6:16\n14,040\n270\nhttps://www.youtube.com/watch?v=Fwda4AWZ6V4\n\n\n1\nEXODO LIRICAL vs RAPDER - Semifinal | Red Bull...\n238,463\nDec 12, 2020\n12:30\n8,135\n927\nhttps://www.youtube.com/watch?v=wIcz1_7qx-4\n\n\n2\nACZINO vs SKONE - Semifinal | Red Bull Interna...\n756,352\nDec 12, 2020\n10:06\n18,458\n1,146\nhttps://www.youtube.com/watch?v=yv8yFhRsWVc"
  },
  {
    "objectID": "posts/data-preprocessing-with-pandas/index.html#learning-the-datasets-properties",
    "href": "posts/data-preprocessing-with-pandas/index.html#learning-the-datasets-properties",
    "title": "Data Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy",
    "section": "Learning the Dataset‚Äôs Properties üí°",
    "text": "Learning the Dataset‚Äôs Properties üí°\nLet‚Äôs take a look at the datafame‚Äôs properties for a better understanding to know what needs to be done. To do so, we can use the info() method which gives us the number of columns, columns names and their data types all together.\n\n# Display number of columns, columns names , Non-Null Count and data types\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 95 entries, 0 to 94\nData columns (total 7 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   title        95 non-null     object\n 1   views        95 non-null     object\n 2   upload_date  95 non-null     object\n 3   length       95 non-null     object\n 4   likes        95 non-null     object\n 5   dislikes     95 non-null     object\n 6   url          95 non-null     object\ndtypes: object(7)\nmemory usage: 5.3+ KB\n\n\nNow that we learn about the dataset in a general way, let‚Äôs also learn in a detailed way by showing a random sample of the dataset to give us an idea of what kind of values we are dealing with. Let‚Äôs start by showing a random sample of the dataset.\n\n# Random sample of 50 % of Dataset\ndata.sample(frac=0.5).head()\n\n\n\n\n\n\n\n\n\ntitle\nviews\nupload_date\nlength\nlikes\ndislikes\nurl\n\n\n\n\n19\nTRUENO vs TITO MC - Octavos | Red Bull Interna...\n2,082,852\nNov 30, 2019\n5:51\n43,267\n3,996\nhttps://www.youtube.com/watch?v=KJbIAlUdmLw\n\n\n71\nJONY BELTRAN vs CHUTY - Octavos | Red Bull Int...\n13,138,438\nNov 12, 2016\n7:09\n182,494\n11,726\nhttps://www.youtube.com/watch?v=C2rXItCS8I0\n\n\n23\nJAZE vs SNK - Octavos | Red Bull Internacional...\n1,407,134\nNov 30, 2019\n7:06\n28,687\n890\nhttps://www.youtube.com/watch?v=gkfOnJI4Byc\n\n\n52\nARKANO vs. YENKY ONE - 3 y 4 Puesto: Final Int...\n2,093,488\nDec 3, 2017\n4:47\n30,378\n1,347\nhttps://www.youtube.com/watch?v=VOHgIr6dSZI\n\n\n60\nJONY BELTRAN vs. ARKANO - Cuartos: Final Inter...\n3,352,057\nDec 3, 2017\n4:41\n37,794\n1,164\nhttps://www.youtube.com/watch?v=wWtcdK7bd4Y"
  },
  {
    "objectID": "posts/data-preprocessing-with-pandas/index.html#data-cleaning-and-transformation",
    "href": "posts/data-preprocessing-with-pandas/index.html#data-cleaning-and-transformation",
    "title": "Data Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy",
    "section": "Data Cleaning üßº and Transformation üî™",
    "text": "Data Cleaning üßº and Transformation üî™\nThere are many tasks involved in Data Preprocessing which in turn are grouped into 4 main processes (Data Integration, Data Cleaning, Data Transformation and Data Reduction) but depending on the data and the scope of this project (Exploratory Data Analysis) we‚Äôll just need to perform some of them. let‚Äôs start assuring the Data Quality for further Analysis.\n\nRenaming Columns Names\nLet‚Äôs first show all Columns Names to check if they required changes.\n\n# Show all columns\ndata.columns\n\nIndex(['title', 'views', 'upload_date', 'length', 'likes', 'dislikes', 'url'], dtype='object')\n\n\nAs we see, almost all Columns Names are ok except for upload_date. Let‚Äôs change it for year Since we only need the year of the date.\n\ndata.rename(columns={'upload_date': 'year'}, inplace=True)\n\n# Verify changes\ndata.columns\n\nIndex(['title', 'views', 'year', 'length', 'likes', 'dislikes', 'url'], dtype='object')\n\n\n\n# see types of all columns and change its type if needed\ndata.dtypes\n\ntitle       object\nviews       object\nyear        object\nlength      object\nlikes       object\ndislikes    object\nurl         object\ndtype: object\n\n\n\n\nDeleting Columns not needed (First Attempt)\nIt‚Äôs useful to remove some Columns that doesn‚Äôt contributed to the Analysis Goal. In this case, url Column is not necesary.\n\n# Another way is not using the axis argument but instead asign columns argument\ndata.drop(columns=['url'], inplace=True)\ndata.columns\n\nIndex(['title', 'views', 'year', 'length', 'likes', 'dislikes'], dtype='object')\n\n\n\n\nModifying values by Removing (Additional meaningless data), Adding or Formating them\nNow in order to Set the proper Data Type to each Column we need to make sure that all Columns Values are clean. Let‚Äôs see a few rows to know what kind of values the dataset has.\n\ndata.head()\n\n\n\n\n\n\n\n\n\ntitle\nviews\nyear\nlength\nlikes\ndislikes\n\n\n\n\n0\nACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R...\n577,503\nDec 12, 2020\n6:16\n14,040\n270\n\n\n1\nEXODO LIRICAL vs RAPDER - Semifinal | Red Bull...\n238,463\nDec 12, 2020\n12:30\n8,135\n927\n\n\n2\nACZINO vs SKONE - Semifinal | Red Bull Interna...\n756,352\nDec 12, 2020\n10:06\n18,458\n1,146\n\n\n3\nRAPDER vs YARTZI - Cuartos | Red Bull Internac...\n47,082\nDec 12, 2020\n6:46\n1,822\n206\n\n\n4\nEXODO LIRICAL vs BNET - Cuartos | Red Bull Int...\n685,109\nDec 12, 2020\n6:40\n23,202\n1,842\n\n\n\n\n\n\n\n\nAs you can see, there are some Undesired characters among the values of some Columns. So it‚Äôs necesary to remove Unnecessary Characteres before doing any conversion task. Let‚Äôs start cleaning the title Column to keep only the Names of Freestylers\n\nImportant: Be careful, sometimes there are some characteres that seems similar like these ones - and ‚Äì but they are completely different and it can take you a while figure out why is not spliting as espected. I also had to add a conditional because the name of a participal has the - in it and it was spliting up incorrectly.\n\n\n# Split by multiple different delimiters\npattern = '[-‚Äì|:]'\n\n# data['title'] = [re.split(pattern, i)[0].strip() if 'VALLES-T' not in i else i for i in data['title']]\ndata['title'] = [re.split(pattern, i)[0].strip() if 'VALLES-T' not in i else re.split(' - ', i)[0].strip() for i in data['title']]\n\ndata['title'] = [i.replace('.', '').strip() for i in data['title']]\n\n# verify changes\ndata[data['title'].str.contains('VALLES')].head()\n\n\n\n\n\n\n\n\n\ntitle\nviews\nyear\nlength\nlikes\ndislikes\n\n\n\n\n10\nBNET vs VALLES-T\n1,350,908\nDec 12, 2020\n9:08\n49,448\n3,012\n\n\n18\nBNET vs VALLES-T\n16,680,349\nNov 30, 2019\n17:12\n282,481\n32,957\n\n\n20\nVALLES-T vs CHANG\n11,477,492\nNov 30, 2019\n6:43\n161,561\n2,969\n\n\n26\nVALLES-T vs JOKKER\n3,221,089\nNov 30, 2019\n6:21\n48,888\n1,155\n\n\n31\nVALLES-T vs ACZINO\n16,277,039\nNov 30, 2019\n13:46\n279,388\n9,027\n\n\n\n\n\n\n\n\nLets continue cleaning the Columns views, likes and dislikes, In this case, we‚Äôll remove the comma (,) from views, likes and dislikes Columns Values. Also, in the row 8 (and some others rows) there is the word Premiered before the date string. It needs to be removed.\n\n# List of characters to remove\nchars_to_remove = [' ', ',']\n\n# List of column names to clean\ncols_to_clean = ['views', 'dislikes', 'likes']\n\n# Loop for each column\nfor col in cols_to_clean:\n\n    # Replace each character with an empty string\n    for char in chars_to_remove:\n        data[col] = data[col].astype(str).str.replace(char,'')\n\n# verify changes\ndata.head(3)\n\n\n\n\n\n\n\n\n\ntitle\nviews\nyear\nlength\nlikes\ndislikes\n\n\n\n\n0\nACZINO vs EXODO LIRICAL\n577503\nDec 12, 2020\n6:16\n14040\n270\n\n\n1\nEXODO LIRICAL vs RAPDER\n238463\nDec 12, 2020\n12:30\n8135\n927\n\n\n2\nACZINO vs SKONE\n756352\nDec 12, 2020\n10:06\n18458\n1146\n\n\n\n\n\n\n\n\nAs we said earlier, we only need the last part of the string for each upload_date Column Value.\n\ndata['year'] = [re.split(',', i)[1].strip() for i in data['year']]\n\n# verify changes\ndata['year'].head()\n\n0    2020\n1    2020\n2    2020\n3    2020\n4    2020\nName: year, dtype: object\n\n\n\n\nData Type Convertion\n(less memory usage)\nLet‚Äôs check what Data Types the Columns are\n\ndata.dtypes\n\ntitle       object\nviews       object\nyear        object\nlength      object\nlikes       object\ndislikes    object\ndtype: object\n\n\nSince we already saw the dataset have String, Datetime and Number values, this is not so specific, we need to set the right Data Type to all Columns. Let‚Äôs first try an Automatic Data Type Conversion Method toy see if this will do the trick.\n\n# Convert to Best Data Types Automatically\ndata.convert_dtypes().dtypes\n\ntitle       string\nviews       string\nyear        string\nlength      string\nlikes       string\ndislikes    string\ndtype: object\n\n\nSince we see the code above it‚Äôs not quite effective, we‚Äôll need to convert them manually. Also, from the above code, we see that it‚Äôs neccesary remove some characteres inside Columns Values, that‚Äôs why the automatic method set all columns as a string.\n\ndata['title'] = data['title'].astype(str)\n\n# List of column names to convert to numberic data\ncols_to_modify_dtype = ['views', 'dislikes', 'likes']\n\nfor col in cols_to_modify_dtype:\n    # Convert col to numeric\n    data[col] = pd.to_numeric(data[col])\n\ndata['length'] = pd.to_datetime(data['length'], format='%M:%S').dt.time\n\ndata['year'] = pd.DatetimeIndex(data['year']).year\n\n# verify changes\ndata.dtypes\n\ntitle       object\nviews        int64\nyear         int64\nlength      object\nlikes        int64\ndislikes     int64\ndtype: object\n\n\nLets print once again a few rows of the dataset to see if changes were applied.\n\ndata.head()\n\n\n\n\n\n\n\n\n\ntitle\nviews\nyear\nlength\nlikes\ndislikes\n\n\n\n\n0\nACZINO vs EXODO LIRICAL\n577503\n2020\n00:06:16\n14040\n270\n\n\n1\nEXODO LIRICAL vs RAPDER\n238463\n2020\n00:12:30\n8135\n927\n\n\n2\nACZINO vs SKONE\n756352\n2020\n00:10:06\n18458\n1146\n\n\n3\nRAPDER vs YARTZI\n47082\n2020\n00:06:46\n1822\n206\n\n\n4\nEXODO LIRICAL vs BNET\n685109\n2020\n00:06:40\n23202\n1842\n\n\n\n\n\n\n\n\n\n\nDealing with Missing Values\nFirst we verify if the dataset have Missing Values.\n\n# Show Whether the dataset contain any Missing Value\ndata.isnull().values.any()\n\nFalse\n\n\nSince there is not Missing Values, Let‚Äôs move on to the next task.\n\n\nRemoving Duplicated Values\nIn order to identify if there are Duplicated Values, we‚Äôll use duplicated() method.\n\n# Select duplicate rows except first occurrence based on all columns\nduplicateRowsDF = data[data.duplicated()]\nif duplicateRowsDF.empty == True:\n    print('There arent Duplicated Values. Good to go!')\nelse:\n    print('Duplicate Rows except first occurrence based on all columns are :')\n    print(duplicateRowsDF)\n\nThere arent Duplicated Values. Good to go!\n\n\n\n\nDealing with Inconsistencies Data\n(Business Rule | Domain Expertice required) Modifying | Removing Erroneus Values\nBecause I‚Äôm myself a fan of such Freestyle Competitions, I know that normally there are up to 16 matches every year. Let‚Äôs verify that.\n\ndata['year'].value_counts()\n\n2018    18\n2020    17\n2019    16\n2017    16\n2016    16\n2015    12\nName: year, dtype: int64\n\n\nAs we can see there are more than that in the year 2018 and 2020, Lets find out what‚Äôs going on.\n\ndata[data['year'] == 2018]\n\n\n\n\n\n\n\n\n\ntitle\nviews\nyear\nlength\nlikes\ndislikes\n\n\n\n\n33\nSWITCH vs BNET\n1637196\n2018\n00:07:19\n33768\n493\n\n\n34\nWOS vs RAPDER Octavos\n1831544\n2018\n00:07:01\n33093\n5586\n\n\n35\nBNET vs ARKANO\n3506340\n2018\n00:07:17\n58063\n1371\n\n\n36\nVALLES T vs PEPE GRILLO\n10975462\n2018\n00:07:18\n166863\n2964\n\n\n37\nNEON vs LETRA\n4750022\n2018\n00:07:36\n77815\n1101\n\n\n38\nWOS vs LETRA\n3875917\n2018\n00:07:34\n72927\n5913\n\n\n39\nVALLES T vs BNET\n3609190\n2018\n00:08:09\n62295\n4511\n\n\n40\nWOS vs ACZINO\n39525308\n2018\n00:19:03\n680254\n73824\n\n\n41\nVALLES T vs KDT\n1858540\n2018\n00:07:50\n33888\n877\n\n\n42\nACZINO vs JAZE\n4673494\n2018\n00:07:53\n70535\n5986\n\n\n43\nBNET vs ACZINO\n6880359\n2018\n00:08:14\n108931\n8573\n\n\n44\nYERIKO vs PEPE GRILLO\n263529\n2018\n00:07:14\n5625\n655\n\n\n45\nRVS vs INDICO\n1488867\n2018\n00:10:48\n31762\n495\n\n\n46\nINDICO vs ACZINO\n1578144\n2018\n00:07:28\n21910\n466\n\n\n47\nVALLES T vs WOS\n6938112\n2018\n00:09:14\n116773\n25799\n\n\n48\nDOZER vs ARKANO\n1231381\n2018\n00:08:07\n27260\n4257\n\n\n49\nPerfil de Gallo\n16675\n2018\n00:00:52\n863\n30\n\n\n50\nMARK GRIST vs GALLOS\n33867\n2018\n00:03:13\n1442\n44\n\n\n\n\n\n\n\n\nRows 49 and 50 are not part of the International Competition‚Äô videos, so they need to be removed. Now, let‚Äôs see the rows of 2020 year\n\ndata[data['year'] == 2020]\n\n\n\n\n\n\n\n\n\ntitle\nviews\nyear\nlength\nlikes\ndislikes\n\n\n\n\n0\nACZINO vs EXODO LIRICAL\n577503\n2020\n00:06:16\n14040\n270\n\n\n1\nEXODO LIRICAL vs RAPDER\n238463\n2020\n00:12:30\n8135\n927\n\n\n2\nACZINO vs SKONE\n756352\n2020\n00:10:06\n18458\n1146\n\n\n3\nRAPDER vs YARTZI\n47082\n2020\n00:06:46\n1822\n206\n\n\n4\nEXODO LIRICAL vs BNET\n685109\n2020\n00:06:40\n23202\n1842\n\n\n5\nSKONE vs ACERTIJO\n179664\n2020\n00:09:36\n5341\n4847\n\n\n6\nACZINO vs NAICEN\n158269\n2020\n00:06:51\n5507\n1713\n\n\n7\nSKONE vs RAPDER\n1651540\n2020\n00:15:19\n64965\n3259\n\n\n8\nELEVN vs YARTZI\n56480\n2020\n00:06:30\n2041\n131\n\n\n9\nRAPDER vs STICK\n122237\n2020\n00:06:21\n4389\n2710\n\n\n10\nBNET vs VALLES-T\n1350908\n2020\n00:09:08\n49448\n3012\n\n\n11\nEXODO LIRICAL vs MAC\n105707\n2020\n00:06:13\n5319\n254\n\n\n12\nACERTIJO vs MINOS\n57738\n2020\n00:06:24\n2530\n55\n\n\n13\nSKONE vs TATA\n436674\n2020\n00:06:36\n12055\n17890\n\n\n14\nNAICEN vs SNK\n66128\n2020\n00:06:44\n3196\n200\n\n\n15\nACZINO vs SHIELD MASTER\n154369\n2020\n00:06:51\n5439\n471\n\n\n16\nBLON vs NEW ERA vs YOIKER\n932161\n2020\n00:25:18\n49375\n859\n\n\n\n\n\n\n\n\nThe same, Even though the row 16 is a international Competition Video (info), this match was done to have a reserve competitor just in case any of the 16 couldn‚Äôt make it. But it didn‚Äôt occur. Now let‚Äôs remove all rows are not part of the Oficial Matches‚Äô Videos.\n\n# Delete rows with index label\ndata = data.drop([16, 49 , 50])\n\n\n\nSetting & Modifying the Index Column\nBencause it was necessary to remove some rows (16,49 and 50), the index was changed. Let fix that. Also, I‚Äôll asign a name to the Index Column.\n\ndata.reset_index(inplace = True, drop=True)\n\n\ndata.loc[48:50,:]\n\n\n\n\n\n\n\n\n\ntitle\nviews\nyear\nlength\nlikes\ndislikes\n\n\n\n\n48\nG vs EL TANQUE\n253069\n2017\n00:05:08\n3640\n424\n\n\n49\nARKANO vs YENKY ONE\n2093488\n2017\n00:04:47\n30378\n1347\n\n\n50\nWOS vs ACZINO\n23008624\n2017\n00:07:51\n261785\n13990"
  },
  {
    "objectID": "posts/data-preprocessing-with-pandas/index.html#exporting-the-clean-dataset",
    "href": "posts/data-preprocessing-with-pandas/index.html#exporting-the-clean-dataset",
    "title": "Data Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy",
    "section": "Exporting the Clean Dataset üíæ",
    "text": "Exporting the Clean Dataset üíæ\nNow that we‚Äôre assure the dataset is clean and contain only the right values. Let‚Äôs export it to move on to Exporing and Analizing the dataset.\n\ndata.to_csv('clean_data.csv')\n\nOr if you prefer, you can download it to your Computer.\n\ndata.to_csv('clean_data.csv')\n\nfrom google.colab import files\nfiles.download('clean_data.csv')\n\n\n\n\n\n\n\nYou‚Äôre Awesome üòç, you just reached the end of this post. If you have any questions just drop me a message on my LikedIn. Also, any suggestion or kudos would be quite appreciated. Did you find it useful? Check out my other posts here, I‚Äôm sure you‚Äôll find something interesting üí°.\nShare this post with your friends/colleagues, and if you are in a good mood, buy me a cup of coffee ‚òï. Nos vemos! üèÉüí®"
  },
  {
    "objectID": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#tldr",
    "href": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#tldr",
    "title": "EDA. Analizing Videos‚Äô Details of Red Bull Batalla de Gallos‚Äô Youtube Channel",
    "section": "TL;DR ü§ì",
    "text": "TL;DR ü§ì\nThis project‚Äôs aim is to perform some common EDA tasks on the created dataset containing information of all International Matches of Freestyle organized by Red Bull from 2015 to 2020 (filtered by internacional and vs keywords). Red Bull Batalla de los Gallos is the Most Recognized Freestyle Competition in Spanish that brings together the 16 winning Freestylers from the competitions organized by Red Bull in each country. After all matches only one of them is crowned as international champion Click here to learn more"
  },
  {
    "objectID": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#importing-libraries",
    "href": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#importing-libraries",
    "title": "EDA. Analizing Videos‚Äô Details of Red Bull Batalla de Gallos‚Äô Youtube Channel",
    "section": "Importing Libraries ‚úîÔ∏è",
    "text": "Importing Libraries ‚úîÔ∏è\nIn order to achieve the goal of this project, It‚Äôs necesary to install & import some libraries that will make our life a lot easier.\n\nNumpy for doing mathematical operations\nPandas for manipulating structured data & making EDA\nMatplotlib & Seaborn, this one help us create graphs to visually understand the EDA\nDatetime will make the task of dealing with time data a lot easier\n\nOnce imported all the libraries requieres, let‚Äôs also check their version as reference.\n\n#collapse-hide\n# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nsns.set_theme(style=\"ticks\", color_codes=True)\n\n# Check Libraries' version\nprint('Numpy version: '+np.__version__)\nprint('Pandas version: '+pd.__version__)\nprint('Matplotlib version: '+matplotlib.__version__)\nprint('Seaborn version: '+sns.__version__)\n\nNumpy version: 1.21.6\nPandas version: 1.3.5\nMatplotlib version: 3.2.2\nSeaborn version: 0.11.2"
  },
  {
    "objectID": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#customized-settings",
    "href": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#customized-settings",
    "title": "EDA. Analizing Videos‚Äô Details of Red Bull Batalla de Gallos‚Äô Youtube Channel",
    "section": "Customized Settings üé®",
    "text": "Customized Settings üé®\nIn the hidden code cell bellow there is two functions, both of them customizes som of the default parameters of the graphs, to make them look a bit cleaner and easy to digest.\nAlso, It‚Äôs advisable to present graphs with the same colors as the brand to make it a bit relatable, so I picked & used the event‚Äôs logo colors for this purpose. To get these color palette I used this website which is very useful for this task: https://coolors.co/image-picker\n\n#collapse-hide\n# Custom palette\n# https://www.youtube.com/watch?v=2wRHBodrWuY\ng=[]\n\n# def customPlotSettings(graph=g, figW=6.4, figH=5, XorY=plt.yticks([])):\ndef customPlotSettings(graph=g, figW=6.4, figH=5, dimension=1000, Character='k'):\n\n    g.fig.set_figwidth(figW)\n    g.fig.set_figheight(figH)\n\n    ax=g.facet_axis(0,0)\n\n    for p in ax.patches:\n        height = p.get_height() # height of each horizontal bar is the same\n        width = p.get_width()\n        ax.text(p.get_x() + (width / 2),\n                height * 1.03, # # y-coordinate position of data label, padded to be in the middle of the bar,\n                f'{(height / dimension ):.0f}'+Character+'',\n                # f'{(height / fHeight ):.0f}K',\n                ha='center'\n                )\n\n    # Remove frame (or all the spines at the same time)\n    ax.set_frame_on(False)\n\n    custom_params = {\n                     'axes.titlesize':16,\n                     'ytick.left': False,\n                     'axes.titlepad': 20\n                    }\n\n    sns.set_theme(style='white', font_scale=1.1 , rc=custom_params)\n\n    custom_palette = ['#203175','#E30C4C','#FDCA24']\n    sns.set_palette(custom_palette)\n\ndef customHistSettings(figW=6.4):\n\n    fig, ax = plt.subplots()\n    custom_params = {\n                     'figure.figsize':(figW,5),\n                     'axes.titlesize':16,\n                     'ytick.left': False\n                    }\n    sns.set_theme(style='white', rc=custom_params)\n\n    ax.grid(axis ='x', color ='0.95')\n    ax.set_frame_on(False)\n    plt.yticks([])\n\n    custom_palette = ['#203175','#E30C4C','#FDCA24']\n    sns.set_palette(custom_palette)"
  },
  {
    "objectID": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#importing-dataset",
    "href": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#importing-dataset",
    "title": "EDA. Analizing Videos‚Äô Details of Red Bull Batalla de Gallos‚Äô Youtube Channel",
    "section": "Importing dataset üóÉÔ∏è",
    "text": "Importing dataset üóÉÔ∏è\nLet‚Äôs start by importing from Github the tidy dataset which was a result from the previous tutorial. This tutorial covered Data Preprocessing Videos Details of a Youtube Channel\nAlso, I‚Äôll print 3 random rows to check the dataset was imported succesfully.\n\ndata_url = 'https://raw.githubusercontent.com/mrenrique/EDA-to-Youtube-Channel-Videos/main/clean_data.csv'\ndata = pd.read_csv(data_url, index_col='id')\n\n# show first three rows\ndata.sample(3)\n\n\n\n  \n    \n      \n\n\n\n\n\n\ntitle\nviews\nyear\nlength\nlikes\ndislikes\n\n\nid\n\n\n\n\n\n\n\n\n\n\n43\nYERIKO vs PEPE GRILLO\n263529\n2018\n00:07:14\n5625\n655\n\n\n65\nGASPER vs SHADOW\n1219652\n2016\n00:04:52\n9546\n3275\n\n\n54\nWOS vs. SKONE\n4220417\n2017\n00:00:14\n58139\n2761"
  },
  {
    "objectID": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#data-pre-processing",
    "href": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#data-pre-processing",
    "title": "EDA. Analizing Videos‚Äô Details of Red Bull Batalla de Gallos‚Äô Youtube Channel",
    "section": "Data Pre-processing üßº",
    "text": "Data Pre-processing üßº\nLet‚Äôs get a glance of the structure of the dataset and their properties\n\ndata.info()\n\nNow, I‚Äôll start with some modifications on the features. From above, I noticed that the column length has time related values, so it‚Äôs requiered to give it a proper format and assign the data type.\n\ndata['length'] = pd.to_datetime(data['length'], format=\"%H:%M:%S\")\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 92 entries, 0 to 91\nData columns (total 6 columns):\n #   Column    Non-Null Count  Dtype         \n---  ------    --------------  -----         \n 0   title     92 non-null     object        \n 1   views     92 non-null     int64         \n 2   year      92 non-null     int64         \n 3   length    92 non-null     datetime64[ns]\n 4   likes     92 non-null     int64         \n 5   dislikes  92 non-null     int64         \ndtypes: datetime64[ns](1), int64(4), object(1)\nmemory usage: 5.0+ KB\n\n\nHow about the videos title? Are there any duplicated value?\n\ndata['title'].unique()\n\nAlmost correct, except for the name of a Frestyler which appears as VALLES T and VALLEST. Since it make reference to the same artist, we go on and replace the assure only one way of naming him.\nWe‚Äôll check the changes by filtering part of his nicknake that contain VALLES in the title column.\n\n# https://note.nkmk.me/en/python-str-replace-translate-re-sub/\ndata['title'] = [i.replace('VALLEST', 'VALLES-T').replace('VALLES T', 'VALLES-T') for i in data['title']]\n\ndata['title'][data['title'].str.contains('VALLES')]\n\nid\n10    BNET vs VALLES-T - Octavos | Red Bull Internac...\n17    BNET vs VALLES-T - Final | Red Bull Internacio...\n19    VALLES-T vs CHANG - Octavos | Red Bull Interna...\n25    VALLES-T vs JOKKER - Cuartos | Red Bull Intern...\n30    VALLES-T vs ACZINO - Semifinal | Red Bull Inte...\n35                              VALLES-T vs PEPE GRILLO\n38                                     VALLES-T vs BNET\n40                                      VALLES-T vs KDT\n46                                      VALLES-T vs WOS\n66                                VALLES-T vs CIUDADANO\n72                                     JOTA vs VALLES-T\nName: title, dtype: object\n\n\nOne these changes were made, we‚Äôre good to go to enrich the dataset."
  },
  {
    "objectID": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#feature-engineering",
    "href": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#feature-engineering",
    "title": "EDA. Analizing Videos‚Äô Details of Red Bull Batalla de Gallos‚Äô Youtube Channel",
    "section": "Feature Engineering üèóÔ∏è",
    "text": "Feature Engineering üèóÔ∏è\nMoving on, to enrich this small dataset & find some insights, I split the title column into Freestyler A & Freesttyler B that are the two rival artists. I used list comprehensions for achieving this task. As always, I printed some samples to check last changes.\n\n# https://stackoverflow.com/questions/40705480/python-pandas-remove-everything-after-a-delimiter-in-a-string\ndata['Freestyler_A'] = [i.replace('.', '').lower().split(' vs ')[0].strip().title() for i in data['title']]\ndata['Freestyler_B'] = [i.replace('.', '').split(' -')[0].lower().split(' vs ')[-1].strip().title() for i in data['title']]\n\n#Moving the columns position\ndata.columns.tolist()\n\ndata = data[['title', 'Freestyler_A', 'Freestyler_B', 'views', 'year', 'length', 'likes', 'dislikes']]\n\ndata.sample(5)\n\n\n\n  \n    \n      \n\n\n\n\n\n\ntitle\nFreestyler_A\nFreestyler_B\nviews\nyear\nlength\nlikes\ndislikes\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n58\nYENKY ONE vs. G\nYenky One\nG\n322030\n2017\n1900-01-01 00:04:48\n5734\n1363\n\n\n15\nACZINO vs SHIELD MASTER\nAczino\nShield Master\n154369\n2020\n1900-01-01 00:06:51\n5439\n471\n\n\n23\nJOKKER vs LITZEN\nJokker\nLitzen\n2546416\n2019\n1900-01-01 00:09:46\n40625\n900\n\n\n72\nJOTA vs VALLES-T\nJota\nValles-T\n1073224\n2016\n1900-01-01 00:05:04\n13330\n6872\n\n\n11\nEXODO LIRICAL vs MAC\nExodo Lirical\nMac\n105707\n2020\n1900-01-01 00:06:13\n5319\n254"
  },
  {
    "objectID": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#exploratory-data-analysis",
    "href": "posts/exploratory-data-analysis-youtube-channel-red-bull/index.html#exploratory-data-analysis",
    "title": "EDA. Analizing Videos‚Äô Details of Red Bull Batalla de Gallos‚Äô Youtube Channel",
    "section": "Exploratory Data Analysis üí°",
    "text": "Exploratory Data Analysis üí°\nNow we are finally in the exciting part of this notebook: EDA Process.\n\nGeneral View of the Dataset\nLet‚Äôs take a look at the datafame‚Äôs properties for a better understanding to know what needs to be done. To do so, we can use the info() method which gives us the number of columns, columns names and their data types all together.\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 92 entries, 0 to 91\nData columns (total 6 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   title     92 non-null     object\n 1   views     92 non-null     int64 \n 2   year      92 non-null     int64 \n 3   length    92 non-null     object\n 4   likes     92 non-null     int64 \n 5   dislikes  92 non-null     int64 \ndtypes: int64(4), object(2)\nmemory usage: 5.0+ KB\n\n\nHow about how many rows and columns the dataset has?\n\ndata.shape\n\nprint(\"The Dataset has\", data.shape[0],\"rows with\", data.shape[1],\"features.\")\n\nThe Dataset has 92 rows with 6 features\n\n\nLet‚Äôs summarize some statistical metrics of the dataset by using describe() function.\n\ndata.describe().T\n\n\n\n  \n    \n      \n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nviews\n92.0\n4.773642e+06\n8.383880e+06\n47082.0\n661099.50\n1715757.0\n4372855.00\n44005544.0\n\n\nyear\n92.0\n2.017609e+03\n1.670405e+00\n2015.0\n2016.00\n2018.0\n2019.00\n2020.0\n\n\nlikes\n92.0\n6.649529e+04\n1.203502e+05\n1510.0\n9971.25\n29532.5\n59178.00\n729024.0\n\n\ndislikes\n92.0\n8.634130e+03\n2.347569e+04\n55.0\n607.00\n1645.0\n5667.75\n194847.0\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nHow about how many unique values it has?\n\ndata.nunique()\n\ntitle       88\nviews       92\nyear         6\nlength      77\nlikes       92\ndislikes    92\ndtype: int64\n\n\n\n\nLet‚Äôs Analyze each Feature\nUnivariate Analysis\nOnce we get a general glance of the datasets properties & statistics, now we can proceed to leverage the power of Data Visualization (graphs) to better understand any aspect of each feature of the dataset.\n\n#collapse-hide\ng = sns.catplot(data=data, x='year', kind='count', palette=sns.blend_palette(['#203175','#E30C4C','#FDCA24'])) # Set your custom color palette\ng.set(ylabel=None)\n\nplt.title('Number of Videos by Year');\n\n\n\n\n\n\n\n\nLet‚Äôs find out, how many times each Freestyler appears on the video‚Äôs title? Put it in other words, how many times Each Freestyler has a battle participation on this international event?\n\nF_concated = pd.concat([data['Freestyler_A'], data['Freestyler_B']])\nF_concated.value_counts()\n\nAczino       21\nArkano       14\nValles-T     11\nSkone        10\nBnet         10\n             ..\nYeriko        1\nRvs           1\nDozer         1\nRedencion     1\nMrjunior      1\nLength: 62, dtype: int64\n\n\nThe same as above, but graphically presented\n\n#collapse-hide\nimport matplotlib.ticker as mticker\n\nF_concated.value_counts().sort_values(ascending=True).plot(kind='barh', figsize=(12, 15), color=['#203175','#E30C4C','#FDCA24'])\n# Show x Axis as integer\nplt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n\ng.set(xlabel=None)\ng.set(ylabel=None)\n\nplt.title('Numbers of Appereances By each Freestyler in any International from 2015 to 2020');\n\n\n\n\n\n\n\n\nNow I wanted to present the distribution of each variable. In this case, the distribution of Views feature.\n\n#collapse-hide\ncustomHistSettings(figW=9)\n\ng=sns.histplot(data.views, bins=25, kde=True, stat='density', linewidth=0)\nplt.xlabel('Views')\nplt.title('Distribution of Views')\n\nxlabels = ['{:,.0f}'.format(x) + 'M' for x in g.get_xticks()/(1000000)]\ng.set_xticklabels(xlabels)\n\n#Plooting the median\nmean = data.views.median()\nmean\nplt.axvline(mean, 0, 1, color='#E30C4C');\n\n\n\n\n\n\n\n\nLet‚Äôs present the same as before but using a boxplot graph that help us to undestard the data ranges by quartiles and also point out any outlies that lies outside the whiskers.\n\n#collapse-hide\ng=sns.catplot(data=data, x='views', kind='box')\ncustomPlotSettings(figW=9)\nplt.title('Distribution of Views (M)')\n\nText(0.5, 1.0, 'Distribution of Views')\n\n\n\n\n\n\n\n\n\nFrom above, we can tell that many videos has less than 1 Million views and that there are some outiers, even so, 3 of them has over 40 million views.\nHow behaves the Likes feature?\n\n#collapse-hide\ncustomHistSettings(figW=9)\n\ng=sns.histplot(data.likes, bins=25, kde=True, stat='density', linewidth=0)\nplt.xlabel('Likes')\nplt.title('Distribution of Likes')\n\nxlabels = ['{:,.0f}'.format(x) + 'K' for x in g.get_xticks()/1000]\ng.set_xticklabels(xlabels)\n\n#Plooting the median\nmean = data.likes.median()\nmean\nplt.axvline(mean, 0, 1, color='#E30C4C');\n\n\n\n\n\n\n\n\nMany of the videos are quite popular & likeables, they range from between 100k & 300k of likes, except for the outlier that has more than 700k.\nNow let‚Äôs analyzed the opposite, the dislikes feature.\n\n#collapse-hide\ncustomHistSettings(figW=9)\n\ng=sns.histplot(data.dislikes, bins=25, kde=True, stat='density', linewidth=0)\nplt.xlabel('Dislikes')\nplt.title('Distribution of Dislikes')\n\nxlabels = ['{:,.0f}'.format(x) + 'K' for x in g.get_xticks()/1000]\ng.set_xticklabels(xlabels)\n\n#Plooting the median\nmean = data.dislikes.median()\nmean\nplt.axvline(mean, 0, 1, color='#E30C4C');\n\n\n\n\n\n\n\n\nMany of the videos falls into the range of 0k to 25k of dislikes, which is okey for videos with views over 170k and likes on average of +20k\n\n\nHow They Behave if We Put Them Together? ü§î\nBivariate Analysis\nLet‚Äôs moving on to find out how these featues behave when we analyzed them together.\nWe can see that, on average, many views were gathered mostly in 2019 & 2015, the latter one also surpass the other four years. Also, the year with less views was 2020.\n\n#collapse-hide\ng=sns.catplot(data=data, x='year', y='views', estimator=np.mean, kind='bar', palette=sns.blend_palette(['#203175','#E30C4C','#FDCA24']))\nplt.title('Average of Views By Year')\n\ncustomPlotSettings(figW=9);\n\n\n\n\n\n\n\n\nWhen plotting the views by year, it‚Äôs noticeable that most years, except for 2020, have outlies which will increment the average of views. Furthermore, 2005, 2018 y 2019 have battle videos (outliers) with more than 40M of views.\n\n#collapse-hide\ng=sns.catplot(data=data, x='year', y='views', kind='box', palette=sns.blend_palette(['#203175','#E30C4C','#FDCA24']))\nplt.title('Distribution of Views By Year')\n\ncustomPlotSettings(figW=9);\n\n\n\n\n\n\n\n\n\n#collapse-hide\ng=sns.catplot(data=data, x='year', y='likes', estimator=np.mean, kind='bar', palette=sns.blend_palette(['#203175','#E30C4C','#FDCA24']))\nplt.title('Average of Likes By Year')\n\ncustomPlotSettings(figW=9);\n\n\n\n\n\n\n\n\nHere, we can see that videos from 2018 and 2019 has the most number of likes (+700K). Also, except for 2019, most years has a close range with not much variation.\n\n#collapse-hide\ng=sns.catplot(data=data, x='year', y='likes', kind='box', palette=sns.blend_palette(['#203175','#E30C4C','#FDCA24']))\nplt.title('Distribution of Likes By Year')\n\ncustomPlotSettings(figW=9);\n\n\n\n\n\n\n\n\n\n\nCorrelation between Numerical Features\nNow let‚Äôs continue to analyze if there is any correlation between Numerical Features. I used .corr() and then seaborn‚Äôs .heatmap() function to plot a heatmap graph for an easy-to-digest understanding of correlation for each numerical features\n\n#collapse-hide\n# Calculate correlation between each pair of variable\ncorr = data.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Insert a figure\nf, ax = plt.subplots(figsize=(10, 7))\ncmap = sns.diverging_palette(10, 220, as_cmap=True)\n\n# Draw the heatmap with the mask\nax = sns.heatmap(corr,\n                 mask=mask,\n                 cmap=cmap,\n                 annot=True,\n                 annot_kws= {'size':11},\n                 square=True, xticklabels=True,\n                 yticklabels=True,\n                 linewidths=.5,\n                 cbar_kws={'shrink': .5},\n                 ax=ax\n                )\nax.set_title('Correlation between Numerical Features', fontsize=20);\n\n\n\n\n\n\n\n\nWe can drew from the previous graph that there is a high positive correlation between views & likes (not surprising). Besides that, theres is a high negative correlation between years & views and a low negative corrlation between years and dislikes.\nHaving into consideration the previous insight, let‚Äôs plot a Scatterplot to show what this correlation between views and likes looks like.\n\n#collapse-hide\nplt.figure(figsize=(12,6))\n# use the scatterplot function to build the bubble map\ng=sns.regplot(data=data, x='likes', y='views')\nsns.despine()\n\n# Add titles (main and on axis)\nplt.xlabel('Likes')\nplt.ylabel('Views')\nplt.title('Relationshitp Between Views & Likes');\n\n\n\n\n\n\n\n\nFinally, let‚Äôs plot it by years to see how this relationship behaves.\n\n#collapse-hide\ng = sns.relplot(data=data,\n                 x='likes',\n                 y='views',\n                 col='year',\n                 kind='scatter',\n                 col_wrap=3,\n                 height=6)\n\ng.fig.subplots_adjust(top=0.9) # adjust the Figure in g\ng.fig.suptitle('Relationshitp Between Views & Likes By Year');\n\nText(0.5, 0.98, 'Relationshitp Between Views & Likes By Year')\n\n\n\n\n\n\n\n\n\nYou‚Äôre Awesome üòç, you just reached the end of this post. If you have any questions just drop me a message on my LikedIn. Also, any suggestion or kudos would be quite appreciated. Did you find it useful? Check out my other posts here, I‚Äôm sure you‚Äôll find something interesting üí°.\nShare this post with your friends/colleagues, and if you are in a good mood, buy me a cup of coffee ‚òï. Nos vemos! üèÉüí®"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  }
]