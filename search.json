[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Projects",
    "section": "",
    "text": "Web Scraping Video Details from a Youtube Channel using Selenium\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a Word Cloud in an image‚Äôs shape by Scraping an Article\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 1, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 28, 2024\n\n\nTristan O‚ÄôMalley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#tldr",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#tldr",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "TL;DR ü§ì",
    "text": "TL;DR ü§ì\nHave you ever crossed some blog post, video or presentation having A fun way to show & analize which are the most relevant topics(repeated words) on a text. Bellow I show you some examples whe can create, it‚Äôs like art made out of words üé®\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#installing-libraries",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#installing-libraries",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "Installing Libraries ‚úîÔ∏è",
    "text": "Installing Libraries ‚úîÔ∏è\nFirst, you need to install all libraries you‚Äôll be using.\n\n!pip install numpy\n!pip install matplotlib\n!pip install newspaper3k\n!pip install pillow\n!pip install wordcloud\n!pip install nltk\n\nprint('Library installation Done!')\n\nCollecting newspaper3k\n  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 211 kB 7.6 MB/s \nRequirement already satisfied: lxml&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\nRequirement already satisfied: Pillow&gt;=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\nCollecting feedparser&gt;=5.2.1\n  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81 kB 10.0 MB/s \nRequirement already satisfied: python-dateutil&gt;=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\nRequirement already satisfied: requests&gt;=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\nCollecting jieba3k&gt;=0.35.1\n  Downloading jieba3k-0.35.1.zip (7.4 MB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.4 MB 60.7 MB/s \nRequirement already satisfied: nltk&gt;=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\nCollecting feedfinder2&gt;=0.0.4\n  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\nCollecting cssselect&gt;=0.9.2\n  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\nRequirement already satisfied: beautifulsoup4&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\nCollecting tinysegmenter==0.3\n  Downloading tinysegmenter-0.3.tar.gz (16 kB)\nRequirement already satisfied: PyYAML&gt;=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\nCollecting tldextract&gt;=2.0.1\n  Downloading tldextract-3.1.2-py2.py3-none-any.whl (87 kB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87 kB 5.8 MB/s \nRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2&gt;=0.0.4-&gt;newspaper3k) (1.15.0)\nCollecting sgmllib3k\n  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (3.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (2021.10.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (1.24.3)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (2.10)\nCollecting requests-file&gt;=1.4\n  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\nRequirement already satisfied: filelock&gt;=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract&gt;=2.0.1-&gt;newspaper3k) (3.4.2)\nBuilding wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n  Building wheel for tinysegmenter (setup.py) ... done\n  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13553 sha256=73e9156b3836ce85dc512439196c449be3d5d33749708372688c3f5d99f1a059\n  Stored in directory: /root/.cache/pip/wheels/df/67/41/faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa\n  Building wheel for feedfinder2 (setup.py) ... done\n  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3357 sha256=fe3990b9a89a29307f18a4767d156083b809112c9ca71e5ef2bbb556ddbd874e\n  Stored in directory: /root/.cache/pip/wheels/7f/d4/8f/6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346\n  Building wheel for jieba3k (setup.py) ... done\n  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398404 sha256=86ab131c0f61eeaef932b8967c1915eaaabc4a1089ea070652c21631f839e9b4\n  Stored in directory: /root/.cache/pip/wheels/4c/91/46/3c208287b726df325a5979574324878b679116e4baae1af3c3\n  Building wheel for sgmllib3k (setup.py) ... done\n  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=9ad6b6bbe0b981e0262c6294017917f7b9cb32e8af7463272df81aaf6f7f2001\n  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\nSuccessfully built tinysegmenter feedfinder2 jieba3k sgmllib3k"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#importing-libraries",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#importing-libraries",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "Importing Libraries üß∞",
    "text": "Importing Libraries üß∞\n\nfrom newspaper import Article\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, ImageColorGenerator\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download(\"stopwords\")\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#step-01-getting-the-corpus-from-an-article",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#step-01-getting-the-corpus-from-an-article",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "Step 01: Getting the Corpus from an Article üìù",
    "text": "Step 01: Getting the Corpus from an Article üìù\n\narticle = Article('https://www.repsol.pe/es/sala-prensa/notas-prensa/comunicado.cshtml')\narticle.download()\narticle.parse()\n\n\nGenerate a Simple Word Cloud Image\n\n# Generate a word cloud image\nwc = WordCloud()\nwc.generate(article.text)\n\n# Display the generated image:\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nGenerate a Customized Word Cloud Image\n\nstopwords = set(stopwords.words('spanish', 'english'))\n\nstopwords.update(['ello', 'cinco', 'd√≠a'])\n\nConverting an image to a numpy array results in an array containing a sequence of values that each represent an individual pixel in the image.\n\nmask = np.array(Image.open('repsol.jpg'))\n# mask = np.array(Image.open('pedro-castillo.jpg'))\n\nwc = WordCloud(stopwords=stopwords,\n               background_color=\"white\",\n               max_words=2000,\n               mask=mask,\n               )\n\nwc.generate(article.text)\n\n# Display the generated image:\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nmask = np.array(Image.open('repsol.jpg'))\n# mask = np.array(Image.open('pedro-castillo.jpg'))\n\nwc = WordCloud(stopwords=stopwords,\n               background_color=\"white\",\n               max_words=2000,\n               mask=mask,\n               max_font_size=256,\n            #    random_state=42,\n            #    width=mask.shape[1],\n            #    height=mask.shape[0]\n               )\n\nwc.generate(article.text)\n\n# create coloring from image\nimage_colors = ImageColorGenerator(mask)\nplt.figure(figsize=[16,14])\nplt.imshow(wc.recolor(color_func=image_colors), interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#references",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#references",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "References:",
    "text": "References:\n\nhttps://medium.com/towards-data-science/create-word-cloud-into-any-shape-you-want-using-python-d0b88834bc32\nhttps://www.repsol.pe/es/sala-prensa/notas-prensa/comunicado.cshtml"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#tldr",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#tldr",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "TL;DR üëÄ",
    "text": "TL;DR üëÄ\nThis project is to perform most common tasks of Web Scraping by using Selenium as a Scraper Tool and Python for coding. The output will be a CSV file containing information of all International Matches of Freestyle organized by Red Bull from 2015 to 2020 (filtered by internacional and vs keywords). Here you can take a peek or download the csv file which is the result of this project. (Also added at the bottom of this notebook)\nFYI: Red Bull Batalla de los Gallos is the Most Recognized Freestyle Competition in Spanish that brings together the 16 winning Freestylers from the competitions organized by Red Bull in each country. After all matches only one of them is crowned as international champion Click here to learn more\nHer I leave you a screenshot of the Youtube Channel used for this project:\n\n\n\nred bull batalla de los gallos 2020.JPG\n\n\nSatisfying the requirements\nAs always, let‚Äôs first install libraries we‚Äôll be using thought the project These ones are Chromium(browser), Selenium (scraper tool), and tqdm (progress bar)."
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#installing-libraries",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#installing-libraries",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Installing Libraries ‚úîÔ∏è",
    "text": "Installing Libraries ‚úîÔ∏è\n\n# install chromium, selenium and tqdm\n!apt update\n!apt install chromium-chromedriver\n!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n!pip install selenium\n!pip install tqdm\n\nprint('Library installation Done!')\n\n0% [Working]            Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Waiting for header                                                                               Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n0% [Connecting to archive.ubuntu.com] [2 InRelease 14.2 kB/88.7 kB 16%] [Waitin                                                                               Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n0% [Connecting to archive.ubuntu.com (91.189.88.152)] [2 InRelease 14.2 kB/88.70% [Connecting to archive.ubuntu.com (91.189.88.152)] [2 InRelease 14.2 kB/88.7                                                                               Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n0% [Connecting to archive.ubuntu.com (91.189.88.152)] [2 InRelease 14.2 kB/88.70% [3 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)                                                                               Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\nHit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\nGet:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\nHit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\nGet:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\nGet:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [41.5 kB]\nHit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\nGet:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\nHit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\nGet:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,700 kB]\nGet:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [870 kB]\nFetched 2,884 kB in 4s (790 kB/s)\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\n17 packages can be upgraded. Run 'apt list --upgradable' to see them.\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\nSuggested packages:\n  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\nThe following NEW packages will be installed:\n  chromium-browser chromium-browser-l10n chromium-chromedriver\n  chromium-codecs-ffmpeg-extra\n0 upgraded, 4 newly installed, 0 to remove and 17 not upgraded.\nNeed to get 81.0 MB of archives.\nAfter this operation, 273 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 87.0.4280.66-0ubuntu0.18.04.1 [1,122 kB]\nGet:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 87.0.4280.66-0ubuntu0.18.04.1 [71.7 MB]\nGet:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 87.0.4280.66-0ubuntu0.18.04.1 [3,716 kB]\nGet:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 87.0.4280.66-0ubuntu0.18.04.1 [4,488 kB]\nFetched 81.0 MB in 5s (15.5 MB/s)\nSelecting previously unselected package chromium-codecs-ffmpeg-extra.\n(Reading database ... 145480 files and directories currently installed.)\nPreparing to unpack .../chromium-codecs-ffmpeg-extra_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\nUnpacking chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\nSelecting previously unselected package chromium-browser.\nPreparing to unpack .../chromium-browser_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\nUnpacking chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\nSelecting previously unselected package chromium-browser-l10n.\nPreparing to unpack .../chromium-browser-l10n_87.0.4280.66-0ubuntu0.18.04.1_all.deb ...\nUnpacking chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\nSelecting previously unselected package chromium-chromedriver.\nPreparing to unpack .../chromium-chromedriver_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\nUnpacking chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\nSetting up chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\nSetting up chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\nupdate-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\nupdate-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\nSetting up chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\nSetting up chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\nProcessing triggers for hicolor-icon-theme (0.17-2) ...\nProcessing triggers for mime-support (3.60ubuntu1) ...\nProcessing triggers for man-db (2.8.3-2ubuntu0.1) ...\ncp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\nCollecting selenium\n  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 911kB 8.4MB/s \nRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\nInstalling collected packages: selenium\nSuccessfully installed selenium-3.141.0\nRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\nLibrary installation Done!"
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#importing-libraries",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#importing-libraries",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Importing Libraries üß∞",
    "text": "Importing Libraries üß∞\nOnce Installed, We‚Äôll procced to import them.\n\n# set options to be headless\nfrom selenium import webdriver\n#the followings are to avoid NoSuchElementException by using WebDriverWait - to wait until an element appears in the DOM\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n# add random pause seconds to avoid getting blocked\nimport time, random\n\n# to use a progress bar for visual feedback\nfrom tqdm import tqdm\n# to get the current date\nfrom datetime import date\n\n# to save Dataframe into a CSV file format\nimport pandas as pd\nimport numpy as np\n\n# Upload or download files\nfrom google.colab import files\n\nprint('All Libraries imported!')\n\nAll Libraries imported!"
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-01-accessing-the-web-page",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-01-accessing-the-web-page",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Phase 01: Accessing the Web Page üåê",
    "text": "Phase 01: Accessing the Web Page üåê\n\nOpening the Browser and Visiting the Target Web Page\n\n# Setting options for the web browser\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('-headless')\nchrome_options.add_argument('-no-sandbox')\nchrome_options.add_argument('-disable-dev-shm-usage')\n\n# Open browser, go to a website, and get results\nbrowser = webdriver.Chrome('chromedriver',options=chrome_options)\nbrowser.execute_script(\"return navigator.userAgent;\")\nprint(browser.execute_script(\"return navigator.userAgent;\"))\n\nchannel_url = 'https://www.youtube.com/c/RedbullOficialGallos/videos'\n\n# Open website\nbrowser.get(channel_url)\n\n# Print page title\nprint(browser.title)\n\nMozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/87.0.4280.66 Safari/537.36\nRed Bull Batalla De Los Gallos - YouTube\n\n\n\n\nReaching the bottom of this Dynamically Loaded Page\nSince this Page‚Äôs content is dinamically loaded by scrolling down, we use a function to dinamically change the scrollHeight.\n\ndef scroll_to_the_page_bottom(browser):\n    height = browser.execute_script(\"return document.documentElement.scrollHeight\")\n    lastheight = 0\n\n    while True:\n        if lastheight == height:\n            break\n        lastheight = height\n        browser.execute_script(\"window.scrollTo(0, \" + str(height) + \");\")\n        # Pause 2 seconds per iteration\n        time.sleep(2)\n        height = browser.execute_script(\"return document.documentElement.scrollHeight\")\n\n    print('The scroll down reached the bottom of the page, all content loaded!')\n\nscroll_to_the_page_bottom(browser)\n\nThe scroll down reached the bottom of the page, all content loaded!"
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-02-scraping-the-data",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-02-scraping-the-data",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Phase 02: Scraping the data ‚õèÔ∏è",
    "text": "Phase 02: Scraping the data ‚õèÔ∏è\n\nGetting the links of all videos\n\nvideo_anchors = browser.find_elements_by_css_selector('#video-title')\n\nprint(f'This Channel has {len(video_anchors)} videos published')\n\nThis Channel has 3226 videos published\n\n\nFor this project, we‚Äôre gonna gather all the videos link that contains the words: - internacional + vs\nTo do so, we‚Äôll use a list comprehension along with all().\nWe‚Äôre using all() instead of any because we want to filter having all elements present inside each text item. Think about it as the and operator.the any() method then would be like any, because any text item that match at least one of the matches would be inserted in the list called video_links.\n\n# initializing  list of keywords to filter (16 videos only should be)\n\nmatchers = [x.lower() for x in ['Internacional', 'vs']]\nvideo_links = [link.get_attribute('href') for link in tqdm(video_anchors, position=0) if all(match in link.text.lower() for match in matchers)]\n\nprint(len(video_links))\n\n#Show the first link\nvideo_links[0]\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3226/3226 [02:30&lt;00:00, 21.44it/s]\n\n\n95\n\n\n'https://www.youtube.com/watch?v=Fwda4AWZ6V4'\n\n\n\n\nGetting all details for each video\nNow, we are going to retrieve the details of all videos we are interested in such us title, views, upload date, lenght of video,likes and dislikes. We‚Äôll use a for loop to iterate over the video_links variable which contains all videos‚Äô urls and per each url we extract the data and save them in variables.\nOnce saved the collection or variables are stored in a Dictionary called data. Finally each dictionary are saved in the variable video_details which basically is a list of dictionaries containing all details per each video scraped. Let‚Äôs jump in the code to better understanding.\n\nvideo_details = []\n\ndelay = 10\n\nfor link in tqdm(video_links, desc='Getting all details for each video', position=0, leave=True):\n\n    try:\n        browser.get(link)\n    except:\n        continue\n\n    # Pause 3 seconds to load content\n    time.sleep(3)\n\n    # Get element  after explicitly waiting for up to 10 seconds\n    title = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , '.title'))).text\n    views = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , '.view-count'))).text.split('\\n')[0].split()[0]\n    upload_date = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , '#date &gt; yt-formatted-string'))).text\n    length = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , '.ytp-time-duration'))).text\n    likes = WebDriverWait(browser, delay).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR , '#top-level-buttons #text')))[0].get_attribute('aria-label').split()[0]\n    dislikes = WebDriverWait(browser, delay).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR , '#top-level-buttons #text')))[1].get_attribute('aria-label').split()[0]\n    url = link\n\n    # inserting all data in the list. We'll also use aternary expression/operator to save a value depending on a condition\n    data = {\n            'title': title,\n            'views': views,\n            'upload_date': upload_date,\n            'length': length,\n            'likes': likes,\n            'dislikes': dislikes,\n            'url': url\n            }\n\n    video_details.append(data)\n\n    # Pause 3 seconds per iteration\n    time.sleep(3)\n\n# Close the browser once the for loop is done\nbrowser.quit()\n\nprint(f'All details of {len(video_links)} videos successfully retrieved')\n\nGetting all details for each video: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [11:53&lt;00:00,  7.51s/it]\n\n\nAll details of 95 videos successfully retrieved\n\n\nExcellent, we just got all videos details and insert them into a list called video_details for convinence.\nTo verify the details per each video were saved correctly let‚Äôs print the first element whitin the list.\n\nvideo_details[0]\n\n{'dislikes': '270',\n 'length': '6:16',\n 'likes': '14,040',\n 'title': 'ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | Red Bull Internacional 2020',\n 'upload_date': 'Dec 12, 2020',\n 'url': 'https://www.youtube.com/watch?v=Fwda4AWZ6V4',\n 'views': '577,503'}"
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-03-saving-the-gathered-data",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#phase-03-saving-the-gathered-data",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Phase 03: Saving the Gathered Data üíæ",
    "text": "Phase 03: Saving the Gathered Data üíæ\n\nSaving data to a CSV file\nTo dynamically name our output csv file, we‚Äôll use from datetime import date which is already imported in the Importing Libraries Section. Let‚Äôs first get the current date and the Youtube Channel‚Äôs Name from the url we provided.\n\ntoday = date.today()\n\n# Month abbreviation, day and year\ntodays_date = today.strftime(\"%b-%d-%Y\")\nprint(f'Fecha de hoy: {todays_date}')\n\nchannel_name = channel_url.split('/')[4]\nprint(channel_name)\n\nFecha de hoy: Dec-27-2020\nRedbullOficialGallos\n\n\nNow, let‚Äôs put all variables together to name the file.\n\n# Programatically naming csv file\ncsv_file_name = f'{channel_name}_videos_details_{todays_date}.csv'.lower()\nprint(csv_file_name)\n\n# Assign columns names\nfield_names = ['title', 'views', 'upload_date', 'length', 'likes', 'dislikes', 'url']\n\nredbulloficialgallos_videos_details_dec-27-2020.csv\n\n\nWe‚Äôre almost done, with the csv_file_name and field_names variables, let‚Äôs turn video_details into a Dataframe which can be used later for any analysis. We‚Äôll need to install Pandas and Numpy to do so. These libraries were already imported in the Importing Libraries Section.\n\n# Create DataFrame\ndf = pd.DataFrame(video_details, columns=field_names)\n\n# Show first 3 rows to verify the dataframe creation\ndf.head(3)\n\n\n\n\n\n\n\n\n\ntitle\nviews\nupload_date\nlength\nlikes\ndislikes\nurl\n\n\n\n\n0\nACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R...\n577,503\nDec 12, 2020\n6:16\n14,040\n270\nhttps://www.youtube.com/watch?v=Fwda4AWZ6V4\n\n\n1\nEXODO LIRICAL vs RAPDER - Semifinal | Red Bull...\n238,463\nDec 12, 2020\n12:30\n8,135\n927\nhttps://www.youtube.com/watch?v=wIcz1_7qx-4\n\n\n2\nACZINO vs SKONE - Semifinal | Red Bull Interna...\n756,352\nDec 12, 2020\n10:06\n18,458\n1,146\nhttps://www.youtube.com/watch?v=yv8yFhRsWVc\n\n\n\n\n\n\n\n\n\n# Save Dataframe into a CSV file format\ndf.to_csv(csv_file_name, index=False)\n\n# Read the file and print the first 3 rows to verify its creation\npd.read_csv(csv_file_name).head(3)\n\n\n\n\n\n\n\n\n\ntitle\nviews\nupload_date\nlength\nlikes\ndislikes\nurl\n\n\n\n\n0\nACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R...\n577,503\nDec 12, 2020\n6:16\n14,040\n270\nhttps://www.youtube.com/watch?v=Fwda4AWZ6V4\n\n\n1\nEXODO LIRICAL vs RAPDER - Semifinal | Red Bull...\n238,463\nDec 12, 2020\n12:30\n8,135\n927\nhttps://www.youtube.com/watch?v=wIcz1_7qx-4\n\n\n2\nACZINO vs SKONE - Semifinal | Red Bull Interna...\n756,352\nDec 12, 2020\n10:06\n18,458\n1,146\nhttps://www.youtube.com/watch?v=yv8yFhRsWVc\n\n\n\n\n\n\n\n\nYay! You reach the end of this article. By now you know how retrieve all videos details from a Youtube Channel. As earlier mentioned, the scraped data should be in the generated csv file. If you worked on it in Jupyter Notebook or your Favorite Code Editor, you can find it in the same folder where you ran your¬†.pynb file. But, if you worked on Google Colab (like me), you need to use the following code to download it: from google.colab import files. This library was already imported in the Importing Libraries Section\n\n# Download the file that contains the scraped table\nfiles.download(csv_file_name)\n\nprint('In a moment the option \"Save As\" will appear to download the file...')\n\n\n\n\n\n\n\nIn a moment the option \"Save As\" will appear to download the file..."
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#takeaways",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#takeaways",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "Takeaways",
    "text": "Takeaways\n\nSince Youtube is a loading content Page, I‚Äôve decided to use Selenium as a tool to scrape\nWhen scraping the video_lenght of a video, For some reason some of them return a None value, so we need to use its text version from the arial-label of the same element\nI‚Äôve decided to use Pandas instead of the CSV library to create and save the Dataframe into a CSV file because it‚Äôs easier to use.\nThe elements were accesed using its css selector because is faster and easier to read\nThis project is to show off skills of Web Scraping using Selenium. For the next tutorial, we‚Äôll do the same but using the Youtube API\nSince this project‚Äôs scope is just to gather all data needed in a machine readable format (CSV). What remains to be done is Data Preprocessing and Exploratory Data Analysis\n\nHere I leave you the csv file we‚Äôve just scraped from Youtube."
  },
  {
    "objectID": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#references",
    "href": "posts/web-scraping-a-youtube-channel-with-selenium/index.html#references",
    "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
    "section": "References",
    "text": "References\nThis is where I got inspiration from\nHow to Extract & Analyze YouTube Data using YouTube API?\nUsing Selenium wthinin Google Colab\nScroll to end of page in dynamically loading webpage. Answered by: user53558\nSaving a Pandas Dataframe as a CSV\nScroll to end of page in dynamically loading webpage\nAsign variables to dictionary based on value\nWebDriverWait on finding element by CSS Selector\nUse of if else inside a dict to set a value to key using Python\nHow to get back to the for loop after exception handling\ntqdm printing to newline"
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "",
    "text": "¬øAlguna vez te topaste con alguna tabla de Wikipedia que justo necesitabas para tu investigaci√≥n o proyecto de an√°lisis? Como la tabla de medallas de los Juegos Panamericanos, Las Elecciones Presidenciales o datos de las Exportaciones de un Pais, ¬øSi? Bueno, Aqu√≠ aprender√°s como obtener datos de cualquier tabla.\nEsta es la tabla que buscamos obtener que contiene informaci√≥n del Censo de la Provincia de Trujillo del a√±o 2017. Para hacer Web Scraping a esta p√°gina, utilizaremos las bibliotecas Requests y Pandas de Python.\nAl finalizar este Tutorial, tendr√°s como resultado una tabla lista para su an√°lisis. Lo podr√°s guardar en tu directorio de trabajo o descargar en tu computadora como archivo CSV (o el formato que gustes). Espero que te resulte √∫til; sin m√°s que a√±adir‚Ä¶ ‚ÄúHappy Coding!‚Äù"
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-1-conecci√≥n-y-obtenci√≥n-de-codigo-fuente",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-1-conecci√≥n-y-obtenci√≥n-de-codigo-fuente",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "Fase 1: Conecci√≥n y obtenci√≥n de codigo fuente",
    "text": "Fase 1: Conecci√≥n y obtenci√≥n de codigo fuente\nComenzaremos importando la biblioteca requests para realizar la petici√≥n HTTP que nos devolver√° el c√≥digo fuente de la p√°gina web y pandas, para usar su m√©todo .read_html() que nos ayudar√° a extraer todas las tablas HTML. Usaremos este m√©todo y no la biblioteca Beautiful Soup porque de esta forma es mucho m√°s f√°cil y funciona bien en cualquier p√°gina web que contenga tablas HTML debido a su estructura simple de leer.\n\nPara tu conocimiento: Usando .read_html() no solo nos permite extraer tablas HTML desde un enlace (lo que usaremos hoy), sino tambi√©n desde un archivo html o una cadena de caracteres (string) que contenga HTML.\n\n\n# Hacer peticion HTTP\nimport requests\n# Manipular c√≥digo y guardar datos tabulares en archivo CSV\nimport pandas as pd\n\n\n# url de la p√°gina web a ¬´escrapear¬ª\nurl = 'https://es.wikipedia.org/wiki/Provincia_de_Trujillo_(Per%C3%BA)'\n\n# pasar \"User-agent\" para simular interacci√≥n con la p√°gina usando Navegador web\nheaders = {\"User-agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'}\n\nrespuesta = requests.get(url, headers=headers)\n\n# El c√≥digo de respuesta &lt;200&gt; indicar√° que todo sali√≥ bien\nprint(respuesta)\n\n&lt;Response [200]&gt;\n\n\nAparte de la respuesta positiva que nos devolvi√≥ la petici√≥n, seguro te diste cuenta que pasamos el par√°metro headers al m√©todo requests.get(), esto sirve para evitar que el servidor de la p√°gina web nos bloqueen el acceso. Aunque con Wikipedia no tenemos ese problema, otras p√°ginas son restrictivas cuando se trata de bots tratando de ¬´escrapear¬ª sus datos. Siguiento esta buena pr√°ctica, nos ahorarremos dolores de cabeza luego.\nPor √∫ltimo, guardamos en una variable todas las tablas encontradas en el objecto HTML.\n\nall_tables = pd.read_html(respuesta.content, encoding = 'utf8')"
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-2-an√°lisis-de-estructura-html-y-extracci√≥n-de-datos",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-2-an√°lisis-de-estructura-html-y-extracci√≥n-de-datos",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "Fase 2: An√°lisis de estructura HTML y extracci√≥n de datos",
    "text": "Fase 2: An√°lisis de estructura HTML y extracci√≥n de datos\nAhora veamos cuantas tablas hay en la p√°gina web\n\nprint(f'Total de tablas encontradas: {len(all_tables)}')\n\nTotal de tablas encontradas: 6\n\n\nDebido a que hay varias tablas, usaremos el par√°metro match y le pasaremos una palabra o frase que solo se encuentre en la tabla que nos interesa.\n\nmatched_table = pd.read_html(respuesta.text, match='Fuente: Censos Nacionales 2017')\n\n# imprime numero de tablas que coinciden con parametro match\nprint(f'Total de tablas encontradas: {len(matched_table)}')\n\nTotal de tablas encontradas: 1\n\n\nGuardamos nuestra tabla de inter√©s en una variable.\n\n# Guardar tabla en variable con nombre sem√°ntico\ncenso_trujillo = matched_table[0]\n\n# Verificamos si es la tabla que buscamos\ncenso_trujillo.tail(5)\n\n\n\n\n\n\n\n\n\nUBIGEO\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\n\n\n8\n130109\nSalaverry\n5599\n5244\n18¬†944\n\n\n9\n130110\nSimbal\n1662\n1151\n4061\n\n\n10\n130111\nV√≠ctor Larco Herrera\n19¬†543\n18¬†461\n68¬†506\n\n\n11\nNaN\nTOTAL\n273¬†619\n250¬†835\n970¬†016\n\n\n12\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\n\n\n\n\n\n\n\n\n¬°Bien! Es la tabla que buscamos exportar, pero vemos que las 2 √∫ltimas filas no son necesarias, por lo que pasamos a eliminarlas.\n\n# Remover ultima(s) n fila(s)\ncenso_trujillo.drop(censo_trujillo.tail(2).index, inplace=True)\n\n# Verificar si se eliminaron los registros no deseados\ncenso_trujillo.tail(2)\n\n\n\n\n\n\n\n\n\nUBIGEO\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\n\n\n9\n130110\nSimbal\n1662\n1151\n4061\n\n\n10\n130111\nV√≠ctor Larco Herrera\n19¬†543\n18¬†461\n68¬†506\n\n\n\n\n\n\n\n\nAhora asignaremos el UBIGEO como √≠ndice de la tabla.\n\ncenso_trujillo.set_index('UBIGEO', inplace = True)\n\n# Verificamos el cambio de √≠ndice\ncenso_trujillo.head(2)\n\n\n\n\n\n\n\n\n\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\nUBIGEO\n\n\n\n\n\n\n\n\n130101\nTrujillo\n87¬†963\n82¬†236\n314¬†939\n\n\n130102\nEl Porvenir\n57¬†878\n50¬†805\n190¬†461\n\n\n\n\n\n\n\n\nTambi√©n, vemos que en las columnas Hogares,Viviendas y Poblaci√≥n que contienen n√∫meros, hay un espacio en medio de los n√∫meros que le da Wikipedia como formato para mejorar su visualizaci√≥n. Sin embargo, necesitamo que nuestros datos num√©ricos est√©n limpios sin ning√∫n simbolo o espacios para poder realizar operaciones.\nRemovamos ese espacio que no aporta nuestros datos. Para ello usaremos la biblioteca normalize.\n\nfrom unicodedata import normalize\n\nCreamos una funcion y lo correremos a toda nuestra tabla para quitar los molestos espacios en blanco.\n\ndef remove_whitespace(x):\n    \"\"\"Funcion para normalizar datos con Unicode para luego quitar los espacios usando .replace().\n\n    Argumentos de entrada: Nombre de columna o lista con nombres de columnas.\n    Retorna: columna o columnas sin espacios en blanco\n    \"\"\"\n    if isinstance(x, str):\n        return normalize('NFKC', x).replace(' ', '')\n    else:\n        return x\n\nAplicamos la funcion para quitar espacios en blanco a toda las columnas con datos num√©ricos.\n\n# Guardamos en variable nombre de columnas a quitar espacios en blanco\nnumeric_cols = ['Hogares','Viviendas','Poblaci√≥n']\n\n# Aplicar funci√≥n remove_whitespace a columnas en variable y las reemplazamos en tabla\ncenso_trujillo[numeric_cols] = censo_trujillo[numeric_cols].applymap(remove_whitespace)\n\n# Verificamos si se quitaron los espacios en blanco\ncenso_trujillo.head()\n\n\n\n\n\n\n\n\n\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\nUBIGEO\n\n\n\n\n\n\n\n\n130101\nTrujillo\n87963\n82236\n314939\n\n\n130102\nEl Porvenir\n57878\n50805\n190461\n\n\n130103\nFlorencia De Mora\n7777\n8635\n37262\n\n\n130104\nHuanchaco\n20206\n16534\n68409\n\n\n130105\nLa Esperanza\n49773\n47896\n189206\n\n\n\n\n\n\n\n\nAhora veamos los tipos de datos.\n\n# Mostrar tipo de datos de la tabla\ncenso_trujillo.dtypes\n\nDistrito     object\nHogares      object\nViviendas    object\nPoblaci√≥n    object\ndtype: object\n\n\nComo vemos, todos las columnas son de tipo de dato Objeto (lo que Pandas considera como una cadena de caracteres o String). Como Object es muy amplio, necesitamos definir el tipo de dato correcto a cada columna para que luego se realizar operaciones con los datos.\nAqu√≠ va la primera opci√≥n para hacerlo, reusando la variable numeric_cols usada l√≠neas arriba.\n\n# Asignar el tipo de dato num√©rico a columnas en variable\ncenso_trujillo[numeric_cols] = censo_trujillo[numeric_cols].apply(pd.to_numeric)\n\n# Verificamos que las columnas con n√∫meros tengan el tipo de dato num√©rico asignado\ncenso_trujillo.dtypes\n\nDistrito     object\nHogares       int64\nViviendas     int64\nPoblaci√≥n     int64\ndtype: object\n\n\nComo ves, nuestras columnas con n√∫meros ya tienen el formato correcto, pero la columna Distrito a√∫n se mantiene como Object. Aunque no habr√≠a mayor inconveniente, es mejor especificar que datos contiene cada columna.\nAqu√≠ va la seguida opci√≥n para cambiar el tipo de dato a m√∫ltiples columnas.\n\n# Creamos diccionario y pasamos m√∫ltiples columnas con el tipo de dato a asignar\nconvert_dict = {\n    'Distrito': 'string',\n    'Hogares': 'int',\n    'Viviendas': 'int',\n    'Poblaci√≥n': 'int'\n}\n\ncenso_trujillo = censo_trujillo.astype(convert_dict)\n\n# Verificamos que las columnas con n√∫meros tengan el tipo de dato num√©rico asignado\ncenso_trujillo.dtypes\n\nDistrito     string\nHogares       int64\nViviendas     int64\nPoblaci√≥n     int64\ndtype: object"
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-3-guardado-del-conjunto-de-datos",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-3-guardado-del-conjunto-de-datos",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "Fase 3: Guardado del Conjunto de Datos",
    "text": "Fase 3: Guardado del Conjunto de Datos\nPor fin, una ves los datos est√°n limpios y con el tipo de dato correcto, vamos a guardarlos en formato CSV para usarlos luego.\n\n# Guarda Dataframe a archivo CSV\ncenso_trujillo.to_csv('censo_provincia_trujillo_2017.csv')\n\n# Leamos el archivo para verificar su creacion\npd.read_csv('censo_provincia_trujillo_2017.csv').head(3)\n\n\n\n\n\n\n\n\n\nUBIGEO\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\n\n\n0\n130101\nTrujillo\n87963\n82236\n314939\n\n\n1\n130102\nEl Porvenir\n57878\n50805\n190461\n\n\n2\n130103\nFlorencia De Mora\n7777\n8635\n37262\n\n\n\n\n\n\n\n\nSi lo trabajaste en Jupyter Notebook o tu Editor de C√≥digo Favorito, debe estar ubicado en la misma carpeta donde corriste tu archivo .pynb. Pero, si lo trabajaste en Google Colab (como yo), puedes usar la biblioteca files para descargar el archivo en la computadora donde est√©s trabajando.\n\n# Cargar o descargar archivos\nfrom google.colab import files\n\n# Descarga archivo con datos de tabla\nfiles.download(\"censo_provincia_trujillo_2017.csv\")\n\nprint('Listo, en un momento saldr√° la opci√≥n \"Guardar Como\" para descargar el archivo...')\n\n\n\n\n\n\n\nListo, en un momento saldr√° la opci√≥n \"Guardar Como\" para descargar el archivo...\n\n\n¬°Genial! Ya tenemos los datos de una tabla de Wikipedia lista para su uso. Aunque la tabla extraida es peque√±a en dimensi√≥n, esta forma de trabajo puedes aplicarla para extraer cualquier tabla que encuentres interesante, ya sea de Wikipedia o de cualquier otra p√°gina web que contenga tablas HTML."
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#resumiendo-lo-realizado",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#resumiendo-lo-realizado",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "Resumiendo lo realizado",
    "text": "Resumiendo lo realizado\n\nLe√≠mos las tablas HTML de una p√°gina de Wikipedia\nRemovimos los espacios en formato Unicode que imped√≠an la conversi√≥n al tipo de dato correcto\nConvertimos el tipo de dato de todas las columnas al correcto\nGuardamos la tabla extra√≠da como formato csv para su posterior utilizaci√≥n\nFinalmente, descargamos el archivo csv en la computadora de trabajo\n\n¬°Espera! Una cosa m√°s, como est√°mos en el mes de diciembre, te regalo este pensamiento:\n\nSi lo lees y lo entiendes est√° genial, aunque eso no te asegura haberlo aprendido. Para dominarlo necesitas practicarlo.\n\nPor eso, te sugiero que guardes este art√≠culo en tus Favoritos para que lo practiques luego. O si quieres practicarlo ya mismo, aqu√≠ te dejo el art√≠culo en Google Colab, donde podr√°s abrir y correr el c√≥digo sin necesidad de instalar nada, solo tu navegador web y tus ganas de aprender.\nSi te gust√≥, puedes ver mis otras publicaciones, seguro te ser√°n de utilidad. Si gustas apoyarme, comparte este art√≠culo en tus Redes Sociales (Facebook, Linkedint, Twitter) o si est√°s de buen √°nimo, inv√≠tame una taza de caf√© ‚òï. Nos vemos üèÉüí®"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  }
]