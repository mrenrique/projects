[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Projects",
    "section": "",
    "text": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a Word Cloud in an image‚Äôs shape by Scraping an Article\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 1, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 28, 2024\n\n\nTristan O‚ÄôMalley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#tldr",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#tldr",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "TL;DR ü§ì",
    "text": "TL;DR ü§ì\nHave you ever crossed some blog post, video or presentation having A fun way to show & analize which are the most relevant topics(repeated words) on a text. Bellow I show you some examples whe can create, it‚Äôs like art made out of words üé®\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#installing-libraries",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#installing-libraries",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "Installing Libraries ‚úîÔ∏è",
    "text": "Installing Libraries ‚úîÔ∏è\nFirst, you need to install all libraries you‚Äôll be using.\n\n!pip install numpy\n!pip install matplotlib\n!pip install newspaper3k\n!pip install pillow\n!pip install wordcloud\n!pip install nltk\n\nprint('Library installation Done!')\n\nCollecting newspaper3k\n  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 211 kB 7.6 MB/s \nRequirement already satisfied: lxml&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\nRequirement already satisfied: Pillow&gt;=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\nCollecting feedparser&gt;=5.2.1\n  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81 kB 10.0 MB/s \nRequirement already satisfied: python-dateutil&gt;=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\nRequirement already satisfied: requests&gt;=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\nCollecting jieba3k&gt;=0.35.1\n  Downloading jieba3k-0.35.1.zip (7.4 MB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.4 MB 60.7 MB/s \nRequirement already satisfied: nltk&gt;=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\nCollecting feedfinder2&gt;=0.0.4\n  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\nCollecting cssselect&gt;=0.9.2\n  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\nRequirement already satisfied: beautifulsoup4&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\nCollecting tinysegmenter==0.3\n  Downloading tinysegmenter-0.3.tar.gz (16 kB)\nRequirement already satisfied: PyYAML&gt;=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\nCollecting tldextract&gt;=2.0.1\n  Downloading tldextract-3.1.2-py2.py3-none-any.whl (87 kB)\n     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87 kB 5.8 MB/s \nRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2&gt;=0.0.4-&gt;newspaper3k) (1.15.0)\nCollecting sgmllib3k\n  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (3.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (2021.10.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (1.24.3)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (2.10)\nCollecting requests-file&gt;=1.4\n  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\nRequirement already satisfied: filelock&gt;=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract&gt;=2.0.1-&gt;newspaper3k) (3.4.2)\nBuilding wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n  Building wheel for tinysegmenter (setup.py) ... done\n  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13553 sha256=73e9156b3836ce85dc512439196c449be3d5d33749708372688c3f5d99f1a059\n  Stored in directory: /root/.cache/pip/wheels/df/67/41/faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa\n  Building wheel for feedfinder2 (setup.py) ... done\n  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3357 sha256=fe3990b9a89a29307f18a4767d156083b809112c9ca71e5ef2bbb556ddbd874e\n  Stored in directory: /root/.cache/pip/wheels/7f/d4/8f/6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346\n  Building wheel for jieba3k (setup.py) ... done\n  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398404 sha256=86ab131c0f61eeaef932b8967c1915eaaabc4a1089ea070652c21631f839e9b4\n  Stored in directory: /root/.cache/pip/wheels/4c/91/46/3c208287b726df325a5979574324878b679116e4baae1af3c3\n  Building wheel for sgmllib3k (setup.py) ... done\n  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=9ad6b6bbe0b981e0262c6294017917f7b9cb32e8af7463272df81aaf6f7f2001\n  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\nSuccessfully built tinysegmenter feedfinder2 jieba3k sgmllib3k"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#importing-libraries",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#importing-libraries",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "Importing Libraries üß∞",
    "text": "Importing Libraries üß∞\n\nfrom newspaper import Article\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, ImageColorGenerator\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download(\"stopwords\")\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#step-01-getting-the-corpus-from-an-article",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#step-01-getting-the-corpus-from-an-article",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "Step 01: Getting the Corpus from an Article üìù",
    "text": "Step 01: Getting the Corpus from an Article üìù\n\narticle = Article('https://www.repsol.pe/es/sala-prensa/notas-prensa/comunicado.cshtml')\narticle.download()\narticle.parse()\n\n\nGenerate a Simple Word Cloud Image\n\n# Generate a word cloud image\nwc = WordCloud()\nwc.generate(article.text)\n\n# Display the generated image:\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nGenerate a Customized Word Cloud Image\n\nstopwords = set(stopwords.words('spanish', 'english'))\n\nstopwords.update(['ello', 'cinco', 'd√≠a'])\n\nConverting an image to a numpy array results in an array containing a sequence of values that each represent an individual pixel in the image.\n\nmask = np.array(Image.open('repsol.jpg'))\n# mask = np.array(Image.open('pedro-castillo.jpg'))\n\nwc = WordCloud(stopwords=stopwords,\n               background_color=\"white\",\n               max_words=2000,\n               mask=mask,\n               )\n\nwc.generate(article.text)\n\n# Display the generated image:\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nmask = np.array(Image.open('repsol.jpg'))\n# mask = np.array(Image.open('pedro-castillo.jpg'))\n\nwc = WordCloud(stopwords=stopwords,\n               background_color=\"white\",\n               max_words=2000,\n               mask=mask,\n               max_font_size=256,\n            #    random_state=42,\n            #    width=mask.shape[1],\n            #    height=mask.shape[0]\n               )\n\nwc.generate(article.text)\n\n# create coloring from image\nimage_colors = ImageColorGenerator(mask)\nplt.figure(figsize=[16,14])\nplt.imshow(wc.recolor(color_func=image_colors), interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "posts/making-a-word-cloud-by-scraping-an-article/index.html#references",
    "href": "posts/making-a-word-cloud-by-scraping-an-article/index.html#references",
    "title": "Making a Word Cloud in an image‚Äôs shape by Scraping an Article",
    "section": "References:",
    "text": "References:\n\nhttps://medium.com/towards-data-science/create-word-cloud-into-any-shape-you-want-using-python-d0b88834bc32\nhttps://www.repsol.pe/es/sala-prensa/notas-prensa/comunicado.cshtml"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "",
    "text": "¬øAlguna vez te topaste con alguna tabla de Wikipedia que justo necesitabas para tu investigaci√≥n o proyecto de an√°lisis? Como la tabla de medallas de los Juegos Panamericanos, Las Elecciones Presidenciales o datos de las Exportaciones de un Pais, ¬øSi? Bueno, Aqu√≠ aprender√°s como obtener datos de cualquier tabla.\nEsta es la tabla que buscamos obtener que contiene informaci√≥n del Censo de la Provincia de Trujillo del a√±o 2017. Para hacer Web Scraping a esta p√°gina, utilizaremos las bibliotecas Requests y Pandas de Python.\nAl finalizar este Tutorial, tendr√°s como resultado una tabla lista para su an√°lisis. Lo podr√°s guardar en tu directorio de trabajo o descargar en tu computadora como archivo CSV (o el formato que gustes). Espero que te resulte √∫til; sin m√°s que a√±adir‚Ä¶ ‚ÄúHappy Coding!‚Äù"
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-1-conecci√≥n-y-obtenci√≥n-de-codigo-fuente",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-1-conecci√≥n-y-obtenci√≥n-de-codigo-fuente",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "Fase 1: Conecci√≥n y obtenci√≥n de codigo fuente",
    "text": "Fase 1: Conecci√≥n y obtenci√≥n de codigo fuente\nComenzaremos importando la biblioteca requests para realizar la petici√≥n HTTP que nos devolver√° el c√≥digo fuente de la p√°gina web y pandas, para usar su m√©todo .read_html() que nos ayudar√° a extraer todas las tablas HTML. Usaremos este m√©todo y no la biblioteca Beautiful Soup porque de esta forma es mucho m√°s f√°cil y funciona bien en cualquier p√°gina web que contenga tablas HTML debido a su estructura simple de leer.\n\nPara tu conocimiento: Usando .read_html() no solo nos permite extraer tablas HTML desde un enlace (lo que usaremos hoy), sino tambi√©n desde un archivo html o una cadena de caracteres (string) que contenga HTML.\n\n\n# Hacer peticion HTTP\nimport requests\n# Manipular c√≥digo y guardar datos tabulares en archivo CSV\nimport pandas as pd\n\n\n# url de la p√°gina web a ¬´escrapear¬ª\nurl = 'https://es.wikipedia.org/wiki/Provincia_de_Trujillo_(Per%C3%BA)'\n\n# pasar \"User-agent\" para simular interacci√≥n con la p√°gina usando Navegador web\nheaders = {\"User-agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'}\n\nrespuesta = requests.get(url, headers=headers)\n\n# El c√≥digo de respuesta &lt;200&gt; indicar√° que todo sali√≥ bien\nprint(respuesta)\n\n&lt;Response [200]&gt;\n\n\nAparte de la respuesta positiva que nos devolvi√≥ la petici√≥n, seguro te diste cuenta que pasamos el par√°metro headers al m√©todo requests.get(), esto sirve para evitar que el servidor de la p√°gina web nos bloqueen el acceso. Aunque con Wikipedia no tenemos ese problema, otras p√°ginas son restrictivas cuando se trata de bots tratando de ¬´escrapear¬ª sus datos. Siguiento esta buena pr√°ctica, nos ahorarremos dolores de cabeza luego.\nPor √∫ltimo, guardamos en una variable todas las tablas encontradas en el objecto HTML.\n\nall_tables = pd.read_html(respuesta.content, encoding = 'utf8')"
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-2-an√°lisis-de-estructura-html-y-extracci√≥n-de-datos",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-2-an√°lisis-de-estructura-html-y-extracci√≥n-de-datos",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "Fase 2: An√°lisis de estructura HTML y extracci√≥n de datos",
    "text": "Fase 2: An√°lisis de estructura HTML y extracci√≥n de datos\nAhora veamos cuantas tablas hay en la p√°gina web\n\nprint(f'Total de tablas encontradas: {len(all_tables)}')\n\nTotal de tablas encontradas: 6\n\n\nDebido a que hay varias tablas, usaremos el par√°metro match y le pasaremos una palabra o frase que solo se encuentre en la tabla que nos interesa.\n\nmatched_table = pd.read_html(respuesta.text, match='Fuente: Censos Nacionales 2017')\n\n# imprime numero de tablas que coinciden con parametro match\nprint(f'Total de tablas encontradas: {len(matched_table)}')\n\nTotal de tablas encontradas: 1\n\n\nGuardamos nuestra tabla de inter√©s en una variable.\n\n# Guardar tabla en variable con nombre sem√°ntico\ncenso_trujillo = matched_table[0]\n\n# Verificamos si es la tabla que buscamos\ncenso_trujillo.tail(5)\n\n\n\n\n\n\n\n\n\nUBIGEO\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\n\n\n8\n130109\nSalaverry\n5599\n5244\n18¬†944\n\n\n9\n130110\nSimbal\n1662\n1151\n4061\n\n\n10\n130111\nV√≠ctor Larco Herrera\n19¬†543\n18¬†461\n68¬†506\n\n\n11\nNaN\nTOTAL\n273¬†619\n250¬†835\n970¬†016\n\n\n12\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\nFuente: Censos Nacionales 2017: X de Poblaci√≥n...\n\n\n\n\n\n\n\n\n¬°Bien! Es la tabla que buscamos exportar, pero vemos que las 2 √∫ltimas filas no son necesarias, por lo que pasamos a eliminarlas.\n\n# Remover ultima(s) n fila(s)\ncenso_trujillo.drop(censo_trujillo.tail(2).index, inplace=True)\n\n# Verificar si se eliminaron los registros no deseados\ncenso_trujillo.tail(2)\n\n\n\n\n\n\n\n\n\nUBIGEO\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\n\n\n9\n130110\nSimbal\n1662\n1151\n4061\n\n\n10\n130111\nV√≠ctor Larco Herrera\n19¬†543\n18¬†461\n68¬†506\n\n\n\n\n\n\n\n\nAhora asignaremos el UBIGEO como √≠ndice de la tabla.\n\ncenso_trujillo.set_index('UBIGEO', inplace = True)\n\n# Verificamos el cambio de √≠ndice\ncenso_trujillo.head(2)\n\n\n\n\n\n\n\n\n\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\nUBIGEO\n\n\n\n\n\n\n\n\n130101\nTrujillo\n87¬†963\n82¬†236\n314¬†939\n\n\n130102\nEl Porvenir\n57¬†878\n50¬†805\n190¬†461\n\n\n\n\n\n\n\n\nTambi√©n, vemos que en las columnas Hogares,Viviendas y Poblaci√≥n que contienen n√∫meros, hay un espacio en medio de los n√∫meros que le da Wikipedia como formato para mejorar su visualizaci√≥n. Sin embargo, necesitamo que nuestros datos num√©ricos est√©n limpios sin ning√∫n simbolo o espacios para poder realizar operaciones.\nRemovamos ese espacio que no aporta nuestros datos. Para ello usaremos la biblioteca normalize.\n\nfrom unicodedata import normalize\n\nCreamos una funcion y lo correremos a toda nuestra tabla para quitar los molestos espacios en blanco.\n\ndef remove_whitespace(x):\n    \"\"\"Funcion para normalizar datos con Unicode para luego quitar los espacios usando .replace().\n\n    Argumentos de entrada: Nombre de columna o lista con nombres de columnas.\n    Retorna: columna o columnas sin espacios en blanco\n    \"\"\"\n    if isinstance(x, str):\n        return normalize('NFKC', x).replace(' ', '')\n    else:\n        return x\n\nAplicamos la funcion para quitar espacios en blanco a toda las columnas con datos num√©ricos.\n\n# Guardamos en variable nombre de columnas a quitar espacios en blanco\nnumeric_cols = ['Hogares','Viviendas','Poblaci√≥n']\n\n# Aplicar funci√≥n remove_whitespace a columnas en variable y las reemplazamos en tabla\ncenso_trujillo[numeric_cols] = censo_trujillo[numeric_cols].applymap(remove_whitespace)\n\n# Verificamos si se quitaron los espacios en blanco\ncenso_trujillo.head()\n\n\n\n\n\n\n\n\n\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\nUBIGEO\n\n\n\n\n\n\n\n\n130101\nTrujillo\n87963\n82236\n314939\n\n\n130102\nEl Porvenir\n57878\n50805\n190461\n\n\n130103\nFlorencia De Mora\n7777\n8635\n37262\n\n\n130104\nHuanchaco\n20206\n16534\n68409\n\n\n130105\nLa Esperanza\n49773\n47896\n189206\n\n\n\n\n\n\n\n\nAhora veamos los tipos de datos.\n\n# Mostrar tipo de datos de la tabla\ncenso_trujillo.dtypes\n\nDistrito     object\nHogares      object\nViviendas    object\nPoblaci√≥n    object\ndtype: object\n\n\nComo vemos, todos las columnas son de tipo de dato Objeto (lo que Pandas considera como una cadena de caracteres o String). Como Object es muy amplio, necesitamos definir el tipo de dato correcto a cada columna para que luego se realizar operaciones con los datos.\nAqu√≠ va la primera opci√≥n para hacerlo, reusando la variable numeric_cols usada l√≠neas arriba.\n\n# Asignar el tipo de dato num√©rico a columnas en variable\ncenso_trujillo[numeric_cols] = censo_trujillo[numeric_cols].apply(pd.to_numeric)\n\n# Verificamos que las columnas con n√∫meros tengan el tipo de dato num√©rico asignado\ncenso_trujillo.dtypes\n\nDistrito     object\nHogares       int64\nViviendas     int64\nPoblaci√≥n     int64\ndtype: object\n\n\nComo ves, nuestras columnas con n√∫meros ya tienen el formato correcto, pero la columna Distrito a√∫n se mantiene como Object. Aunque no habr√≠a mayor inconveniente, es mejor especificar que datos contiene cada columna.\nAqu√≠ va la seguida opci√≥n para cambiar el tipo de dato a m√∫ltiples columnas.\n\n# Creamos diccionario y pasamos m√∫ltiples columnas con el tipo de dato a asignar\nconvert_dict = {\n    'Distrito': 'string',\n    'Hogares': 'int',\n    'Viviendas': 'int',\n    'Poblaci√≥n': 'int'\n}\n\ncenso_trujillo = censo_trujillo.astype(convert_dict)\n\n# Verificamos que las columnas con n√∫meros tengan el tipo de dato num√©rico asignado\ncenso_trujillo.dtypes\n\nDistrito     string\nHogares       int64\nViviendas     int64\nPoblaci√≥n     int64\ndtype: object"
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-3-guardado-del-conjunto-de-datos",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#fase-3-guardado-del-conjunto-de-datos",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "Fase 3: Guardado del Conjunto de Datos",
    "text": "Fase 3: Guardado del Conjunto de Datos\nPor fin, una ves los datos est√°n limpios y con el tipo de dato correcto, vamos a guardarlos en formato CSV para usarlos luego.\n\n# Guarda Dataframe a archivo CSV\ncenso_trujillo.to_csv('censo_provincia_trujillo_2017.csv')\n\n# Leamos el archivo para verificar su creacion\npd.read_csv('censo_provincia_trujillo_2017.csv').head(3)\n\n\n\n\n\n\n\n\n\nUBIGEO\nDistrito\nHogares\nViviendas\nPoblaci√≥n\n\n\n\n\n0\n130101\nTrujillo\n87963\n82236\n314939\n\n\n1\n130102\nEl Porvenir\n57878\n50805\n190461\n\n\n2\n130103\nFlorencia De Mora\n7777\n8635\n37262\n\n\n\n\n\n\n\n\nSi lo trabajaste en Jupyter Notebook o tu Editor de C√≥digo Favorito, debe estar ubicado en la misma carpeta donde corriste tu archivo .pynb. Pero, si lo trabajaste en Google Colab (como yo), puedes usar la biblioteca files para descargar el archivo en la computadora donde est√©s trabajando.\n\n# Cargar o descargar archivos\nfrom google.colab import files\n\n# Descarga archivo con datos de tabla\nfiles.download(\"censo_provincia_trujillo_2017.csv\")\n\nprint('Listo, en un momento saldr√° la opci√≥n \"Guardar Como\" para descargar el archivo...')\n\n\n\n\n\n\n\nListo, en un momento saldr√° la opci√≥n \"Guardar Como\" para descargar el archivo...\n\n\n¬°Genial! Ya tenemos los datos de una tabla de Wikipedia lista para su uso. Aunque la tabla extraida es peque√±a en dimensi√≥n, esta forma de trabajo puedes aplicarla para extraer cualquier tabla que encuentres interesante, ya sea de Wikipedia o de cualquier otra p√°gina web que contenga tablas HTML."
  },
  {
    "objectID": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#resumiendo-lo-realizado",
    "href": "posts/web-scraping-de-tablas-html-desde-wikipedia/index.html#resumiendo-lo-realizado",
    "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
    "section": "Resumiendo lo realizado",
    "text": "Resumiendo lo realizado\n\nLe√≠mos las tablas HTML de una p√°gina de Wikipedia\nRemovimos los espacios en formato Unicode que imped√≠an la conversi√≥n al tipo de dato correcto\nConvertimos el tipo de dato de todas las columnas al correcto\nGuardamos la tabla extra√≠da como formato csv para su posterior utilizaci√≥n\nFinalmente, descargamos el archivo csv en la computadora de trabajo\n\n¬°Espera! Una cosa m√°s, como est√°mos en el mes de diciembre, te regalo este pensamiento:\n\nSi lo lees y lo entiendes est√° genial, aunque eso no te asegura haberlo aprendido. Para dominarlo necesitas practicarlo.\n\nPor eso, te sugiero que guardes este art√≠culo en tus Favoritos para que lo practiques luego. O si quieres practicarlo ya mismo, aqu√≠ te dejo el art√≠culo en Google Colab, donde podr√°s abrir y correr el c√≥digo sin necesidad de instalar nada, solo tu navegador web y tus ganas de aprender.\nSi te gust√≥, puedes ver mis otras publicaciones, seguro te ser√°n de utilidad. Si gustas apoyarme, comparte este art√≠culo en tus Redes Sociales (Facebook, Linkedint, Twitter) o si est√°s de buen √°nimo, inv√≠tame una taza de caf√© ‚òï. Nos vemos üèÉüí®"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  }
]